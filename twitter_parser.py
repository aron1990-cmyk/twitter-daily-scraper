# -*- coding: utf-8 -*-
"""
Twitter è§£æå™¨
ä½¿ç”¨ Playwright æ§åˆ¶æµè§ˆå™¨å¹¶æŠ“å–æ¨æ–‡æ•°æ®
"""

import asyncio
import logging
import re
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from playwright.async_api import async_playwright, Browser, Page
# é…ç½®å°†ä»è°ƒç”¨æ–¹ä¼ å…¥æˆ–ä½¿ç”¨é»˜è®¤é…ç½®
from human_behavior_simulator import HumanBehaviorSimulator
from performance_optimizer import EnhancedSearchOptimizer

class TwitterParser:
    def __init__(self, debug_port: str = None):
        self.debug_port = debug_port
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.behavior_simulator = None
        self.search_optimizer = EnhancedSearchOptimizer()
        self.logger = logging.getLogger(__name__)
        self.config = None
        
        # ä¼˜åŒ–åŠŸèƒ½å±æ€§
        self.seen_tweet_ids: Set[str] = set()
        self.content_cache: Dict[str, str] = {}
        self.optimization_enabled = True
    
    async def initialize(self, debug_port: str = None):
        """åˆå§‹åŒ–TwitterParser
        
        Args:
            debug_port: æµè§ˆå™¨è°ƒè¯•ç«¯å£
        """
        if debug_port:
            self.debug_port = debug_port
        
        if not self.debug_port:
            raise ValueError("debug_port is required for initialization")
        
        self.logger.info(f"Initializing TwitterParser with debug_port: {self.debug_port}")
        
        # è¿æ¥æµè§ˆå™¨
        await self.connect_browser()
        
        self.logger.info("TwitterParser initialization completed")
        
    async def connect_browser(self):
        """
        è¿æ¥åˆ° AdsPower æµè§ˆå™¨
        """
        try:
            self.logger.info("å¼€å§‹å¯åŠ¨ Playwright...")
            playwright = await async_playwright().start()
            
            self.logger.info(f"å¼€å§‹è¿æ¥åˆ°æµè§ˆå™¨è°ƒè¯•ç«¯å£: {self.debug_port}")
            # è¿æ¥åˆ°ç°æœ‰çš„æµè§ˆå™¨å®ä¾‹
            self.browser = await playwright.chromium.connect_over_cdp(self.debug_port)
            self.logger.info("æˆåŠŸè¿æ¥åˆ°æµè§ˆå™¨å®ä¾‹")
            
            # è·å–ç°æœ‰ä¸Šä¸‹æ–‡å’Œé¡µé¢
            contexts = self.browser.contexts
            self.logger.info(f"æ‰¾åˆ° {len(contexts)} ä¸ªæµè§ˆå™¨ä¸Šä¸‹æ–‡")
            
            if contexts:
                context = contexts[0]
                pages = context.pages
                self.logger.info(f"åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ° {len(pages)} ä¸ªé¡µé¢")
                
                if pages:
                    # ä½¿ç”¨ç°æœ‰é¡µé¢
                    self.page = pages[0]
                    try:
                        current_url = self.page.url
                        self.logger.info(f"ä½¿ç”¨ç°æœ‰é¡µé¢ï¼Œå½“å‰URL: {current_url}")
                    except Exception as url_error:
                        self.logger.warning(f"æ— æ³•è·å–é¡µé¢URL: {url_error}")
                        # åˆ›å»ºæ–°é¡µé¢ä½œä¸ºå¤‡é€‰
                        self.page = await context.new_page()
                        self.logger.info("åˆ›å»ºæ–°é¡µé¢ä½œä¸ºå¤‡é€‰")
                else:
                    # åœ¨ç°æœ‰ä¸Šä¸‹æ–‡ä¸­åˆ›å»ºæ–°é¡µé¢
                    self.page = await context.new_page()
                    self.logger.info("åœ¨ç°æœ‰ä¸Šä¸‹æ–‡ä¸­åˆ›å»ºæ–°é¡µé¢")
            else:
                # åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œè®¾ç½®ç”¨æˆ·ä»£ç†å’Œå…¶ä»–é€‰é¡¹
                self.logger.info("åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡...")
                context = await self.browser.new_context(
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080},
                    extra_http_headers={
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
                    }
                )
                self.page = await context.new_page()
                self.logger.info("åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡å’Œé¡µé¢")
            
            # è®¾ç½®é¡µé¢é»˜è®¤è¶…æ—¶æ—¶é—´
            default_timeout = 30000
            self.page.set_default_timeout(default_timeout)
            self.logger.info(f"è®¾ç½®é¡µé¢è¶…æ—¶æ—¶é—´: {default_timeout}ms")
            
            # è®¾ç½®é¡µé¢å¯¼èˆªè¶…æ—¶æ—¶é—´
            navigation_timeout = 60000
            self.page.set_default_navigation_timeout(navigation_timeout)
            self.logger.info(f"è®¾ç½®å¯¼èˆªè¶…æ—¶æ—¶é—´: {navigation_timeout}ms")
            
            # åˆå§‹åŒ–äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨
            self.behavior_simulator = HumanBehaviorSimulator(self.page)
            self.logger.info("äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨åˆå§‹åŒ–å®Œæˆ")
            
            self.logger.info("æˆåŠŸè¿æ¥åˆ°æµè§ˆå™¨")
            
        except Exception as e:
            self.logger.error(f"è¿æ¥æµè§ˆå™¨å¤±è´¥: {e}")
            raise
    
    async def navigate_to_twitter(self, max_retries: int = 3):
        """
        å¯¼èˆªåˆ° Twitter ä¸»é¡µ
        
        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for attempt in range(max_retries):
            try:
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹
                await self.ensure_page_focus()
                
                current_url = self.page.url
                self.logger.info(f"å½“å‰é¡µé¢URL: {current_url}")
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ x.com
                if 'x.com' not in current_url:
                    self.logger.info(f"é¡µé¢ä¸åœ¨ x.comï¼Œå¼€å§‹å¯¼èˆª... (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
                    
                    # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´
                    await self.page.goto('https://x.com', timeout=60000)
                    
                    # åˆ†æ­¥ç­‰å¾…åŠ è½½
                    try:
                        await self.page.wait_for_load_state('domcontentloaded', timeout=30000)
                        await self.page.wait_for_load_state('networkidle', timeout=30000)
                    except Exception as load_error:
                        self.logger.warning(f"ç­‰å¾…åŠ è½½çŠ¶æ€å¤±è´¥: {load_error}ï¼Œç»§ç»­å°è¯•")
                    
                    self.logger.info("æˆåŠŸå¯¼èˆªåˆ° X (Twitter)")
                else:
                    self.logger.info("é¡µé¢å·²ç»åœ¨ X (Twitter)ï¼Œè·³è¿‡å¯¼èˆª")
                
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹åå†è¿›è¡Œæµ‹è¯•
                await self.ensure_page_focus()
                
                # æµ‹è¯•é¡µé¢äº¤äº’ - æ‰§è¡Œå‡ æ¬¡ä¸‹æ‹‰æ“ä½œ
                self.logger.info("å¼€å§‹æµ‹è¯•é¡µé¢ä¸‹æ‹‰åŠŸèƒ½...")
                for i in range(3):
                    # å‘ä¸‹æ»šåŠ¨
                    await self.page.evaluate('window.scrollBy(0, 500)')
                    await asyncio.sleep(1)
                    self.logger.info(f"æ‰§è¡Œç¬¬ {i+1} æ¬¡ä¸‹æ‹‰")
                
                # æ»šåŠ¨å›é¡¶éƒ¨
                await self.page.evaluate('window.scrollTo(0, 0)')
                await asyncio.sleep(1)
                self.logger.info("é¡µé¢ä¸‹æ‹‰æµ‹è¯•å®Œæˆï¼Œå·²æ»šåŠ¨å›é¡¶éƒ¨")
                return
                
            except Exception as e:
                self.logger.warning(f"ç¬¬{attempt + 1}æ¬¡å¯¼èˆªå°è¯•å¤±è´¥: {e}")
                
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5
                    self.logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    self.logger.error(f"å¯¼èˆªåˆ° X (Twitter) å¤±è´¥ï¼Œå·²å°è¯•{max_retries}æ¬¡")
                    raise Exception(f"å¯¼èˆªåˆ° X (Twitter) å¤±è´¥: {e}")
    
    async def navigate_to_profile(self, username: str, max_retries: int = 3):
        """
        å¯¼èˆªåˆ°æŒ‡å®šç”¨æˆ·çš„ä¸ªäººèµ„æ–™é¡µé¢
        
        Args:
            username: Twitter ç”¨æˆ·åï¼ˆä¸åŒ…å«@ç¬¦å·ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        profile_url = f'https://x.com/{username}'
        
        for attempt in range(max_retries):
            try:
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹
                await self.ensure_page_focus()
                
                self.logger.info(f"å°è¯•å¯¼èˆªåˆ° @{username} çš„ä¸ªäººèµ„æ–™é¡µé¢ (ç¬¬{attempt + 1}æ¬¡)")
                
                # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´è¿›è¡Œå¯¼èˆª
                await self.page.goto(profile_url, timeout=60000)
                
                # åˆ†æ­¥ç­‰å¾…åŠ è½½çŠ¶æ€
                try:
                    await self.page.wait_for_load_state('domcontentloaded', timeout=30000)
                    self.logger.info("DOMå†…å®¹åŠ è½½å®Œæˆ")
                    
                    await self.page.wait_for_load_state('networkidle', timeout=30000)
                    self.logger.info("ç½‘ç»œç©ºé—²çŠ¶æ€è¾¾åˆ°")
                except Exception as load_error:
                    self.logger.warning(f"ç­‰å¾…åŠ è½½çŠ¶æ€å¤±è´¥: {load_error}ï¼Œç»§ç»­å°è¯•")
                
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹åå†ç­‰å¾…
                await self.ensure_page_focus()
                
                # é¢å¤–ç­‰å¾…æ—¶é—´ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
                await asyncio.sleep(2)
                
                # éªŒè¯é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
                current_url = self.page.url
                if username.lower() in current_url.lower():
                    self.logger.info(f"æˆåŠŸå¯¼èˆªåˆ° @{username} çš„ä¸ªäººèµ„æ–™é¡µé¢")
                    return
                else:
                    raise Exception(f"é¡µé¢URLä¸åŒ¹é…ï¼ŒæœŸæœ›åŒ…å«'{username}'ï¼Œå®é™…ä¸º'{current_url}'")
                
            except Exception as e:
                self.logger.warning(f"ç¬¬{attempt + 1}æ¬¡å¯¼èˆªå°è¯•å¤±è´¥: {e}")
                
                if attempt < max_retries - 1:
                    # ç­‰å¾…åé‡è¯•ï¼ˆæé€Ÿæ¨¡å¼ï¼‰
                    wait_time = (attempt + 1) * 1  # æçŸ­é€’å¢ç­‰å¾…æ—¶é—´
                    self.logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                    self.logger.error(f"å¯¼èˆªåˆ° @{username} ä¸ªäººèµ„æ–™é¡µé¢å¤±è´¥ï¼Œå·²å°è¯•{max_retries}æ¬¡")
                    raise Exception(f"å¯¼èˆªåˆ° @{username} ä¸ªäººèµ„æ–™é¡µé¢å¤±è´¥: {e}")
    
    async def search_tweets(self, keyword: str, max_retries: int = 2):
        """
        æœç´¢åŒ…å«æŒ‡å®šå…³é”®è¯çš„æ¨æ–‡
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for attempt in range(max_retries + 1):
            try:
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹
                await self.ensure_page_focus()
                
                self.logger.info(f"å¼€å§‹æœç´¢å…³é”®è¯ '{keyword}' (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
                
                # æ„å»ºæœç´¢URLï¼Œä½¿ç”¨URLç¼–ç 
                import urllib.parse
                encoded_keyword = urllib.parse.quote(keyword)
                search_url = f'https://x.com/search?q={encoded_keyword}&src=typed_query&f=live'
                self.logger.info(f"æœç´¢URL: {search_url}")
                
                # å¯¼èˆªåˆ°æœç´¢é¡µé¢
                self.logger.info("æ­£åœ¨å¯¼èˆªåˆ°æœç´¢é¡µé¢...")
                await self.page.goto(search_url, timeout=30000)
                self.logger.info("å¯¼èˆªå®Œæˆï¼Œç­‰å¾…é¡µé¢åŠ è½½...")
                
                # ç®€åŒ–ç­‰å¾…ç­–ç•¥
                try:
                    await self.page.wait_for_load_state('domcontentloaded', timeout=10000)
                    self.logger.info("DOMå†…å®¹åŠ è½½å®Œæˆ")
                except Exception as load_error:
                    self.logger.warning(f"DOMåŠ è½½è¶…æ—¶: {load_error}")
                
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹åå†ç­‰å¾…æœç´¢ç»“æœ
                await self.ensure_page_focus()
                
                # ç­‰å¾…æœç´¢ç»“æœåŠ è½½ï¼ˆæé€Ÿæ¨¡å¼ï¼‰
                self.logger.info("ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")
                await asyncio.sleep(1)  # æçŸ­ç­‰å¾…æ—¶é—´
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
                try:
                    # ç­‰å¾…æ¨æ–‡å…ƒç´ å‡ºç°
                    await self.page.wait_for_selector('[data-testid="tweet"]', timeout=10000)
                    self.logger.info("æ‰¾åˆ°æ¨æ–‡å…ƒç´ ")
                except Exception:
                    self.logger.warning("æœªæ‰¾åˆ°æ¨æ–‡å…ƒç´ ï¼Œå¯èƒ½æ²¡æœ‰æœç´¢ç»“æœ")
                
                self.logger.info(f"æˆåŠŸæœç´¢å…³é”®è¯: {keyword}")
                return
                
            except Exception as e:
                self.logger.warning(f"ç¬¬{attempt + 1}æ¬¡æœç´¢å°è¯•å¤±è´¥: {e}")
                
                if attempt < max_retries:
                    wait_time = (attempt + 1) * 0.8  # æçŸ­é‡è¯•ç­‰å¾…æ—¶é—´
                    self.logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    self.logger.error(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥ï¼Œå·²å°è¯•{max_retries + 1}æ¬¡")
                    raise Exception(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
    
    async def ensure_page_focus(self):
        """
        ç¡®ä¿é¡µé¢è·å¾—ç„¦ç‚¹ï¼Œå¤„ç†é¡µé¢è¢«åˆ‡æ¢å‡ºå»çš„æƒ…å†µ
        """
        try:
            # æ£€æŸ¥é¡µé¢æ˜¯å¦å¯è§
            is_visible = await self.page.evaluate('!document.hidden')
            if not is_visible:
                self.logger.info("æ£€æµ‹åˆ°é¡µé¢å¤±å»ç„¦ç‚¹ï¼Œå°è¯•æ¢å¤...")
                
                # å°è¯•å°†é¡µé¢å¸¦åˆ°å‰å°
                await self.page.bring_to_front()
                await asyncio.sleep(0.3)              
                # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼æ¢å¤ç„¦ç‚¹ï¼Œé¿å…è¯¯ç‚¹å‡»é“¾æ¥
                try:
                    # æ–¹æ³•1ï¼šç›´æ¥èšç„¦åˆ°é¡µé¢
                    await self.page.evaluate('window.focus()')
                    await asyncio.sleep(0.1)
                    
                    # æ–¹æ³•2ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰ç„¦ç‚¹ï¼Œå°è¯•ç‚¹å‡»ä¸€ä¸ªå®‰å…¨çš„åŒºåŸŸï¼ˆé¡µé¢è¾¹ç¼˜ï¼‰
                    is_visible = await self.page.evaluate('!document.hidden')
                    if not is_visible:
                        # ç‚¹å‡»é¡µé¢å·¦ä¸Šè§’çš„å®‰å…¨åŒºåŸŸï¼Œé¿å…ç‚¹å‡»åˆ°é“¾æ¥
                        await self.page.mouse.click(10, 10)
                        await asyncio.sleep(0.1)
                except Exception as focus_error:
                    self.logger.debug(f"ç„¦ç‚¹æ¢å¤æ“ä½œå¤±è´¥: {focus_error}")
                
                # å†æ¬¡æ£€æŸ¥
                is_visible = await self.page.evaluate('!document.hidden')
                if is_visible:
                    self.logger.info("é¡µé¢ç„¦ç‚¹å·²æ¢å¤")
                else:
                    self.logger.warning("é¡µé¢ä»ç„¶å¤±å»ç„¦ç‚¹ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
                    
        except Exception as e:
            self.logger.warning(f"é¡µé¢ç„¦ç‚¹æ£€æŸ¥å¤±è´¥: {e}ï¼Œç»§ç»­æ‰§è¡Œ")
    
    async def scroll_and_load_tweets(self, max_tweets: int = 10):
        """
        ä½¿ç”¨ä¼˜åŒ–çš„æ»šåŠ¨ç­–ç•¥åŠ è½½æ›´å¤šæ¨æ–‡
        
        Args:
            max_tweets: æœ€å¤§åŠ è½½æ¨æ–‡æ•°é‡
        """
        if self.optimization_enabled:
            self.logger.info(f"ğŸ”§ ä½¿ç”¨ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥ï¼Œç›®æ ‡æ¨æ–‡æ•°: {max_tweets}")
            result = await self.scroll_and_load_tweets_optimized(max_tweets)
            self.logger.info(f"ğŸ”§ ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥å®Œæˆï¼Œè¿”å›ç»“æœ: {result}")
            return result
        
        try:
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹
            await self.ensure_page_focus()
            
            scroll_attempts = 0
            current_tweets = 0
            last_tweet_count = 0
            stagnant_count = 0
            
            self.logger.info(f"å¼€å§‹ä¼˜åŒ–æ»šåŠ¨åŠ è½½æ¨æ–‡ï¼Œç›®æ ‡: {max_tweets} æ¡")
            
            while current_tweets < max_tweets:
                # è·å–å½“å‰æ¨æ–‡æ•°é‡
                tweets = await self.page.query_selector_all('[data-testid="tweet"]')
                current_tweets = len(tweets)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡åŠ è½½
                if current_tweets == last_tweet_count:
                    stagnant_count += 1
                else:
                    stagnant_count = 0
                last_tweet_count = current_tweets
                
                # ä½¿ç”¨ä¼˜åŒ–çš„æ»šåŠ¨ç­–ç•¥
                strategy = self.search_optimizer.optimize_scroll_strategy(
                    current_tweets, max_tweets, scroll_attempts
                )
                
                if not strategy['should_continue']:
                    self.logger.info(f"æ»šåŠ¨ç­–ç•¥å»ºè®®åœæ­¢ï¼Œå½“å‰æ¨æ–‡æ•°: {current_tweets}")
                    break
                
                # å¦‚æœè¿ç»­å¤šæ¬¡æ²¡æœ‰æ–°å†…å®¹ï¼Œå¯ç”¨æ¿€è¿›æ¨¡å¼
                if stagnant_count >= 3:
                    strategy['scroll_distance'] = 3000
                    strategy['wait_time'] = 1.0
                    strategy['aggressive_mode'] = True
                    self.logger.debug("æ£€æµ‹åˆ°å†…å®¹åœæ»ï¼Œå¯ç”¨æ¿€è¿›æ»šåŠ¨æ¨¡å¼")
                
                # æ‰§è¡Œæ»šåŠ¨
                if strategy['aggressive_mode']:
                    self.logger.debug(f"æ¿€è¿›æ»šåŠ¨æ¨¡å¼ï¼Œè·ç¦»: {strategy['scroll_distance']}")
                
                await self.ensure_page_focus()
                await self.page.evaluate(f'window.scrollBy(0, {strategy["scroll_distance"]})')
                await asyncio.sleep(strategy['wait_time'])
                
                scroll_attempts += 1
                
                # é˜²æ­¢æ— é™æ»šåŠ¨
                if scroll_attempts >= strategy['max_scrolls']:
                    self.logger.warning(f"è¾¾åˆ°æœ€å¤§æ»šåŠ¨æ¬¡æ•° {strategy['max_scrolls']}ï¼Œåœæ­¢æ»šåŠ¨")
                    break
                
                # å¦‚æœé•¿æ—¶é—´æ²¡æœ‰æ–°å†…å®¹ï¼Œå°è¯•åˆ·æ–°é¡µé¢
                if stagnant_count >= 8:
                    self.logger.info("é•¿æ—¶é—´æ— æ–°å†…å®¹ï¼Œå°è¯•é¡µé¢åˆ·æ–°")
                    await self.page.reload()
                    await asyncio.sleep(2)
                    stagnant_count = 0
            
            # å¦‚æœæœ‰äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨ä¸”æ•ˆæœä¸ä½³ï¼Œä½œä¸ºè¡¥å……
            if self.behavior_simulator and current_tweets < max_tweets * 0.7:
                self.logger.info("å½“å‰æ•ˆæœä¸ä½³ï¼Œä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨ä½œä¸ºè¡¥å……")
                try:
                    await self.behavior_simulator.smart_scroll_and_collect(
                        max_tweets=max_tweets - current_tweets,
                        target_selector='[data-testid="tweet"]'
                    )
                except Exception as e:
                    self.logger.warning(f"äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¡¥å……å¤±è´¥: {e}")
            
            # è·å–æœ€ç»ˆçš„æ¨æ–‡æ•°é‡
            final_tweets = await self.page.query_selector_all('[data-testid="tweet"]')
            efficiency = len(final_tweets) / max(scroll_attempts, 1)
            self.logger.info(f"ä¼˜åŒ–æ»šåŠ¨å®Œæˆï¼Œæ»šåŠ¨ {scroll_attempts} æ¬¡ï¼Œæœ€ç»ˆæ¨æ–‡æ•°é‡: {len(final_tweets)}ï¼Œæ•ˆç‡: {efficiency:.2f} æ¨æ–‡/æ»šåŠ¨")
            
        except Exception as e:
            self.logger.error(f"ä¼˜åŒ–æ»šåŠ¨å¤±è´¥: {e}")
            raise
    
    def extract_number(self, text: str) -> int:
        """
        ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆå¤„ç†Kã€Mç­‰å•ä½ï¼‰
        
        Args:
            text: åŒ…å«æ•°å­—çš„æ–‡æœ¬
            
        Returns:
            æå–çš„æ•°å­—
        """
        if not text:
            return 0
        
        # ç§»é™¤é€—å·å’Œç©ºæ ¼
        text = text.replace(',', '').replace(' ', '').lower()
        
        # æå–æ•°å­—å’Œå•ä½
        match = re.search(r'([0-9.]+)([km]?)', text)
        if not match:
            return 0
        
        number = float(match.group(1))
        unit = match.group(2)
        
        if unit == 'k':
            return int(number * 1000)
        elif unit == 'm':
            return int(number * 1000000)
        else:
            return int(number)
    
    async def parse_tweet_element(self, tweet_element) -> Optional[Dict[str, Any]]:
        """
        è§£æå•ä¸ªæ¨æ–‡å…ƒç´ 
        
        Args:
            tweet_element: æ¨æ–‡DOMå…ƒç´ 
            
        Returns:
            æ¨æ–‡æ•°æ®å­—å…¸
        """
        # å¦‚æœå¯ç”¨äº†ä¼˜åŒ–åŠŸèƒ½ï¼Œä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
        if self.optimization_enabled:
            self.logger.info(f"ğŸ”§ è°ƒç”¨ä¼˜åŒ–ç‰ˆæœ¬è§£ææ–¹æ³•ï¼Œoptimization_enabled={self.optimization_enabled}")
            result = await self.parse_tweet_element_optimized(tweet_element)
            self.logger.info(f"ğŸ”§ ä¼˜åŒ–ç‰ˆæœ¬è§£æç»“æœ: {result is not None}")
            return result
        
        try:
            tweet_data = {}
            
            # æ£€æŸ¥å…ƒç´ æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            try:
                await tweet_element.is_visible()
            except Exception:
                self.logger.warning("æ¨æ–‡å…ƒç´ å·²å¤±æ•ˆï¼Œè·³è¿‡è§£æ")
                return None
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–ç”¨æˆ·å
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨
                username_selectors = [
                    '[data-testid="User-Name"] a',
                    '[data-testid="User-Names"] a',
                    'a[href^="/"][role="link"]'
                ]
                
                username_element = None
                for selector in username_selectors:
                    try:
                        username_element = await tweet_element.query_selector(selector)
                        if username_element:
                            break
                    except Exception:
                        continue
                
                if username_element:
                    username_href = await username_element.get_attribute('href')
                    if username_href and '/' in username_href:
                        tweet_data['username'] = username_href.split('/')[-1]
            except Exception as e:
                self.logger.debug(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–æ¨æ–‡å†…å®¹
            try:
                content_selectors = [
                    '[data-testid="tweetText"]',
                    '[data-testid="tweetText"] span',
                    'div[lang] span',
                    'div[dir="auto"] span',
                    'div[dir="ltr"] span',
                    'div[dir="rtl"] span',
                    '[lang] span',
                    'span[dir="auto"]',
                    'div[data-testid="tweetText"] > span',
                    'article div[lang] span',
                    'article span[dir]'
                ]
                
                content_text = ''
                # å°è¯•å¤šç§æ–¹å¼æå–å†…å®¹
                for selector in content_selectors:
                    try:
                        content_elements = await tweet_element.query_selector_all(selector)
                        if content_elements:
                            # æ”¶é›†æ‰€æœ‰æ–‡æœ¬å†…å®¹
                            text_parts = []
                            for elem in content_elements:
                                text = await elem.inner_text()
                                if text and text.strip():
                                    text_parts.append(text.strip())
                            
                            if text_parts:
                                content_text = ' '.join(text_parts)
                                break
                    except Exception:
                        continue
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œå°è¯•è·å–æ•´ä¸ªæ¨æ–‡åŒºåŸŸçš„æ–‡æœ¬
                if not content_text:
                    try:
                        # å°è¯•ä»æ•´ä¸ªæ¨æ–‡å…ƒç´ ä¸­æå–æ–‡æœ¬ï¼Œä½†æ’é™¤ç”¨æˆ·åã€æ—¶é—´ç­‰
                        all_text = await tweet_element.inner_text()
                        if all_text:
                            # ç®€å•è¿‡æ»¤ï¼Œç§»é™¤æ˜æ˜¾çš„éå†…å®¹æ–‡æœ¬
                            lines = all_text.split('\n')
                            filtered_lines = []
                            for line in lines:
                                line = line.strip()
                                # è·³è¿‡ç©ºè¡Œã€ç”¨æˆ·åã€æ—¶é—´æˆ³ç­‰
                                if (line and 
                                    not line.startswith('@') and 
                                    not line.endswith('h') and 
                                    not line.endswith('m') and 
                                    not line.endswith('s') and
                                    not line.isdigit() and
                                    len(line) > 3):
                                    filtered_lines.append(line)
                            
                            if filtered_lines:
                                content_text = ' '.join(filtered_lines[:3])  # å–å‰3è¡Œä½œä¸ºå†…å®¹
                    except Exception:
                        pass
                
                if content_text and content_text.strip():
                    tweet_data['content'] = content_text.strip()
                    
            except Exception as e:
                self.logger.debug(f"æå–æ¨æ–‡å†…å®¹å¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–å‘å¸ƒæ—¶é—´
            try:
                time_element = await tweet_element.query_selector('time')
                if time_element:
                    datetime_attr = await time_element.get_attribute('datetime')
                    if datetime_attr:
                        tweet_data['publish_time'] = datetime_attr
            except Exception as e:
                self.logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–æ¨æ–‡é“¾æ¥
            try:
                link_elements = await tweet_element.query_selector_all('a[href*="/status/"]')
                if link_elements:
                    href = await link_elements[0].get_attribute('href')
                    if href:
                        if href.startswith('/'):
                            tweet_data['link'] = f"https://x.com{href}"
                        else:
                            tweet_data['link'] = href
            except Exception as e:
                self.logger.debug(f"æå–æ¨æ–‡é“¾æ¥å¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–äº’åŠ¨æ•°æ®
            interaction_data = {'likes': 0, 'comments': 0, 'retweets': 0}
            
            # ç‚¹èµæ•°
            try:
                like_selectors = [
                    '[data-testid="like"]',
                    '[aria-label*="like"]',
                    '[aria-label*="Like"]'
                ]
                
                for selector in like_selectors:
                    try:
                        like_element = await tweet_element.query_selector(selector)
                        if like_element:
                            like_text = await like_element.get_attribute('aria-label') or ''
                            if like_text:
                                interaction_data['likes'] = self.extract_number(like_text)
                                break
                    except Exception:
                        continue
            except Exception as e:
                self.logger.debug(f"æå–ç‚¹èµæ•°å¤±è´¥: {e}")
            
            # è¯„è®ºæ•°
            try:
                reply_selectors = [
                    '[data-testid="reply"]',
                    '[aria-label*="repl"]',
                    '[aria-label*="Reply"]'
                ]
                
                for selector in reply_selectors:
                    try:
                        reply_element = await tweet_element.query_selector(selector)
                        if reply_element:
                            reply_text = await reply_element.get_attribute('aria-label') or ''
                            if reply_text:
                                interaction_data['comments'] = self.extract_number(reply_text)
                                break
                    except Exception:
                        continue
            except Exception as e:
                self.logger.debug(f"æå–è¯„è®ºæ•°å¤±è´¥: {e}")
            
            # è½¬å‘æ•°
            try:
                retweet_selectors = [
                    '[data-testid="retweet"]',
                    '[aria-label*="retweet"]',
                    '[aria-label*="Retweet"]'
                ]
                
                for selector in retweet_selectors:
                    try:
                        retweet_element = await tweet_element.query_selector(selector)
                        if retweet_element:
                            retweet_text = await retweet_element.get_attribute('aria-label') or ''
                            if retweet_text:
                                interaction_data['retweets'] = self.extract_number(retweet_text)
                                break
                    except Exception:
                        continue
            except Exception as e:
                self.logger.debug(f"æå–è½¬å‘æ•°å¤±è´¥: {e}")
            
            # åˆå¹¶äº’åŠ¨æ•°æ®
            tweet_data.update(interaction_data)
            
            # æå–åª’ä½“å†…å®¹ï¼ˆå›¾ç‰‡å’Œè§†é¢‘ï¼‰
            try:
                media_content = await self.extract_media_content(tweet_element)
                if media_content:
                    tweet_data['media'] = media_content
                    self.logger.debug(f"æå–åˆ°åª’ä½“å†…å®¹: {len(media_content.get('images', []))} å¼ å›¾ç‰‡, {len(media_content.get('videos', []))} ä¸ªè§†é¢‘")
            except Exception as e:
                self.logger.debug(f"æå–åª’ä½“å†…å®¹å¤±è´¥: {e}")
            
            # è®¾ç½®é»˜è®¤å€¼
            tweet_data.setdefault('username', 'unknown')
            tweet_data.setdefault('content', '')
            tweet_data.setdefault('publish_time', datetime.now().isoformat())
            tweet_data.setdefault('link', '')
            tweet_data.setdefault('likes', 0)
            tweet_data.setdefault('comments', 0)
            tweet_data.setdefault('retweets', 0)
            tweet_data.setdefault('media', {'images': [], 'videos': []})
            
            # è¯†åˆ«å¸–å­ç±»å‹
            try:
                tweet_data['post_type'] = self.identify_tweet_type(tweet_data)
                self.logger.debug(f"è¯†åˆ«å¸–å­ç±»å‹: {tweet_data['post_type']}")
            except Exception as e:
                self.logger.debug(f"è¯†åˆ«å¸–å­ç±»å‹å¤±è´¥: {e}")
                tweet_data['post_type'] = 'æ–‡å­—'
            
            # éªŒè¯æ¨æ–‡æ•°æ®çš„æœ‰æ•ˆæ€§ - è¿›ä¸€æ­¥æ”¾å®½éªŒè¯æ¡ä»¶
            # åªè¦æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶å°±è®¤ä¸ºæ˜¯æœ‰æ•ˆæ¨æ–‡ï¼š
            # 1. æœ‰ç”¨æˆ·å
            # 2. æœ‰å†…å®¹
            # 3. æœ‰é“¾æ¥
            # 4. æœ‰åª’ä½“å†…å®¹
            # 5. æœ‰äº’åŠ¨æ•°æ®ï¼ˆç‚¹èµã€è¯„è®ºã€è½¬å‘ï¼‰
            has_username = tweet_data.get('username', '') and tweet_data.get('username', '') != 'unknown'
            has_content = tweet_data.get('content', '').strip()
            has_link = tweet_data.get('link', '').strip()
            has_media = (tweet_data.get('media', {}).get('images', []) or 
                        tweet_data.get('media', {}).get('videos', []))
            has_interactions = (tweet_data.get('likes', 0) > 0 or 
                              tweet_data.get('comments', 0) > 0 or 
                              tweet_data.get('retweets', 0) > 0)
            
            # åªè¦æœ‰ä»»ä½•ä¸€é¡¹æœ‰æ•ˆä¿¡æ¯å°±ä¿ç•™æ¨æ–‡
            if not (has_username or has_content or has_link or has_media or has_interactions):
                self.logger.debug(f"æ¨æ–‡æ•°æ®æ— æ•ˆï¼Œè·³è¿‡ - ç”¨æˆ·å: {tweet_data.get('username', 'None')}, å†…å®¹é•¿åº¦: {len(tweet_data.get('content', ''))}, é“¾æ¥: {bool(tweet_data.get('link', ''))}, åª’ä½“: {bool(has_media)}, äº’åŠ¨: {bool(has_interactions)}")
                return None
            
            self.logger.debug(f"æ¨æ–‡éªŒè¯é€šè¿‡ - ç”¨æˆ·å: {tweet_data.get('username', 'None')}, å†…å®¹é•¿åº¦: {len(tweet_data.get('content', ''))}, é“¾æ¥: {bool(tweet_data.get('link', ''))}")
            
            return tweet_data
            
        except Exception as e:
            self.logger.error(f"è§£ææ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return None
    
    async def scrape_tweets(self, max_tweets: int = 10, enable_enhanced: bool = False) -> List[Dict[str, Any]]:
        """
        æŠ“å–å½“å‰é¡µé¢çš„æ¨æ–‡æ•°æ®
        
        Args:
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        if enable_enhanced:
            return await self.enhanced_tweet_scraping(max_tweets)
            
        try:
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹
            await self.ensure_page_focus()
            
            # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šæ¨æ–‡
            await self.scroll_and_load_tweets(max_tweets)
            
            # è·å–æ‰€æœ‰æ¨æ–‡å…ƒç´ 
            tweet_elements = await self.page.query_selector_all('[data-testid="tweet"]')
            self.logger.info(f"æ‰¾åˆ° {len(tweet_elements)} ä¸ªæ¨æ–‡å…ƒç´ ")
            
            tweets_data = []
            
            for i, tweet_element in enumerate(tweet_elements[:max_tweets]):
                # æ¯éš”å‡ æ¡æ¨æ–‡æ£€æŸ¥é¡µé¢ç„¦ç‚¹
                if i % 5 == 0:
                    await self.ensure_page_focus()
                
                # æ¨¡æ‹Ÿäººå·¥æŸ¥çœ‹æ¨æ–‡çš„è¡Œä¸º
                if self.behavior_simulator and i % 3 == 0:  # æ¯3æ¡æ¨æ–‡æ¨¡æ‹Ÿä¸€æ¬¡äº¤äº’
                    await self.behavior_simulator.simulate_tweet_interaction(tweet_element)
                
                self.logger.debug(f"å¼€å§‹è§£æç¬¬ {i+1} ä¸ªæ¨æ–‡å…ƒç´ ")
                tweet_data = await self.parse_tweet_element(tweet_element)
                if tweet_data:
                    tweets_data.append(tweet_data)
                    self.logger.info(f"æˆåŠŸè§£æç¬¬ {i+1} æ¡æ¨æ–‡: @{tweet_data['username']}")
                else:
                    self.logger.debug(f"ç¬¬ {i+1} ä¸ªæ¨æ–‡å…ƒç´ è§£æå¤±è´¥æˆ–è¿”å›None")
                
                # æ¨¡æ‹Ÿäººå·¥é˜…è¯»é—´éš”ï¼ˆæé€Ÿæ¨¡å¼ï¼‰
                if self.behavior_simulator:
                    await asyncio.sleep(0.05)  # æçŸ­ç­‰å¾…æ—¶é—´
            
            self.logger.info(f"æ€»å…±æŠ“å–åˆ° {len(tweets_data)} æ¡æ¨æ–‡")
            return tweets_data
            
        except Exception as e:
            self.logger.error(f"æŠ“å–æ¨æ–‡å¤±è´¥: {e}")
            return []
    
    async def scrape_user_tweets(self, username: str, max_tweets: int = 10, enable_enhanced: bool = False) -> List[Dict[str, Any]]:
        """
        æŠ“å–æŒ‡å®šç”¨æˆ·çš„æ¨æ–‡
        
        Args:
            username: Twitter ç”¨æˆ·å
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹
            await self.ensure_page_focus()
            
            await self.navigate_to_profile(username)
            
            # ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œé¡µé¢æ¢ç´¢
            if self.behavior_simulator:
                self.logger.info(f"å¼€å§‹æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆ @{username} çš„é¡µé¢")
                await self.behavior_simulator.simulate_page_exploration()
                
                # æ¨¡æ‹Ÿé˜…è¯»è¡Œä¸º
                await self.behavior_simulator.simulate_reading_behavior()
            else:
                await asyncio.sleep(0.8)  # æé€Ÿå›é€€ç­‰å¾…
            
            tweets = await self.scrape_tweets(max_tweets, enable_enhanced)
            
            # æ¨¡æ‹Ÿç”¨æˆ·ä¼šè¯ç»“æŸè¡Œä¸º
            if self.behavior_simulator:
                await self.behavior_simulator.simulate_natural_browsing([f"https://twitter.com/{username}"])
            
            # ä¸ºæ¯æ¡æ¨æ–‡æ·»åŠ æ¥æºä¿¡æ¯
            for tweet in tweets:
                tweet['source'] = f'@{username}'
                tweet['source_type'] = 'user_profile'
            
            return tweets
            
        except Exception as e:
            self.logger.error(f"æŠ“å–ç”¨æˆ· @{username} çš„æ¨æ–‡å¤±è´¥: {e}")
            return []
    
    async def scrape_keyword_tweets(self, keyword: str, max_tweets: int = 10, enable_enhanced: bool = False) -> List[Dict[str, Any]]:
        """
        æŠ“å–åŒ…å«æŒ‡å®šå…³é”®è¯çš„æ¨æ–‡
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹
            await self.ensure_page_focus()
            
            await self.search_tweets(keyword)
            
            # ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œæœç´¢é¡µé¢æ¢ç´¢
            if self.behavior_simulator:
                self.logger.info(f"å¼€å§‹æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆå…³é”®è¯ '{keyword}' çš„æœç´¢ç»“æœ")
                await self.behavior_simulator.simulate_page_exploration()
                
                # æ¨¡æ‹Ÿé˜…è¯»æœç´¢ç»“æœ
                await self.behavior_simulator.simulate_reading_behavior()
            else:
                await asyncio.sleep(0.8)  # æé€Ÿå›é€€ç­‰å¾…
            
            tweets = await self.scrape_tweets(max_tweets, enable_enhanced)
            
            # æ¨¡æ‹Ÿæœç´¢ä¼šè¯ç»“æŸè¡Œä¸º
            if self.behavior_simulator:
                await self.behavior_simulator.simulate_natural_browsing([f"https://twitter.com/search?q={keyword}"])
            
            # ä¸ºæ¯æ¡æ¨æ–‡æ·»åŠ æ¥æºä¿¡æ¯
            for tweet in tweets:
                tweet['source'] = keyword
                tweet['source_type'] = 'keyword_search'
            
            return tweets
            
        except Exception as e:
            self.logger.error(f"æŠ“å–å…³é”®è¯ '{keyword}' çš„æ¨æ–‡å¤±è´¥: {e}")
            return []
    
    async def scrape_user_keyword_tweets(self, username: str, keyword: str, max_tweets: int = 10, enable_enhanced: bool = False) -> List[Dict[str, Any]]:
        """
        åœ¨æŒ‡å®šç”¨æˆ·ä¸‹æœç´¢åŒ…å«å…³é”®è¯çš„æ¨æ–‡
        
        Args:
            username: Twitter ç”¨æˆ·å
            keyword: æœç´¢å…³é”®è¯
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹
            await self.ensure_page_focus()
            
            self.logger.info(f"å¼€å§‹åœ¨ç”¨æˆ· @{username} ä¸‹æœç´¢å…³é”®è¯ '{keyword}'")
            
            # æ„å»ºç”¨æˆ·ç‰¹å®šçš„æœç´¢URL
            import urllib.parse
            encoded_keyword = urllib.parse.quote(keyword)
            encoded_username = urllib.parse.quote(username)
            # ä½¿ç”¨ from:username è¯­æ³•åœ¨ç‰¹å®šç”¨æˆ·ä¸‹æœç´¢
            search_query = f"from:{encoded_username} {encoded_keyword}"
            search_url = f'https://x.com/search?q={urllib.parse.quote(search_query)}&src=typed_query&f=live'
            
            self.logger.info(f"ç”¨æˆ·å…³é”®è¯æœç´¢URL: {search_url}")
            
            # å¯¼èˆªåˆ°æœç´¢é¡µé¢
            await self.page.goto(search_url, timeout=BROWSER_CONFIG['timeout'])
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            try:
                await self.page.wait_for_load_state('domcontentloaded', timeout=10000)
                self.logger.info("DOMå†…å®¹åŠ è½½å®Œæˆ")
            except Exception as load_error:
                self.logger.warning(f"DOMåŠ è½½è¶…æ—¶: {load_error}")
            
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹åå†ç­‰å¾…æœç´¢ç»“æœ
            await self.ensure_page_focus()
            
            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½ï¼ˆæé€Ÿæ¨¡å¼ï¼‰
            self.logger.info("ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")
            await asyncio.sleep(0.8)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
            try:
                await self.page.wait_for_selector('[data-testid="tweet"]', timeout=10000)
                self.logger.info("æ‰¾åˆ°æ¨æ–‡å…ƒç´ ")
            except Exception:
                self.logger.warning("æœªæ‰¾åˆ°æ¨æ–‡å…ƒç´ ï¼Œå¯èƒ½æ²¡æœ‰æœç´¢ç»“æœ")
            
            # ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œæœç´¢é¡µé¢æ¢ç´¢
            if self.behavior_simulator:
                self.logger.info(f"å¼€å§‹æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆ @{username} ä¸‹å…³é”®è¯ '{keyword}' çš„æœç´¢ç»“æœ")
                await self.behavior_simulator.simulate_page_exploration()
                await self.behavior_simulator.simulate_reading_behavior()
            else:
                await asyncio.sleep(0.8)  # æé€Ÿå›é€€ç­‰å¾…
            
            tweets = await self.scrape_tweets(max_tweets, enable_enhanced)
            
            # ä¸ºæ¯æ¡æ¨æ–‡æ·»åŠ æ¥æºä¿¡æ¯
            for tweet in tweets:
                tweet['source'] = f"@{username} + {keyword}"
                tweet['source_type'] = 'user_keyword_search'
                tweet['target_username'] = username
                tweet['target_keyword'] = keyword
            
            self.logger.info(f"åœ¨ç”¨æˆ· @{username} ä¸‹æœç´¢å…³é”®è¯ '{keyword}' å®Œæˆï¼Œè·å¾— {len(tweets)} æ¡æ¨æ–‡")
            return tweets
            
        except Exception as e:
            self.logger.error(f"åœ¨ç”¨æˆ· @{username} ä¸‹æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
            return []
    
    async def scrape_tweet_details(self, tweet_url: str) -> Dict[str, Any]:
        """
        æŠ“å–æ¨æ–‡è¯¦æƒ…é¡µçš„å®Œæ•´å†…å®¹
        
        Args:
            tweet_url: æ¨æ–‡è¯¦æƒ…é¡µURL
            
        Returns:
            åŒ…å«å®Œæ•´å†…å®¹çš„æ¨æ–‡æ•°æ®
        """
        try:
            self.logger.info(f"å¼€å§‹æŠ“å–æ¨æ–‡è¯¦æƒ…: {tweet_url}")
            
            # å¯¼èˆªåˆ°æ¨æ–‡è¯¦æƒ…é¡µ
            await self.page.goto(tweet_url, timeout=BROWSER_CONFIG['navigation_timeout'])
            await self.page.wait_for_load_state('domcontentloaded', timeout=30000)
            
            # ç­‰å¾…å†…å®¹åŠ è½½ï¼ˆæé€Ÿæ¨¡å¼ï¼‰
            await asyncio.sleep(1)
            
            # æŠ“å–å®Œæ•´çš„æ¨æ–‡å†…å®¹
            full_content = await self.extract_full_tweet_content()
            
            # æŠ“å–å¤šåª’ä½“å†…å®¹
            media_content = await self.extract_media_content()
            
            # æŠ“å–æ¨æ–‡çº¿ç¨‹
            thread_tweets = await self.extract_tweet_thread()
            
            # æŠ“å–å¼•ç”¨æ¨æ–‡
            quoted_tweet = await self.extract_quoted_tweet()
            
            return {
                'full_content': full_content,
                'media': media_content,
                'thread': thread_tweets,
                'quoted_tweet': quoted_tweet,
                'has_detailed_content': True
            }
            
        except Exception as e:
            self.logger.error(f"æŠ“å–æ¨æ–‡è¯¦æƒ…å¤±è´¥: {e}")
            return {'has_detailed_content': False}
    
    async def extract_full_tweet_content(self) -> str:
        """
        æå–æ¨æ–‡çš„å®Œæ•´å†…å®¹æ–‡æœ¬
        
        Returns:
            å®Œæ•´çš„æ¨æ–‡å†…å®¹
        """
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–å®Œæ•´å†…å®¹
            content_selectors = [
                '[data-testid="tweetText"]',
                '[lang] span',
                'div[dir="auto"] span',
                'article div[lang] span'
            ]
            
            full_content = ""
            for selector in content_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        content_parts = []
                        for element in elements:
                            text = await element.inner_text()
                            if text and text.strip():
                                content_parts.append(text.strip())
                        
                        if content_parts:
                            full_content = ' '.join(content_parts)
                            break
                except Exception:
                    continue
            
            return full_content
            
        except Exception as e:
            self.logger.error(f"æå–å®Œæ•´æ¨æ–‡å†…å®¹å¤±è´¥: {e}")
            return ""
    
    def identify_tweet_type(self, tweet_data: Dict[str, Any]) -> str:
        """
        è¯†åˆ«æ¨æ–‡çš„ç±»å‹
        
        Args:
            tweet_data: æ¨æ–‡æ•°æ®
            
        Returns:
            æ¨æ–‡ç±»å‹: 'æ–‡å­—', 'å›¾æ–‡', 'è§†é¢‘', 'åµŒå¥—è§†é¢‘', 'åµŒå¥—å›¾æ–‡'
        """
        try:
            content = tweet_data.get('content', '')
            media = tweet_data.get('media', {})
            quoted_tweet = tweet_data.get('quoted_tweet')
            
            # è·å–åª’ä½“æ•°é‡
            image_count = len(media.get('images', []))
            video_count = len(media.get('videos', []))
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼•ç”¨æ¨æ–‡ï¼ˆåµŒå¥—ï¼‰
            has_quoted = quoted_tweet is not None
            
            # åˆ¤æ–­ç±»å‹
            if has_quoted:
                # åµŒå¥—æ¨æ–‡
                if video_count > 0:
                    return 'åµŒå¥—è§†é¢‘'
                elif image_count > 0:
                    return 'åµŒå¥—å›¾æ–‡'
                else:
                    return 'åµŒå¥—æ–‡å­—'
            else:
                # æ™®é€šæ¨æ–‡
                if video_count > 0:
                    return 'è§†é¢‘'
                elif image_count > 0:
                    return 'å›¾æ–‡'
                else:
                    return 'æ–‡å­—'
                    
        except Exception as e:
            self.logger.error(f"è¯†åˆ«æ¨æ–‡ç±»å‹å¤±è´¥: {e}")
            return 'æ–‡å­—'  # é»˜è®¤è¿”å›æ–‡å­—ç±»å‹
    
    async def extract_media_content(self, tweet_element=None) -> Dict[str, List[Dict[str, Any]]]:
        """
        æå–æ¨æ–‡ä¸­çš„å›¾ç‰‡ã€è§†é¢‘ç­‰å¤šåª’ä½“å†…å®¹
        
        Args:
            tweet_element: æ¨æ–‡å…ƒç´ ï¼Œå¦‚æœä¸ºNoneåˆ™åœ¨æ•´ä¸ªé¡µé¢ä¸­æœç´¢
        
        Returns:
            åŒ…å«imageså’Œvideosåˆ—è¡¨çš„å­—å…¸
        """
        media_content = {'images': [], 'videos': []}
        
        try:
            # ç¡®å®šæœç´¢èŒƒå›´
            search_context = tweet_element if tweet_element else self.page
            
            # æŠ“å–å›¾ç‰‡
            image_selectors = [
                '[data-testid="tweetPhoto"] img',
                'img[src*="pbs.twimg.com"]',
                'div[data-testid="tweetPhoto"] img',
                '[data-testid="card.layoutLarge.media"] img'
            ]
            
            for selector in image_selectors:
                try:
                    images = await search_context.query_selector_all(selector)
                    for img in images:
                        src = await img.get_attribute('src')
                        alt = await img.get_attribute('alt') or ''
                        if src and ('pbs.twimg.com' in src or 'twimg.com' in src):
                            media_content['images'].append({
                                'type': 'image',
                                'url': src,
                                'description': alt,
                                'original_url': src.replace(':small', ':orig').replace(':medium', ':orig').replace(':large', ':orig')
                            })
                    if images:
                        break
                except Exception:
                    continue
            
            # æŠ“å–è§†é¢‘
            video_selectors = [
                'video',
                '[data-testid="videoPlayer"] video',
                'div[data-testid="videoComponent"] video',
                '[data-testid="previewInterstitial"] video'
            ]
            
            for selector in video_selectors:
                try:
                    videos = await search_context.query_selector_all(selector)
                    for video in videos:
                        poster = await video.get_attribute('poster')
                        src = await video.get_attribute('src')
                        media_content['videos'].append({
                            'type': 'video',
                            'poster': poster,
                            'url': src,
                            'description': 'è§†é¢‘å†…å®¹'
                        })
                    if videos:
                        break
                except Exception:
                    continue
            
            # æŠ“å–GIFï¼ˆä½œä¸ºç‰¹æ®Šçš„å›¾ç‰‡ç±»å‹ï¼‰
            gif_selectors = [
                'img[src*="video.twimg.com"]',
                '[data-testid="tweetPhoto"] img[src*=".gif"]'
            ]
            
            for selector in gif_selectors:
                try:
                    gifs = await search_context.query_selector_all(selector)
                    for gif in gifs:
                        src = await gif.get_attribute('src')
                        alt = await gif.get_attribute('alt') or ''
                        if src:
                            media_content['images'].append({
                                'type': 'gif',
                                'url': src,
                                'description': alt
                            })
                    if gifs:
                        break
                except Exception:
                    continue
            
            total_media = len(media_content['images']) + len(media_content['videos'])
            if total_media > 0:
                self.logger.debug(f"æå–åˆ° {len(media_content['images'])} å¼ å›¾ç‰‡, {len(media_content['videos'])} ä¸ªè§†é¢‘")
            
            return media_content
            
        except Exception as e:
            self.logger.error(f"æå–å¤šåª’ä½“å†…å®¹å¤±è´¥: {e}")
            return {'images': [], 'videos': []}
    
    async def extract_tweet_thread(self) -> List[Dict[str, Any]]:
        """
        æå–æ¨æ–‡çº¿ç¨‹ï¼ˆè¿ç»­çš„ç›¸å…³æ¨æ–‡ï¼‰
        
        Returns:
            æ¨æ–‡çº¿ç¨‹åˆ—è¡¨
        """
        thread_tweets = []
        
        try:
            # æŸ¥æ‰¾çº¿ç¨‹ä¸­çš„å…¶ä»–æ¨æ–‡
            thread_selectors = [
                'article[data-testid="tweet"]',
                '[data-testid="tweet"]'
            ]
            
            for selector in thread_selectors:
                try:
                    tweet_elements = await self.page.query_selector_all(selector)
                    
                    # å¦‚æœæ‰¾åˆ°å¤šæ¡æ¨æ–‡ï¼Œè¯´æ˜å¯èƒ½æ˜¯çº¿ç¨‹
                    if len(tweet_elements) > 1:
                        for i, element in enumerate(tweet_elements[1:], 1):  # è·³è¿‡ç¬¬ä¸€æ¡ï¼ˆä¸»æ¨æ–‡ï¼‰
                            thread_tweet = await self.parse_tweet_element(element)
                            if thread_tweet:
                                thread_tweet['thread_position'] = i
                                thread_tweets.append(thread_tweet)
                    
                    break
                except Exception:
                    continue
            
            self.logger.info(f"æå–åˆ° {len(thread_tweets)} æ¡çº¿ç¨‹æ¨æ–‡")
            return thread_tweets
            
        except Exception as e:
            self.logger.error(f"æå–æ¨æ–‡çº¿ç¨‹å¤±è´¥: {e}")
            return []
    
    async def extract_quoted_tweet(self) -> Optional[Dict[str, Any]]:
        """
        æå–å¼•ç”¨çš„æ¨æ–‡å†…å®¹
        
        Returns:
            å¼•ç”¨æ¨æ–‡æ•°æ®
        """
        try:
            # æŸ¥æ‰¾å¼•ç”¨æ¨æ–‡
            quoted_selectors = [
                '[data-testid="quoteTweet"]',
                'div[role="blockquote"]',
                'blockquote'
            ]
            
            for selector in quoted_selectors:
                try:
                    quoted_element = await self.page.query_selector(selector)
                    if quoted_element:
                        quoted_tweet = await self.parse_tweet_element(quoted_element)
                        if quoted_tweet:
                            quoted_tweet['is_quoted'] = True
                            self.logger.info("æå–åˆ°å¼•ç”¨æ¨æ–‡")
                            return quoted_tweet
                except Exception:
                    continue
            
            return None
            
        except Exception as e:
            self.logger.error(f"æå–å¼•ç”¨æ¨æ–‡å¤±è´¥: {e}")
            return None
    
    def is_content_truncated(self, content: str) -> bool:
        """
        æ£€æµ‹å†…å®¹æ˜¯å¦è¢«æˆªæ–­
        
        Args:
            content: æ¨æ–‡å†…å®¹
            
        Returns:
            æ˜¯å¦è¢«æˆªæ–­
        """
        truncation_indicators = [
            content.endswith('...'),
            content.endswith('â€¦'),
            len(content) > 280 and not content.endswith('.'),
            'æ˜¾ç¤ºæ›´å¤š' in content,
            'Show more' in content,
            'æŸ¥çœ‹æ›´å¤š' in content,
            'See more' in content
        ]
        return any(truncation_indicators)
    
    def has_rich_media(self, tweet: Dict[str, Any]) -> bool:
        """
        æ£€æµ‹æ˜¯å¦åŒ…å«ä¸°å¯Œåª’ä½“å†…å®¹
        
        Args:
            tweet: æ¨æ–‡æ•°æ®
            
        Returns:
            æ˜¯å¦åŒ…å«åª’ä½“å†…å®¹
        """
        content = tweet.get('content', '')
        media_indicators = [
            'ğŸ“·' in content,
            'ğŸ¥' in content,
            'ğŸ“¸' in content,
            'ğŸ¬' in content,
            'å›¾ç‰‡' in content,
            'è§†é¢‘' in content,
            'ç…§ç‰‡' in content,
            'photo' in content.lower(),
            'video' in content.lower(),
            'image' in content.lower(),
            tweet.get('media_count', 0) > 0
        ]
        return any(media_indicators)
    
    def is_thread_content(self, content: str) -> bool:
        """
        è¯†åˆ«æ¨æ–‡çº¿ç¨‹
        
        Args:
            content: æ¨æ–‡å†…å®¹
            
        Returns:
            æ˜¯å¦ä¸ºçº¿ç¨‹å†…å®¹
        """
        import re
        
        thread_patterns = [
            r'\d+/\d+',  # 1/5, 2/10 ç­‰æ ¼å¼
            r'\d+/',     # 1/, 2/ ç­‰æ ¼å¼
            r'\(\d+/\d+\)',  # (1/5) æ ¼å¼
        ]
        
        thread_indicators = [
            'ğŸ§µ', 'ğŸ“', 'ğŸ‘‡', 'â¬‡ï¸',
            'Thread', 'thread', 'çº¿ç¨‹',
            'æ¥ä¸‹æ¥', 'ç»§ç»­', 'continued',
            'ä¸‹ä¸€æ¡', 'next tweet'
        ]
        
        # æ£€æŸ¥æ­£åˆ™æ¨¡å¼
        for pattern in thread_patterns:
            if re.search(pattern, content):
                return True
        
        # æ£€æŸ¥çº¿ç¨‹æŒ‡ç¤ºè¯
        return any(indicator in content for indicator in thread_indicators)
    
    def is_high_value_content(self, tweet: Dict[str, Any]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºé«˜ä»·å€¼å†…å®¹
        
        Args:
            tweet: æ¨æ–‡æ•°æ®
            
        Returns:
            æ˜¯å¦ä¸ºé«˜ä»·å€¼å†…å®¹
        """
        content = tweet.get('content', '').lower()
        
        # è®¡ç®—äº’åŠ¨åˆ†æ•°
        engagement_score = (
            tweet.get('likes', 0) * 1 +
            tweet.get('retweets', 0) * 2 +
            tweet.get('comments', 0) * 3
        )
        
        # é«˜ä»·å€¼å…³é”®è¯
        value_keywords = [
            'æ•™ç¨‹', 'æ–¹æ³•', 'æŠ€å·§', 'ç»éªŒ', 'åˆ†äº«', 'å¹²è´§',
            'tutorial', 'guide', 'tips', 'how to', 'method',
            'æ”»ç•¥', 'ç§˜ç±', 'å¿ƒå¾—', 'æ€»ç»“', 'å¤ç›˜',
            'å·¥å…·', 'èµ„æº', 'æ¨è', 'tools', 'resources'
        ]
        
        value_indicators = [
            engagement_score > 50,  # é«˜äº’åŠ¨
            len(tweet.get('content', '')) > 200,  # é•¿å†…å®¹
            any(keyword in content for keyword in value_keywords),
            tweet.get('comments', 0) > 20,  # é«˜è¯„è®ºæ•°
            tweet.get('retweets', 0) > 10   # é«˜è½¬å‘æ•°
        ]
        
        return any(value_indicators)
    
    def get_scraping_strategy(self, account_type: str = 'general', follower_count: int = 0) -> Dict[str, Any]:
        """
        æ ¹æ®è´¦å·ç±»å‹è·å–æŠ“å–ç­–ç•¥
        
        Args:
            account_type: è´¦å·ç±»å‹
            follower_count: ç²‰ä¸æ•°é‡
            
        Returns:
            æŠ“å–ç­–ç•¥é…ç½®
        """
        strategies = {
            'æŠ€æœ¯åšä¸»': {
                'detail_threshold': 0.4,
                'engagement_threshold': 30,
                'content_length_threshold': 150,
                'priority_keywords': ['ä»£ç ', 'æŠ€æœ¯', 'å¼€å‘', 'code', 'tech']
            },
            'è¥é”€åšä¸»': {
                'detail_threshold': 0.6,
                'engagement_threshold': 15,
                'content_length_threshold': 100,
                'priority_keywords': ['è¥é”€', 'æ¨å¹¿', 'å˜ç°', 'marketing']
            },
            'æŠ•èµ„åšä¸»': {
                'detail_threshold': 0.5,
                'engagement_threshold': 25,
                'content_length_threshold': 120,
                'priority_keywords': ['æŠ•èµ„', 'ç†è´¢', 'è‚¡ç¥¨', 'investment']
            },
            'general': {
                'detail_threshold': 0.3,
                'engagement_threshold': 20,
                'content_length_threshold': 150,
                'priority_keywords': []
            }
        }
        
        # æ ¹æ®ç²‰ä¸æ•°è°ƒæ•´ç­–ç•¥
        strategy = strategies.get(account_type, strategies['general']).copy()
        if follower_count > 100000:  # å¤§Vè´¦å·
            strategy['detail_threshold'] *= 1.2
            strategy['engagement_threshold'] *= 0.8
        
        return strategy
    
    def should_scrape_details(self, tweet: Dict[str, Any], account_type: str = 'general') -> bool:
        """
        æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦æŠ“å–æ¨æ–‡è¯¦æƒ…
        
        Args:
            tweet: åŸºç¡€æ¨æ–‡æ•°æ®
            account_type: è´¦å·ç±»å‹
            
        Returns:
            æ˜¯å¦éœ€è¦æ·±åº¦æŠ“å–
        """
        content = tweet.get('content', '')
        strategy = self.get_scraping_strategy(account_type)
        
        # å¿…é¡»æ·±åº¦æŠ“å–çš„æ¡ä»¶
        must_scrape = [
            self.is_content_truncated(content),  # å†…å®¹è¢«æˆªæ–­
            self.is_thread_content(content),     # çº¿ç¨‹å†…å®¹
            self.has_rich_media(tweet)           # åŒ…å«å¤šåª’ä½“
        ]
        
        if any(must_scrape):
            return True
        
        # é€‰æ‹©æ€§æ·±åº¦æŠ“å–çš„æ¡ä»¶
        optional_scrape = [
            self.is_high_value_content(tweet),   # é«˜ä»·å€¼å†…å®¹
            len(content) > strategy['content_length_threshold'],  # é•¿å†…å®¹
            tweet.get('comments', 0) > strategy['engagement_threshold'],  # é«˜äº’åŠ¨
            any(keyword in content.lower() for keyword in strategy['priority_keywords'])  # ä¼˜å…ˆå…³é”®è¯
        ]
        
        # æ ¹æ®ç­–ç•¥é˜ˆå€¼å†³å®š
        score = sum(optional_scrape) / len(optional_scrape) if optional_scrape else 0
        return score >= strategy['detail_threshold']
    
    async def enhanced_tweet_scraping(self, max_tweets: int = 10, enable_details: bool = True) -> List[Dict[str, Any]]:
        """
        å¢å¼ºçš„æ¨æ–‡æŠ“å–ï¼ŒåŒ…å«è¯¦æƒ…é¡µå†…å®¹
        
        Args:
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_details: æ˜¯å¦å¯ç”¨è¯¦æƒ…é¡µæŠ“å–
            
        Returns:
            å¢å¼ºçš„æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            # å…ˆæŠ“å–æ—¶é—´çº¿ä¸Šçš„åŸºæœ¬æ¨æ–‡
            basic_tweets = await self.scrape_tweets(max_tweets)
            
            if not enable_details:
                return basic_tweets
            
            enhanced_tweets = []
            details_scraped = 0
            max_details = min(5, max_tweets // 2)  # é™åˆ¶è¯¦æƒ…é¡µæŠ“å–æ•°é‡
            
            for i, tweet in enumerate(basic_tweets):
                enhanced_tweet = tweet.copy()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·±åº¦æŠ“å–
                if (details_scraped < max_details and 
                    tweet.get('link') and 
                    self.should_scrape_details(tweet, 'general')):
                    
                    self.logger.info(f"å¯¹ç¬¬ {i+1} æ¡æ¨æ–‡è¿›è¡Œè¯¦æƒ…æŠ“å–")
                    
                    try:
                        # æŠ“å–è¯¦æƒ…é¡µå†…å®¹
                        details = await self.scrape_tweet_details(tweet['link'])
                        enhanced_tweet.update(details)
                        details_scraped += 1
                        
                        # æ¨¡æ‹Ÿäººå·¥æµè§ˆé—´éš”
                        if self.behavior_simulator:
                            await self.behavior_simulator.random_pause(2, 5)
                        else:
                            await asyncio.sleep(3)
                            
                    except Exception as e:
                        self.logger.warning(f"è¯¦æƒ…æŠ“å–å¤±è´¥: {e}")
                        enhanced_tweet['detail_error'] = str(e)
                
                enhanced_tweets.append(enhanced_tweet)
            
            self.logger.info(f"å¢å¼ºæŠ“å–å®Œæˆï¼Œå…±å¤„ç† {len(enhanced_tweets)} æ¡æ¨æ–‡ï¼Œå…¶ä¸­ {details_scraped} æ¡è¿›è¡Œäº†è¯¦æƒ…æŠ“å–")
            return enhanced_tweets
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºæ¨æ–‡æŠ“å–å¤±è´¥: {e}")
            return basic_tweets if 'basic_tweets' in locals() else []
    
    def parse_tweets(self, tweet_elements: List[Any]) -> List[Dict[str, Any]]:
        """
        è§£ææ¨æ–‡å…ƒç´ åˆ—è¡¨
        
        Args:
            tweet_elements: æ¨æ–‡DOMå…ƒç´ åˆ—è¡¨
            
        Returns:
            è§£æåçš„æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        parsed_tweets = []
        
        for element in tweet_elements:
            try:
                tweet_data = self.extract_tweet_data(element)
                if tweet_data:
                    parsed_tweets.append(tweet_data)
            except Exception as e:
                self.logger.warning(f"è§£ææ¨æ–‡å¤±è´¥: {e}")
                continue
        
        return parsed_tweets
    
    def parse_user_profile(self, profile_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æç”¨æˆ·ä¸ªäººèµ„æ–™æ•°æ®
        
        Args:
            profile_data: åŸå§‹ä¸ªäººèµ„æ–™æ•°æ®
            
        Returns:
            è§£æåçš„ç”¨æˆ·èµ„æ–™
        """
        try:
            parsed_profile = {
                'username': profile_data.get('username', ''),
                'display_name': profile_data.get('display_name', ''),
                'bio': profile_data.get('bio', ''),
                'followers_count': self._parse_count(profile_data.get('followers', '0')),
                'following_count': self._parse_count(profile_data.get('following', '0')),
                'tweets_count': self._parse_count(profile_data.get('tweets', '0')),
                'verified': profile_data.get('verified', False),
                'profile_image': profile_data.get('profile_image', ''),
                'banner_image': profile_data.get('banner_image', ''),
                'location': profile_data.get('location', ''),
                'website': profile_data.get('website', ''),
                'joined_date': profile_data.get('joined_date', ''),
                'parsed_at': datetime.now().isoformat()
            }
            
            return parsed_profile
            
        except Exception as e:
            self.logger.error(f"è§£æç”¨æˆ·èµ„æ–™å¤±è´¥: {e}")
            return {}
    
    def extract_tweet_data(self, tweet_element: Any) -> Dict[str, Any]:
        """
        ä»æ¨æ–‡å…ƒç´ ä¸­æå–æ•°æ®
        
        Args:
            tweet_element: æ¨æ–‡DOMå…ƒç´ 
            
        Returns:
            æå–çš„æ¨æ–‡æ•°æ®
        """
        try:
            # è¿™é‡Œåº”è¯¥æ ¹æ®å®é™…çš„DOMç»“æ„æ¥æå–æ•°æ®
            # ç”±äºè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ–¹æ³•ï¼Œè¿”å›åŸºæœ¬ç»“æ„
            tweet_data = {
                'id': '',
                'username': '',
                'display_name': '',
                'content': '',
                'timestamp': '',
                'likes': 0,
                'retweets': 0,
                'comments': 0,
                'link': '',
                'images': [],
                'videos': [],
                'extracted_at': datetime.now().isoformat()
            }
            
            # å¦‚æœtweet_elementæ˜¯å­—å…¸ç±»å‹ï¼ˆå·²è§£æçš„æ•°æ®ï¼‰
            if isinstance(tweet_element, dict):
                tweet_data.update(tweet_element)
            
            return tweet_data
            
        except Exception as e:
            self.logger.error(f"æå–æ¨æ–‡æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def _parse_count(self, count_str: str) -> int:
        """
        è§£æè®¡æ•°å­—ç¬¦ä¸²ï¼ˆå¦‚1.2K, 5.6Mç­‰ï¼‰
        
        Args:
            count_str: è®¡æ•°å­—ç¬¦ä¸²
            
        Returns:
            è§£æåçš„æ•°å­—
        """
        try:
            if not count_str or count_str == '-':
                return 0
            
            count_str = count_str.strip().replace(',', '')
            
            if count_str.endswith('K'):
                return int(float(count_str[:-1]) * 1000)
            elif count_str.endswith('M'):
                return int(float(count_str[:-1]) * 1000000)
            elif count_str.endswith('B'):
                return int(float(count_str[:-1]) * 1000000000)
            else:
                return int(count_str)
                
        except (ValueError, AttributeError):
            return 0

    # ==================== ä¼˜åŒ–åŠŸèƒ½æ–¹æ³• ====================
    
    def clean_tweet_content(self, content: str) -> str:
        """ä¼˜åŒ–çš„æ¨æ–‡å†…å®¹æ¸…ç†"""
        if not content:
            return ""
        
        # ç¼“å­˜æ£€æŸ¥
        if content in self.content_cache:
            return self.content_cache[content]
        
        original_content = content
        
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content.strip())
        
        # å»é™¤é‡å¤çš„ç”¨æˆ·åæ¨¡å¼ (å¦‚: "Elon Musk Elon Musk @elonmusk")
        content = re.sub(r'(\w+\s+\w+)\s+\1', r'\1', content)
        
        # å»é™¤é‡å¤çš„æ•°å­—æ¨¡å¼ (å¦‚: "4,8K 4,8K 4,8K")
        content = re.sub(r'(\d+[,.]?\d*[KMB]?)\s+\1(\s+\1)*', r'\1', content)
        
        # å»é™¤é‡å¤çš„ç¬¦å·æ¨¡å¼
        content = re.sub(r'(Â·\s*)+', 'Â· ', content)
        
        # å»é™¤æœ«å°¾çš„ç»Ÿè®¡æ•°æ®æ¨¡å¼
        content = re.sub(r'\s*Â·\s*[\d,KMB.\s]+$', '', content)
        
        # å»é™¤å¼€å¤´çš„ç”¨æˆ·åé‡å¤
        content = re.sub(r'^(@?\w+\s+){2,}', '', content)
        
        # å»é™¤å¤šä½™çš„ç‚¹å’Œç©ºæ ¼
        content = re.sub(r'\s*Â·\s*$', '', content)
        
        # å»é™¤è¿ç»­çš„é‡å¤è¯æ±‡
        words = content.split()
        cleaned_words = []
        for i, word in enumerate(words):
            if i == 0 or word != words[i-1]:
                cleaned_words.append(word)
        
        cleaned_content = ' '.join(cleaned_words).strip()
        
        # ç¼“å­˜ç»“æœ
        self.content_cache[original_content] = cleaned_content
        
        return cleaned_content
    
    def extract_tweet_id(self, tweet_link: str) -> str:
        """ä»æ¨æ–‡é“¾æ¥ä¸­æå–ID"""
        try:
            if '/status/' in tweet_link:
                return tweet_link.split('/status/')[-1].split('?')[0]
            return ''
        except Exception:
            return ''
    
    def is_duplicate_tweet(self, tweet_link: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ¨æ–‡"""
        tweet_id = self.extract_tweet_id(tweet_link)
        if tweet_id:
            if tweet_id in self.seen_tweet_ids:
                return True
            self.seen_tweet_ids.add(tweet_id)
        return False
    
    def parse_engagement_number(self, num_str: str) -> int:
        """è§£æäº’åŠ¨æ•°å­— (å¦‚: 1.2K -> 1200)"""
        try:
            if not num_str:
                return 0
            
            num_str = num_str.replace(',', '').replace(' ', '')
            
            if num_str.endswith('K'):
                return int(float(num_str[:-1]) * 1000)
            elif num_str.endswith('M'):
                return int(float(num_str[:-1]) * 1000000)
            elif num_str.endswith('B'):
                return int(float(num_str[:-1]) * 1000000000)
            else:
                return int(num_str)
        except (ValueError, IndexError):
            return 0
    
    async def scroll_and_load_tweets_optimized(self, target_tweets: int = 15, max_attempts: int = 20) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„æ»šåŠ¨ç­–ç•¥"""
        self.logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥ï¼Œç›®æ ‡: {target_tweets} æ¡æ¨æ–‡")
        
        scroll_attempt = 0
        stagnant_rounds = 0
        last_unique_count = 0
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°
        base_scroll_distance = 800
        base_wait_time = 1.5
        
        while scroll_attempt < max_attempts:
            # è·å–å½“å‰æ¨æ–‡æ•°é‡
            try:
                await self.page.wait_for_selector('[data-testid="tweet"]', timeout=5000)
                current_elements = await self.page.query_selector_all('[data-testid="tweet"]')
                current_unique_tweets = len(self.seen_tweet_ids)
            except Exception:
                current_elements = []
                current_unique_tweets = 0
            
            self.logger.debug(f"ğŸ“Š æ»šåŠ¨å°è¯• {scroll_attempt + 1}/{max_attempts}ï¼Œå½“å‰å”¯ä¸€æ¨æ–‡: {current_unique_tweets}/{target_tweets}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if current_unique_tweets >= target_tweets:
                self.logger.info(f"ğŸ¯ è¾¾åˆ°ç›®æ ‡æ¨æ–‡æ•°é‡: {current_unique_tweets}")
                break
            
            # æ£€æŸ¥åœæ»æƒ…å†µ
            if current_unique_tweets == last_unique_count:
                stagnant_rounds += 1
            else:
                stagnant_rounds = 0
            
            last_unique_count = current_unique_tweets
            
            # æ ¹æ®åœæ»æƒ…å†µè°ƒæ•´ç­–ç•¥
            if stagnant_rounds >= 3:
                # æ¿€è¿›æ¨¡å¼ï¼šå¢åŠ æ»šåŠ¨è·ç¦»ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
                scroll_distance = base_scroll_distance * 2
                wait_time = base_wait_time * 0.7
                self.logger.debug(f"ğŸ”¥ æ¿€è¿›æ¨¡å¼ï¼šæ»šåŠ¨è·ç¦» {scroll_distance}ï¼Œç­‰å¾…æ—¶é—´ {wait_time:.1f}s")
            elif stagnant_rounds >= 6:
                # è¶…æ¿€è¿›æ¨¡å¼ï¼šå¤§å¹…æ»šåŠ¨
                scroll_distance = base_scroll_distance * 3
                wait_time = base_wait_time * 0.5
                self.logger.debug(f"âš¡ è¶…æ¿€è¿›æ¨¡å¼ï¼šæ»šåŠ¨è·ç¦» {scroll_distance}ï¼Œç­‰å¾…æ—¶é—´ {wait_time:.1f}s")
            else:
                # æ­£å¸¸æ¨¡å¼
                scroll_distance = base_scroll_distance
                wait_time = base_wait_time
            
            # æ‰§è¡Œæ»šåŠ¨
            try:
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹
                await self.page.evaluate('window.focus()')
                
                # å¹³æ»‘æ»šåŠ¨
                current_scroll = await self.page.evaluate('window.pageYOffset')
                await self.page.evaluate(f'''
                    window.scrollTo({{
                        top: {current_scroll + scroll_distance},
                        behavior: 'smooth'
                    }});
                ''')
                
                # ç­‰å¾…æ»šåŠ¨å®Œæˆå’Œå†…å®¹åŠ è½½
                await asyncio.sleep(wait_time)
                
                # æ£€æŸ¥æ–°æ¨æ–‡å¹¶æ›´æ–°seen_tweet_ids
                await self.update_seen_tweets()
                
            except Exception as e:
                self.logger.warning(f"æ»šåŠ¨å¤±è´¥: {e}")
                await asyncio.sleep(1)
            
            # å¦‚æœè¿ç»­å¤šè½®æ— æ–°å†…å®¹ï¼Œè€ƒè™‘åˆ·æ–°
            if stagnant_rounds >= 8:
                self.logger.info("ğŸ”„ é•¿æ—¶é—´æ— æ–°å†…å®¹ï¼Œå°è¯•åˆ·æ–°é¡µé¢")
                try:
                    await self.page.reload(wait_until='domcontentloaded')
                    await asyncio.sleep(3)
                    stagnant_rounds = 0
                    # é‡æ–°æ”¶é›†å·²è§è¿‡çš„æ¨æ–‡ID
                    await self.rebuild_seen_tweets()
                except Exception as e:
                    self.logger.warning(f"é¡µé¢åˆ·æ–°å¤±è´¥: {e}")
            
            scroll_attempt += 1
        
        final_unique_tweets = len(self.seen_tweet_ids)
        self.logger.info(f"ğŸ“Š æ»šåŠ¨ç­–ç•¥å®Œæˆ: {final_unique_tweets} æ¡å”¯ä¸€æ¨æ–‡ï¼Œ{scroll_attempt} æ¬¡æ»šåŠ¨")
        
        return {
            'final_tweet_count': final_unique_tweets,
            'scroll_attempts': scroll_attempt,
            'target_reached': final_unique_tweets >= target_tweets,
            'efficiency': final_unique_tweets / max(scroll_attempt, 1)
        }
    
    async def update_seen_tweets(self):
        """æ›´æ–°å·²è§æ¨æ–‡IDé›†åˆ"""
        try:
            current_elements = await self.page.query_selector_all('[data-testid="tweet"]')
            
            for element in current_elements:
                try:
                    link_element = await element.query_selector('a[href*="/status/"]')
                    if link_element:
                        href = await link_element.get_attribute('href')
                        if href:
                            tweet_id = self.extract_tweet_id(href)
                            if tweet_id:
                                self.seen_tweet_ids.add(tweet_id)
                except Exception:
                    continue
        except Exception as e:
            self.logger.debug(f"æ›´æ–°å·²è§æ¨æ–‡IDå¤±è´¥: {e}")
    
    async def rebuild_seen_tweets(self):
        """é‡æ–°æ„å»ºå·²è§æ¨æ–‡IDé›†åˆ"""
        try:
            self.seen_tweet_ids.clear()
            await self.update_seen_tweets()
            self.logger.debug(f"é‡å»ºå·²è§æ¨æ–‡IDé›†åˆ: {len(self.seen_tweet_ids)} æ¡")
        except Exception as e:
            self.logger.warning(f"é‡å»ºå·²è§æ¨æ–‡IDå¤±è´¥: {e}")
    
    async def parse_tweet_element_optimized(self, element) -> Optional[Dict[str, Any]]:
        """ä¼˜åŒ–çš„æ¨æ–‡å…ƒç´ è§£æ"""
        try:
            self.logger.info("ğŸ”§ å¼€å§‹è§£ææ¨æ–‡å…ƒç´ ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
            # æå–ç”¨æˆ·å
            username = await self.extract_clean_username(element)
            
            # æå–å†…å®¹
            content = await self.extract_clean_content(element)
            
            # æå–é“¾æ¥
            link = await self.extract_tweet_link(element)
            
            # æ£€æŸ¥é‡å¤
            if self.is_duplicate_tweet(link):
                return None
            
            # æå–æ—¶é—´
            publish_time = await self.extract_publish_time(element)
            
            # æå–äº’åŠ¨æ•°æ®
            engagement = await self.extract_engagement_data(element)
            
            # æå–åª’ä½“å†…å®¹
            media = await self.extract_media_content(element)
            
            # ç¡®å®šå¸–å­ç±»å‹
            post_type = 'çº¯æ–‡æœ¬'
            if media['images']:
                post_type = 'å›¾æ–‡'
            elif media['videos']:
                post_type = 'è§†é¢‘'
            
            # æ„å»ºæ¨æ–‡æ•°æ®
            tweet_data = {
                'username': username,
                'content': content,
                'publish_time': publish_time,
                'link': link,
                'likes': engagement['likes'],
                'comments': engagement['comments'],
                'retweets': engagement['retweets'],
                'media': media,
                'post_type': post_type
            }
            
            # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§ - æ”¾å®½éªŒè¯æ¡ä»¶
            # åªè¦æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶å°±è®¤ä¸ºæ˜¯æœ‰æ•ˆæ¨æ–‡ï¼š
            # 1. æœ‰æœ‰æ•ˆç”¨æˆ·å
            # 2. æœ‰å†…å®¹ï¼ˆå³ä½¿æ˜¯é»˜è®¤å†…å®¹ï¼‰
            # 3. æœ‰é“¾æ¥
            # 4. æœ‰åª’ä½“å†…å®¹
            # 5. æœ‰äº’åŠ¨æ•°æ®
            has_username = username and username != 'unknown'
            has_content = content and content != 'No content available'
            has_link = link and link.strip()
            has_media = media['images'] or media['videos']
            has_engagement = engagement['likes'] > 0 or engagement['comments'] > 0 or engagement['retweets'] > 0
            
            # è¯¦ç»†è®°å½•éªŒè¯ä¿¡æ¯
            self.logger.info(f"ğŸ”§ æ¨æ–‡éªŒè¯è¯¦æƒ…:")
            self.logger.info(f"   - ç”¨æˆ·å: '{username}' (æœ‰æ•ˆ: {has_username})")
            self.logger.info(f"   - å†…å®¹: '{content[:50] if content else 'None'}...' (æœ‰æ•ˆ: {has_content})")
            self.logger.info(f"   - é“¾æ¥: '{link}' (æœ‰æ•ˆ: {has_link})")
            self.logger.info(f"   - åª’ä½“: {media} (æœ‰æ•ˆ: {has_media})")
            self.logger.info(f"   - äº’åŠ¨: {engagement} (æœ‰æ•ˆ: {has_engagement})")
            
            # åªè¦æœ‰ä»»ä½•ä¸€é¡¹æœ‰æ•ˆä¿¡æ¯å°±ä¿ç•™æ¨æ–‡
            if has_username or has_content or has_link or has_media or has_engagement:
                self.logger.info(f"ğŸ”§ æ¨æ–‡éªŒè¯é€šè¿‡ - è‡³å°‘æœ‰ä¸€é¡¹æœ‰æ•ˆä¿¡æ¯")
                return tweet_data
            
            self.logger.info(f"ğŸ”§ æ¨æ–‡æ•°æ®æ— æ•ˆï¼Œè·³è¿‡ - æ‰€æœ‰éªŒè¯é¡¹éƒ½å¤±è´¥")
            return None
            
        except Exception as e:
            self.logger.debug(f"è§£ææ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return None
    
    async def extract_clean_username(self, element) -> str:
        """æå–å¹²å‡€çš„ç”¨æˆ·å"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            username_selectors = [
                '[data-testid="User-Name"] [dir="ltr"]',
                '[data-testid="User-Name"] span',
                'a[href^="/"][role="link"] span',
                '[dir="ltr"] span'
            ]
            
            for selector in username_selectors:
                username_element = await element.query_selector(selector)
                if username_element:
                    username = await username_element.text_content()
                    username = username.strip()
                    self.logger.debug(f"æ‰¾åˆ°ç”¨æˆ·ååŸå§‹æ–‡æœ¬: '{username}' (é€‰æ‹©å™¨: {selector})")
                    # æ¸…ç†ç”¨æˆ·å
                    username = re.sub(r'^@', '', username)
                    username = re.sub(r'\s.*', '', username)  # åªä¿ç•™ç¬¬ä¸€ä¸ªè¯
                    if username and not re.match(r'^\d+[KMB]?$', username):
                        self.logger.debug(f"æå–åˆ°æœ‰æ•ˆç”¨æˆ·å: '{username}'")
                        return username
            
            # ä»é“¾æ¥ä¸­æå–ç”¨æˆ·å
            link_element = await element.query_selector('a[href^="/"][role="link"]')
            if link_element:
                href = await link_element.get_attribute('href')
                if href:
                    self.logger.debug(f"æ‰¾åˆ°é“¾æ¥: {href}")
                    match = re.match(r'^/([^/]+)', href)
                    if match:
                        username = match.group(1)
                        self.logger.debug(f"ä»é“¾æ¥æå–ç”¨æˆ·å: '{username}'")
                        return username
            
            self.logger.debug("æœªæ‰¾åˆ°æœ‰æ•ˆç”¨æˆ·åï¼Œè¿”å› 'unknown'")
            return 'unknown'
            
        except Exception as e:
            self.logger.debug(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
            return 'unknown'
    
    async def extract_clean_content(self, element) -> str:
        """æå–å¹²å‡€çš„æ¨æ–‡å†…å®¹"""
        try:
            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                '[data-testid="tweetText"]',
                '[lang] span',
                'div[dir="auto"] span'
            ]
            
            content_parts = []
            
            for selector in content_selectors:
                content_elements = await element.query_selector_all(selector)
                self.logger.debug(f"é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(content_elements)} ä¸ªå†…å®¹å…ƒç´ ")
                for content_element in content_elements:
                    text = await content_element.text_content()
                    text = text.strip()
                    if text and text not in content_parts:
                        self.logger.debug(f"æ‰¾åˆ°å†…å®¹ç‰‡æ®µ: '{text[:50]}...'")
                        content_parts.append(text)
            
            # åˆå¹¶å†…å®¹
            raw_content = ' '.join(content_parts)
            self.logger.debug(f"åˆå¹¶åçš„åŸå§‹å†…å®¹: '{raw_content[:100]}...'")
            
            # æ¸…ç†å†…å®¹
            clean_content = self.clean_tweet_content(raw_content)
            self.logger.debug(f"æ¸…ç†åçš„å†…å®¹: '{clean_content[:100]}...'")
            
            result = clean_content if clean_content else 'No content available'
            self.logger.debug(f"æœ€ç»ˆè¿”å›å†…å®¹: '{result[:50]}...'")
            return result
            
        except Exception as e:
            self.logger.debug(f"æå–æ¨æ–‡å†…å®¹å¤±è´¥: {e}")
            return 'No content available'
    
    async def extract_tweet_link(self, element) -> str:
        """æå–æ¨æ–‡é“¾æ¥"""
        try:
            link_element = await element.query_selector('a[href*="/status/"]')
            if link_element:
                href = await link_element.get_attribute('href')
                if href:
                    if href.startswith('/'):
                        return f'https://x.com{href}'
                    else:
                        return href
            return ''
        except Exception:
            return ''
    
    async def extract_publish_time(self, element) -> str:
        """æå–å‘å¸ƒæ—¶é—´"""
        try:
            time_element = await element.query_selector('time')
            if time_element:
                datetime_attr = await time_element.get_attribute('datetime')
                if datetime_attr:
                    return datetime_attr
            return ''
        except Exception:
            return ''
    
    async def extract_engagement_data(self, element) -> Dict[str, int]:
        """æå–äº’åŠ¨æ•°æ®"""
        engagement = {'likes': 0, 'comments': 0, 'retweets': 0}
        
        try:
            # æŸ¥æ‰¾äº’åŠ¨æ•°æ®
            engagement_selectors = {
                'likes': ['[data-testid="like"]', '[aria-label*="like"]'],
                'comments': ['[data-testid="reply"]', '[aria-label*="repl"]'],
                'retweets': ['[data-testid="retweet"]', '[aria-label*="repost"]']
            }
            
            for metric, selectors in engagement_selectors.items():
                for selector in selectors:
                    metric_element = await element.query_selector(selector)
                    if metric_element:
                        # æŸ¥æ‰¾æ•°å­—
                        text = await metric_element.text_content()
                        numbers = re.findall(r'[\d,]+[KMB]?', text)
                        if numbers:
                            engagement[metric] = self.parse_engagement_number(numbers[0])
                            break
            
            return engagement
            
        except Exception as e:
            self.logger.debug(f"æå–äº’åŠ¨æ•°æ®å¤±è´¥: {e}")
            return engagement
    
    async def extract_media_content(self, element) -> Dict[str, List[Dict]]:
        """æå–åª’ä½“å†…å®¹"""
        media = {'images': [], 'videos': []}
        
        try:
            # æå–å›¾ç‰‡
            img_elements = await element.query_selector_all('img[src*="pbs.twimg.com"]')
            for img in img_elements:
                src = await img.get_attribute('src')
                alt = await img.get_attribute('alt') or 'Image'
                if src:
                    media['images'].append({
                        'type': 'image',
                        'url': src,
                        'description': alt,
                        'original_url': src
                    })
            
            # æå–è§†é¢‘
            video_elements = await element.query_selector_all('video, [data-testid="videoPlayer"]')
            for video in video_elements:
                poster = await video.get_attribute('poster')
                if poster:
                    media['videos'].append({
                        'type': 'video',
                        'poster': poster,
                        'description': 'Video content'
                    })
            
            return media
            
        except Exception as e:
            self.logger.debug(f"æå–åª’ä½“å†…å®¹å¤±è´¥: {e}")
            return media
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æ‘˜è¦"""
        return {
            'unique_tweets_processed': len(self.seen_tweet_ids),
            'content_cache_size': len(self.content_cache),
            'optimization_enabled': self.optimization_enabled,
            'optimizations_applied': [
                'intelligent_scroll_strategy',
                'content_deduplication',
                'enhanced_text_cleaning',
                'improved_element_extraction',
                'engagement_data_parsing',
                'media_content_detection'
            ]
        }
    
    def enable_optimizations(self):
        """å¯ç”¨ä¼˜åŒ–åŠŸèƒ½"""
        self.optimization_enabled = True
        self.logger.info("âœ… ä¼˜åŒ–åŠŸèƒ½å·²å¯ç”¨")
    
    def disable_optimizations(self):
        """ç¦ç”¨ä¼˜åŒ–åŠŸèƒ½"""
        self.optimization_enabled = False
        self.logger.info("âŒ ä¼˜åŒ–åŠŸèƒ½å·²ç¦ç”¨")
    
    def clear_optimization_cache(self):
        """æ¸…ç†ä¼˜åŒ–ç¼“å­˜"""
        self.seen_tweet_ids.clear()
        self.content_cache.clear()
        self.logger.info("ğŸ§¹ ä¼˜åŒ–ç¼“å­˜å·²æ¸…ç†")

    async def close(self):
        """
        å…³é—­æµè§ˆå™¨è¿æ¥
        """
        try:
            if self.browser:
                await self.browser.close()
                self.logger.info("æµè§ˆå™¨è¿æ¥å·²å…³é—­")
        except Exception as e:
            self.logger.error(f"å…³é—­æµè§ˆå™¨è¿æ¥å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    async def main():
        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        
        # è¿™é‡Œéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„è°ƒè¯•ç«¯å£
        debug_port = "ws://127.0.0.1:9222"
        
        parser = TwitterParser(debug_port)
        
        try:
            await parser.connect_browser()
            await parser.navigate_to_twitter()
            
            # æŠ“å–æŒ‡å®šç”¨æˆ·çš„æ¨æ–‡
            tweets = await parser.scrape_user_tweets('elonmusk', 5)
            
            # æ¨æ–‡æŠ“å–å®Œæˆ
            pass
                
        except Exception as e:
            pass
        finally:
            await parser.close()
    
    asyncio.run(main())