# -*- coding: utf-8 -*-
"""
Twitter è§£æå™¨
ä½¿ç”¨ Playwright æ§åˆ¶æµè§ˆå™¨å¹¶æŠ“å–æ¨æ–‡æ•°æ®
"""

import asyncio
import logging
import re
from typing import List, Dict, Any, Optional
from datetime import datetime
from playwright.async_api import async_playwright, Browser, Page
from config import BROWSER_CONFIG, TWITTER_TARGETS, FILTER_CONFIG
from human_behavior_simulator import HumanBehaviorSimulator

class TwitterParser:
    def __init__(self, debug_port: str):
        self.debug_port = debug_port
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.behavior_simulator = None
        self.logger = logging.getLogger(__name__)
        
    async def connect_browser(self):
        """
        è¿æ¥åˆ° AdsPower æµè§ˆå™¨
        """
        try:
            self.logger.info("å¼€å§‹å¯åŠ¨ Playwright...")
            playwright = await async_playwright().start()
            
            self.logger.info(f"å¼€å§‹è¿æ¥åˆ°æµè§ˆå™¨è°ƒè¯•ç«¯å£: {self.debug_port}")
            # è¿æ¥åˆ°ç°æœ‰çš„æµè§ˆå™¨å®ä¾‹
            self.browser = await playwright.chromium.connect_over_cdp(self.debug_port)
            self.logger.info("æˆåŠŸè¿æ¥åˆ°æµè§ˆå™¨å®ä¾‹")
            
            # è·å–ç°æœ‰ä¸Šä¸‹æ–‡å’Œé¡µé¢
            contexts = self.browser.contexts
            self.logger.info(f"æ‰¾åˆ° {len(contexts)} ä¸ªæµè§ˆå™¨ä¸Šä¸‹æ–‡")
            
            if contexts:
                context = contexts[0]
                pages = context.pages
                self.logger.info(f"åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ° {len(pages)} ä¸ªé¡µé¢")
                
                if pages:
                    # ä½¿ç”¨ç°æœ‰é¡µé¢
                    self.page = pages[0]
                    try:
                        current_url = self.page.url
                        self.logger.info(f"ä½¿ç”¨ç°æœ‰é¡µé¢ï¼Œå½“å‰URL: {current_url}")
                    except Exception as url_error:
                        self.logger.warning(f"æ— æ³•è·å–é¡µé¢URL: {url_error}")
                        # åˆ›å»ºæ–°é¡µé¢ä½œä¸ºå¤‡é€‰
                        self.page = await context.new_page()
                        self.logger.info("åˆ›å»ºæ–°é¡µé¢ä½œä¸ºå¤‡é€‰")
                else:
                    # åœ¨ç°æœ‰ä¸Šä¸‹æ–‡ä¸­åˆ›å»ºæ–°é¡µé¢
                    self.page = await context.new_page()
                    self.logger.info("åœ¨ç°æœ‰ä¸Šä¸‹æ–‡ä¸­åˆ›å»ºæ–°é¡µé¢")
            else:
                # åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œè®¾ç½®ç”¨æˆ·ä»£ç†å’Œå…¶ä»–é€‰é¡¹
                self.logger.info("åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡...")
                context = await self.browser.new_context(
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080},
                    extra_http_headers={
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
                    }
                )
                self.page = await context.new_page()
                self.logger.info("åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡å’Œé¡µé¢")
            
            # è®¾ç½®é¡µé¢é»˜è®¤è¶…æ—¶æ—¶é—´
            self.page.set_default_timeout(BROWSER_CONFIG['timeout'])
            self.logger.info(f"è®¾ç½®é¡µé¢è¶…æ—¶æ—¶é—´: {BROWSER_CONFIG['timeout']}ms")
            
            # è®¾ç½®é¡µé¢å¯¼èˆªè¶…æ—¶æ—¶é—´
            self.page.set_default_navigation_timeout(BROWSER_CONFIG['navigation_timeout'])
            self.logger.info(f"è®¾ç½®å¯¼èˆªè¶…æ—¶æ—¶é—´: {BROWSER_CONFIG['navigation_timeout']}ms")
            
            # åˆå§‹åŒ–äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨
            self.behavior_simulator = HumanBehaviorSimulator(self.page)
            self.logger.info("äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨åˆå§‹åŒ–å®Œæˆ")
            
            self.logger.info("æˆåŠŸè¿æ¥åˆ°æµè§ˆå™¨")
            
        except Exception as e:
            self.logger.error(f"è¿æ¥æµè§ˆå™¨å¤±è´¥: {e}")
            raise
    
    async def navigate_to_twitter(self, max_retries: int = 3):
        """
        å¯¼èˆªåˆ° Twitter ä¸»é¡µ
        
        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for attempt in range(max_retries):
            try:
                current_url = self.page.url
                self.logger.info(f"å½“å‰é¡µé¢URL: {current_url}")
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ x.com
                if 'x.com' not in current_url:
                    self.logger.info(f"é¡µé¢ä¸åœ¨ x.comï¼Œå¼€å§‹å¯¼èˆª... (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
                    
                    # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´
                    await self.page.goto('https://x.com', timeout=BROWSER_CONFIG['navigation_timeout'])
                    
                    # åˆ†æ­¥ç­‰å¾…åŠ è½½
                    try:
                        await self.page.wait_for_load_state('domcontentloaded', timeout=30000)
                        await self.page.wait_for_load_state('networkidle', timeout=BROWSER_CONFIG['load_state_timeout'])
                    except Exception as load_error:
                        self.logger.warning(f"ç­‰å¾…åŠ è½½çŠ¶æ€å¤±è´¥: {load_error}ï¼Œç»§ç»­å°è¯•")
                    
                    self.logger.info("æˆåŠŸå¯¼èˆªåˆ° X (Twitter)")
                else:
                    self.logger.info("é¡µé¢å·²ç»åœ¨ X (Twitter)ï¼Œè·³è¿‡å¯¼èˆª")
                
                # æµ‹è¯•é¡µé¢äº¤äº’ - æ‰§è¡Œå‡ æ¬¡ä¸‹æ‹‰æ“ä½œ
                self.logger.info("å¼€å§‹æµ‹è¯•é¡µé¢ä¸‹æ‹‰åŠŸèƒ½...")
                for i in range(3):
                    # å‘ä¸‹æ»šåŠ¨
                    await self.page.evaluate('window.scrollBy(0, 500)')
                    await asyncio.sleep(1)
                    self.logger.info(f"æ‰§è¡Œç¬¬ {i+1} æ¬¡ä¸‹æ‹‰")
                
                # æ»šåŠ¨å›é¡¶éƒ¨
                await self.page.evaluate('window.scrollTo(0, 0)')
                await asyncio.sleep(1)
                self.logger.info("é¡µé¢ä¸‹æ‹‰æµ‹è¯•å®Œæˆï¼Œå·²æ»šåŠ¨å›é¡¶éƒ¨")
                return
                
            except Exception as e:
                self.logger.warning(f"ç¬¬{attempt + 1}æ¬¡å¯¼èˆªå°è¯•å¤±è´¥: {e}")
                
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5
                    self.logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    self.logger.error(f"å¯¼èˆªåˆ° X (Twitter) å¤±è´¥ï¼Œå·²å°è¯•{max_retries}æ¬¡")
                    raise Exception(f"å¯¼èˆªåˆ° X (Twitter) å¤±è´¥: {e}")
    
    async def navigate_to_profile(self, username: str, max_retries: int = 3):
        """
        å¯¼èˆªåˆ°æŒ‡å®šç”¨æˆ·çš„ä¸ªäººèµ„æ–™é¡µé¢
        
        Args:
            username: Twitter ç”¨æˆ·åï¼ˆä¸åŒ…å«@ç¬¦å·ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        profile_url = f'https://x.com/{username}'
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"å°è¯•å¯¼èˆªåˆ° @{username} çš„ä¸ªäººèµ„æ–™é¡µé¢ (ç¬¬{attempt + 1}æ¬¡)")
                
                # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´è¿›è¡Œå¯¼èˆª
                await self.page.goto(profile_url, timeout=BROWSER_CONFIG['navigation_timeout'])
                
                # åˆ†æ­¥ç­‰å¾…åŠ è½½çŠ¶æ€
                try:
                    await self.page.wait_for_load_state('domcontentloaded', timeout=30000)
                    self.logger.info("DOMå†…å®¹åŠ è½½å®Œæˆ")
                    
                    await self.page.wait_for_load_state('networkidle', timeout=BROWSER_CONFIG['load_state_timeout'])
                    self.logger.info("ç½‘ç»œç©ºé—²çŠ¶æ€è¾¾åˆ°")
                except Exception as load_error:
                    self.logger.warning(f"ç­‰å¾…åŠ è½½çŠ¶æ€å¤±è´¥: {load_error}ï¼Œç»§ç»­å°è¯•")
                
                # é¢å¤–ç­‰å¾…æ—¶é—´ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
                await asyncio.sleep(BROWSER_CONFIG['wait_time'])
                
                # éªŒè¯é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
                current_url = self.page.url
                if username.lower() in current_url.lower():
                    self.logger.info(f"æˆåŠŸå¯¼èˆªåˆ° @{username} çš„ä¸ªäººèµ„æ–™é¡µé¢")
                    return
                else:
                    raise Exception(f"é¡µé¢URLä¸åŒ¹é…ï¼ŒæœŸæœ›åŒ…å«'{username}'ï¼Œå®é™…ä¸º'{current_url}'")
                
            except Exception as e:
                self.logger.warning(f"ç¬¬{attempt + 1}æ¬¡å¯¼èˆªå°è¯•å¤±è´¥: {e}")
                
                if attempt < max_retries - 1:
                    # ç­‰å¾…åé‡è¯•
                    wait_time = (attempt + 1) * 5  # é€’å¢ç­‰å¾…æ—¶é—´
                    self.logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                    self.logger.error(f"å¯¼èˆªåˆ° @{username} ä¸ªäººèµ„æ–™é¡µé¢å¤±è´¥ï¼Œå·²å°è¯•{max_retries}æ¬¡")
                    raise Exception(f"å¯¼èˆªåˆ° @{username} ä¸ªäººèµ„æ–™é¡µé¢å¤±è´¥: {e}")
    
    async def search_tweets(self, keyword: str, max_retries: int = 2):
        """
        æœç´¢åŒ…å«æŒ‡å®šå…³é”®è¯çš„æ¨æ–‡
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for attempt in range(max_retries + 1):
            try:
                self.logger.info(f"å¼€å§‹æœç´¢å…³é”®è¯ '{keyword}' (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
                
                # æ„å»ºæœç´¢URLï¼Œä½¿ç”¨URLç¼–ç 
                import urllib.parse
                encoded_keyword = urllib.parse.quote(keyword)
                search_url = f'https://x.com/search?q={encoded_keyword}&src=typed_query&f=live'
                self.logger.info(f"æœç´¢URL: {search_url}")
                
                # å¯¼èˆªåˆ°æœç´¢é¡µé¢
                self.logger.info("æ­£åœ¨å¯¼èˆªåˆ°æœç´¢é¡µé¢...")
                await self.page.goto(search_url, timeout=BROWSER_CONFIG['timeout'])
                self.logger.info("å¯¼èˆªå®Œæˆï¼Œç­‰å¾…é¡µé¢åŠ è½½...")
                
                # ç®€åŒ–ç­‰å¾…ç­–ç•¥
                try:
                    await self.page.wait_for_load_state('domcontentloaded', timeout=10000)
                    self.logger.info("DOMå†…å®¹åŠ è½½å®Œæˆ")
                except Exception as load_error:
                    self.logger.warning(f"DOMåŠ è½½è¶…æ—¶: {load_error}")
                
                # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
                self.logger.info("ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")
                await asyncio.sleep(5)  # ä½¿ç”¨å›ºå®šç­‰å¾…æ—¶é—´
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
                try:
                    # ç­‰å¾…æ¨æ–‡å…ƒç´ å‡ºç°
                    await self.page.wait_for_selector('[data-testid="tweet"]', timeout=10000)
                    self.logger.info("æ‰¾åˆ°æ¨æ–‡å…ƒç´ ")
                except Exception:
                    self.logger.warning("æœªæ‰¾åˆ°æ¨æ–‡å…ƒç´ ï¼Œå¯èƒ½æ²¡æœ‰æœç´¢ç»“æœ")
                
                self.logger.info(f"æˆåŠŸæœç´¢å…³é”®è¯: {keyword}")
                return
                
            except Exception as e:
                self.logger.warning(f"ç¬¬{attempt + 1}æ¬¡æœç´¢å°è¯•å¤±è´¥: {e}")
                
                if attempt < max_retries:
                    wait_time = (attempt + 1) * 3
                    self.logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    self.logger.error(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥ï¼Œå·²å°è¯•{max_retries + 1}æ¬¡")
                    raise Exception(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
    
    async def scroll_and_load_tweets(self, max_tweets: int = 10):
        """
        ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨æ»šåŠ¨é¡µé¢å¹¶åŠ è½½æ›´å¤šæ¨æ–‡
        
        Args:
            max_tweets: æœ€å¤§åŠ è½½æ¨æ–‡æ•°é‡
        """
        try:
            if not self.behavior_simulator:
                self.logger.warning("äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨åŸºç¡€æ»šåŠ¨")
                # å›é€€åˆ°åŸºç¡€æ»šåŠ¨
                for i in range(max_tweets // 5 + 1):
                    await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                    await asyncio.sleep(BROWSER_CONFIG['scroll_pause_time'])
                    tweets = await self.page.query_selector_all('[data-testid="tweet"]')
                    if len(tweets) >= max_tweets:
                        break
                return
            
            self.logger.info(f"å¼€å§‹ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨åŠ è½½æ¨æ–‡ï¼Œç›®æ ‡: {max_tweets} æ¡")
            
            # ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œæ™ºèƒ½æ»šåŠ¨
            collected_data = await self.behavior_simulator.smart_scroll_and_collect(
                max_tweets=max_tweets,
                target_selector='[data-testid="tweet"]'
            )
            
            # è·å–æœ€ç»ˆçš„æ¨æ–‡æ•°é‡
            final_tweets = await self.page.query_selector_all('[data-testid="tweet"]')
            self.logger.info(f"äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå®Œæˆï¼Œå½“å‰å¯è§æ¨æ–‡æ•°é‡: {len(final_tweets)}")
            
        except Exception as e:
            self.logger.error(f"äººå·¥è¡Œä¸ºæ»šåŠ¨å¤±è´¥: {e}")
            raise
    
    def extract_number(self, text: str) -> int:
        """
        ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆå¤„ç†Kã€Mç­‰å•ä½ï¼‰
        
        Args:
            text: åŒ…å«æ•°å­—çš„æ–‡æœ¬
            
        Returns:
            æå–çš„æ•°å­—
        """
        if not text:
            return 0
        
        # ç§»é™¤é€—å·å’Œç©ºæ ¼
        text = text.replace(',', '').replace(' ', '').lower()
        
        # æå–æ•°å­—å’Œå•ä½
        match = re.search(r'([0-9.]+)([km]?)', text)
        if not match:
            return 0
        
        number = float(match.group(1))
        unit = match.group(2)
        
        if unit == 'k':
            return int(number * 1000)
        elif unit == 'm':
            return int(number * 1000000)
        else:
            return int(number)
    
    async def parse_tweet_element(self, tweet_element) -> Optional[Dict[str, Any]]:
        """
        è§£æå•ä¸ªæ¨æ–‡å…ƒç´ 
        
        Args:
            tweet_element: æ¨æ–‡DOMå…ƒç´ 
            
        Returns:
            æ¨æ–‡æ•°æ®å­—å…¸
        """
        try:
            tweet_data = {}
            
            # æ£€æŸ¥å…ƒç´ æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            try:
                await tweet_element.is_visible()
            except Exception:
                self.logger.warning("æ¨æ–‡å…ƒç´ å·²å¤±æ•ˆï¼Œè·³è¿‡è§£æ")
                return None
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–ç”¨æˆ·å
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨
                username_selectors = [
                    '[data-testid="User-Name"] a',
                    '[data-testid="User-Names"] a',
                    'a[href^="/"][role="link"]'
                ]
                
                username_element = None
                for selector in username_selectors:
                    try:
                        username_element = await tweet_element.query_selector(selector)
                        if username_element:
                            break
                    except Exception:
                        continue
                
                if username_element:
                    username_href = await username_element.get_attribute('href')
                    if username_href and '/' in username_href:
                        tweet_data['username'] = username_href.split('/')[-1]
            except Exception as e:
                self.logger.debug(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–æ¨æ–‡å†…å®¹
            try:
                content_selectors = [
                    '[data-testid="tweetText"]',
                    '[lang] span',
                    'div[dir="auto"] span'
                ]
                
                content_element = None
                for selector in content_selectors:
                    try:
                        content_element = await tweet_element.query_selector(selector)
                        if content_element:
                            content_text = await content_element.inner_text()
                            if content_text and content_text.strip():
                                tweet_data['content'] = content_text.strip()
                                break
                    except Exception:
                        continue
            except Exception as e:
                self.logger.debug(f"æå–æ¨æ–‡å†…å®¹å¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–å‘å¸ƒæ—¶é—´
            try:
                time_element = await tweet_element.query_selector('time')
                if time_element:
                    datetime_attr = await time_element.get_attribute('datetime')
                    if datetime_attr:
                        tweet_data['publish_time'] = datetime_attr
            except Exception as e:
                self.logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–æ¨æ–‡é“¾æ¥
            try:
                link_elements = await tweet_element.query_selector_all('a[href*="/status/"]')
                if link_elements:
                    href = await link_elements[0].get_attribute('href')
                    if href:
                        if href.startswith('/'):
                            tweet_data['link'] = f"https://x.com{href}"
                        else:
                            tweet_data['link'] = href
            except Exception as e:
                self.logger.debug(f"æå–æ¨æ–‡é“¾æ¥å¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–äº’åŠ¨æ•°æ®
            interaction_data = {'likes': 0, 'comments': 0, 'retweets': 0}
            
            # ç‚¹èµæ•°
            try:
                like_selectors = [
                    '[data-testid="like"]',
                    '[aria-label*="like"]',
                    '[aria-label*="Like"]'
                ]
                
                for selector in like_selectors:
                    try:
                        like_element = await tweet_element.query_selector(selector)
                        if like_element:
                            like_text = await like_element.get_attribute('aria-label') or ''
                            if like_text:
                                interaction_data['likes'] = self.extract_number(like_text)
                                break
                    except Exception:
                        continue
            except Exception as e:
                self.logger.debug(f"æå–ç‚¹èµæ•°å¤±è´¥: {e}")
            
            # è¯„è®ºæ•°
            try:
                reply_selectors = [
                    '[data-testid="reply"]',
                    '[aria-label*="repl"]',
                    '[aria-label*="Reply"]'
                ]
                
                for selector in reply_selectors:
                    try:
                        reply_element = await tweet_element.query_selector(selector)
                        if reply_element:
                            reply_text = await reply_element.get_attribute('aria-label') or ''
                            if reply_text:
                                interaction_data['comments'] = self.extract_number(reply_text)
                                break
                    except Exception:
                        continue
            except Exception as e:
                self.logger.debug(f"æå–è¯„è®ºæ•°å¤±è´¥: {e}")
            
            # è½¬å‘æ•°
            try:
                retweet_selectors = [
                    '[data-testid="retweet"]',
                    '[aria-label*="retweet"]',
                    '[aria-label*="Retweet"]'
                ]
                
                for selector in retweet_selectors:
                    try:
                        retweet_element = await tweet_element.query_selector(selector)
                        if retweet_element:
                            retweet_text = await retweet_element.get_attribute('aria-label') or ''
                            if retweet_text:
                                interaction_data['retweets'] = self.extract_number(retweet_text)
                                break
                    except Exception:
                        continue
            except Exception as e:
                self.logger.debug(f"æå–è½¬å‘æ•°å¤±è´¥: {e}")
            
            # åˆå¹¶äº’åŠ¨æ•°æ®
            tweet_data.update(interaction_data)
            
            # è®¾ç½®é»˜è®¤å€¼
            tweet_data.setdefault('username', 'unknown')
            tweet_data.setdefault('content', '')
            tweet_data.setdefault('publish_time', datetime.now().isoformat())
            tweet_data.setdefault('link', '')
            tweet_data.setdefault('likes', 0)
            tweet_data.setdefault('comments', 0)
            tweet_data.setdefault('retweets', 0)
            
            # éªŒè¯æ¨æ–‡æ•°æ®çš„æœ‰æ•ˆæ€§
            if not tweet_data.get('content') and not tweet_data.get('username', '').replace('unknown', ''):
                self.logger.debug("æ¨æ–‡æ•°æ®æ— æ•ˆï¼Œè·³è¿‡")
                return None
            
            return tweet_data
            
        except Exception as e:
            self.logger.error(f"è§£ææ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return None
    
    async def scrape_tweets(self, max_tweets: int = 10, enable_enhanced: bool = False) -> List[Dict[str, Any]]:
        """
        æŠ“å–å½“å‰é¡µé¢çš„æ¨æ–‡æ•°æ®
        
        Args:
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        if enable_enhanced:
            return await self.enhanced_tweet_scraping(max_tweets)
            
        try:
            # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šæ¨æ–‡
            await self.scroll_and_load_tweets(max_tweets)
            
            # è·å–æ‰€æœ‰æ¨æ–‡å…ƒç´ 
            tweet_elements = await self.page.query_selector_all('[data-testid="tweet"]')
            
            tweets_data = []
            
            for i, tweet_element in enumerate(tweet_elements[:max_tweets]):
                # æ¨¡æ‹Ÿäººå·¥æŸ¥çœ‹æ¨æ–‡çš„è¡Œä¸º
                if self.behavior_simulator and i % 3 == 0:  # æ¯3æ¡æ¨æ–‡æ¨¡æ‹Ÿä¸€æ¬¡äº¤äº’
                    await self.behavior_simulator.simulate_tweet_interaction(tweet_element)
                
                tweet_data = await self.parse_tweet_element(tweet_element)
                if tweet_data:
                    tweets_data.append(tweet_data)
                    self.logger.info(f"æˆåŠŸè§£æç¬¬ {i+1} æ¡æ¨æ–‡: @{tweet_data['username']}")
                
                # æ¨¡æ‹Ÿäººå·¥é˜…è¯»é—´éš”
                if self.behavior_simulator:
                    await self.behavior_simulator.random_pause(0.5, 2.0)
            
            self.logger.info(f"æ€»å…±æŠ“å–åˆ° {len(tweets_data)} æ¡æ¨æ–‡")
            return tweets_data
            
        except Exception as e:
            self.logger.error(f"æŠ“å–æ¨æ–‡å¤±è´¥: {e}")
            return []
    
    async def scrape_user_tweets(self, username: str, max_tweets: int = 10, enable_enhanced: bool = False) -> List[Dict[str, Any]]:
        """
        æŠ“å–æŒ‡å®šç”¨æˆ·çš„æ¨æ–‡
        
        Args:
            username: Twitter ç”¨æˆ·å
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            await self.navigate_to_profile(username)
            
            # ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œé¡µé¢æ¢ç´¢
            if self.behavior_simulator:
                self.logger.info(f"å¼€å§‹æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆ @{username} çš„é¡µé¢")
                await self.behavior_simulator.simulate_page_exploration()
                
                # æ¨¡æ‹Ÿé˜…è¯»è¡Œä¸º
                await self.behavior_simulator.simulate_reading_behavior()
            else:
                await asyncio.sleep(3)  # å›é€€åˆ°åŸºç¡€ç­‰å¾…
            
            tweets = await self.scrape_tweets(max_tweets, enable_enhanced)
            
            # æ¨¡æ‹Ÿç”¨æˆ·ä¼šè¯ç»“æŸè¡Œä¸º
            if self.behavior_simulator:
                await self.behavior_simulator.simulate_natural_browsing([f"https://twitter.com/{username}"])
            
            # ä¸ºæ¯æ¡æ¨æ–‡æ·»åŠ æ¥æºä¿¡æ¯
            for tweet in tweets:
                tweet['source'] = f'@{username}'
                tweet['source_type'] = 'user_profile'
            
            return tweets
            
        except Exception as e:
            self.logger.error(f"æŠ“å–ç”¨æˆ· @{username} çš„æ¨æ–‡å¤±è´¥: {e}")
            return []
    
    async def scrape_keyword_tweets(self, keyword: str, max_tweets: int = 10, enable_enhanced: bool = False) -> List[Dict[str, Any]]:
        """
        æŠ“å–åŒ…å«æŒ‡å®šå…³é”®è¯çš„æ¨æ–‡
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            await self.search_tweets(keyword)
            
            # ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œæœç´¢é¡µé¢æ¢ç´¢
            if self.behavior_simulator:
                self.logger.info(f"å¼€å§‹æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆå…³é”®è¯ '{keyword}' çš„æœç´¢ç»“æœ")
                await self.behavior_simulator.simulate_page_exploration()
                
                # æ¨¡æ‹Ÿé˜…è¯»æœç´¢ç»“æœ
                await self.behavior_simulator.simulate_reading_behavior()
            else:
                await asyncio.sleep(3)  # å›é€€åˆ°åŸºç¡€ç­‰å¾…
            
            tweets = await self.scrape_tweets(max_tweets, enable_enhanced)
            
            # æ¨¡æ‹Ÿæœç´¢ä¼šè¯ç»“æŸè¡Œä¸º
            if self.behavior_simulator:
                await self.behavior_simulator.simulate_natural_browsing([f"https://twitter.com/search?q={keyword}"])
            
            # ä¸ºæ¯æ¡æ¨æ–‡æ·»åŠ æ¥æºä¿¡æ¯
            for tweet in tweets:
                tweet['source'] = keyword
                tweet['source_type'] = 'keyword_search'
            
            return tweets
            
        except Exception as e:
            self.logger.error(f"æŠ“å–å…³é”®è¯ '{keyword}' çš„æ¨æ–‡å¤±è´¥: {e}")
            return []
    
    async def scrape_tweet_details(self, tweet_url: str) -> Dict[str, Any]:
        """
        æŠ“å–æ¨æ–‡è¯¦æƒ…é¡µçš„å®Œæ•´å†…å®¹
        
        Args:
            tweet_url: æ¨æ–‡è¯¦æƒ…é¡µURL
            
        Returns:
            åŒ…å«å®Œæ•´å†…å®¹çš„æ¨æ–‡æ•°æ®
        """
        try:
            self.logger.info(f"å¼€å§‹æŠ“å–æ¨æ–‡è¯¦æƒ…: {tweet_url}")
            
            # å¯¼èˆªåˆ°æ¨æ–‡è¯¦æƒ…é¡µ
            await self.page.goto(tweet_url, timeout=BROWSER_CONFIG['navigation_timeout'])
            await self.page.wait_for_load_state('domcontentloaded', timeout=30000)
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            await asyncio.sleep(3)
            
            # æŠ“å–å®Œæ•´çš„æ¨æ–‡å†…å®¹
            full_content = await self.extract_full_tweet_content()
            
            # æŠ“å–å¤šåª’ä½“å†…å®¹
            media_content = await self.extract_media_content()
            
            # æŠ“å–æ¨æ–‡çº¿ç¨‹
            thread_tweets = await self.extract_tweet_thread()
            
            # æŠ“å–å¼•ç”¨æ¨æ–‡
            quoted_tweet = await self.extract_quoted_tweet()
            
            return {
                'full_content': full_content,
                'media': media_content,
                'thread': thread_tweets,
                'quoted_tweet': quoted_tweet,
                'has_detailed_content': True
            }
            
        except Exception as e:
            self.logger.error(f"æŠ“å–æ¨æ–‡è¯¦æƒ…å¤±è´¥: {e}")
            return {'has_detailed_content': False}
    
    async def extract_full_tweet_content(self) -> str:
        """
        æå–æ¨æ–‡çš„å®Œæ•´å†…å®¹æ–‡æœ¬
        
        Returns:
            å®Œæ•´çš„æ¨æ–‡å†…å®¹
        """
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–å®Œæ•´å†…å®¹
            content_selectors = [
                '[data-testid="tweetText"]',
                '[lang] span',
                'div[dir="auto"] span',
                'article div[lang] span'
            ]
            
            full_content = ""
            for selector in content_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        content_parts = []
                        for element in elements:
                            text = await element.inner_text()
                            if text and text.strip():
                                content_parts.append(text.strip())
                        
                        if content_parts:
                            full_content = ' '.join(content_parts)
                            break
                except Exception:
                    continue
            
            return full_content
            
        except Exception as e:
            self.logger.error(f"æå–å®Œæ•´æ¨æ–‡å†…å®¹å¤±è´¥: {e}")
            return ""
    
    async def extract_media_content(self) -> List[Dict[str, Any]]:
        """
        æå–æ¨æ–‡ä¸­çš„å›¾ç‰‡ã€è§†é¢‘ç­‰å¤šåª’ä½“å†…å®¹
        
        Returns:
            å¤šåª’ä½“å†…å®¹åˆ—è¡¨
        """
        media_items = []
        
        try:
            # æŠ“å–å›¾ç‰‡
            image_selectors = [
                '[data-testid="tweetPhoto"] img',
                'img[src*="pbs.twimg.com"]',
                'div[data-testid="tweetPhoto"] img'
            ]
            
            for selector in image_selectors:
                try:
                    images = await self.page.query_selector_all(selector)
                    for img in images:
                        src = await img.get_attribute('src')
                        alt = await img.get_attribute('alt') or ''
                        if src and 'pbs.twimg.com' in src:
                            media_items.append({
                                'type': 'image',
                                'url': src,
                                'description': alt,
                                'original_url': src.replace(':small', ':orig').replace(':medium', ':orig')
                            })
                    if images:
                        break
                except Exception:
                    continue
            
            # æŠ“å–è§†é¢‘
            video_selectors = [
                'video',
                '[data-testid="videoPlayer"] video',
                'div[data-testid="videoComponent"] video'
            ]
            
            for selector in video_selectors:
                try:
                    videos = await self.page.query_selector_all(selector)
                    for video in videos:
                        poster = await video.get_attribute('poster')
                        src = await video.get_attribute('src')
                        media_items.append({
                            'type': 'video',
                            'poster': poster,
                            'url': src,
                            'description': 'è§†é¢‘å†…å®¹'
                        })
                    if videos:
                        break
                except Exception:
                    continue
            
            # æŠ“å–GIF
            gif_selectors = [
                'img[src*="video.twimg.com"]',
                '[data-testid="tweetPhoto"] img[src*=".gif"]'
            ]
            
            for selector in gif_selectors:
                try:
                    gifs = await self.page.query_selector_all(selector)
                    for gif in gifs:
                        src = await gif.get_attribute('src')
                        alt = await gif.get_attribute('alt') or ''
                        if src:
                            media_items.append({
                                'type': 'gif',
                                'url': src,
                                'description': alt
                            })
                    if gifs:
                        break
                except Exception:
                    continue
            
            self.logger.info(f"æå–åˆ° {len(media_items)} ä¸ªå¤šåª’ä½“å†…å®¹")
            return media_items
            
        except Exception as e:
            self.logger.error(f"æå–å¤šåª’ä½“å†…å®¹å¤±è´¥: {e}")
            return []
    
    async def extract_tweet_thread(self) -> List[Dict[str, Any]]:
        """
        æå–æ¨æ–‡çº¿ç¨‹ï¼ˆè¿ç»­çš„ç›¸å…³æ¨æ–‡ï¼‰
        
        Returns:
            æ¨æ–‡çº¿ç¨‹åˆ—è¡¨
        """
        thread_tweets = []
        
        try:
            # æŸ¥æ‰¾çº¿ç¨‹ä¸­çš„å…¶ä»–æ¨æ–‡
            thread_selectors = [
                'article[data-testid="tweet"]',
                '[data-testid="tweet"]'
            ]
            
            for selector in thread_selectors:
                try:
                    tweet_elements = await self.page.query_selector_all(selector)
                    
                    # å¦‚æœæ‰¾åˆ°å¤šæ¡æ¨æ–‡ï¼Œè¯´æ˜å¯èƒ½æ˜¯çº¿ç¨‹
                    if len(tweet_elements) > 1:
                        for i, element in enumerate(tweet_elements[1:], 1):  # è·³è¿‡ç¬¬ä¸€æ¡ï¼ˆä¸»æ¨æ–‡ï¼‰
                            thread_tweet = await self.parse_tweet_element(element)
                            if thread_tweet:
                                thread_tweet['thread_position'] = i
                                thread_tweets.append(thread_tweet)
                    
                    break
                except Exception:
                    continue
            
            self.logger.info(f"æå–åˆ° {len(thread_tweets)} æ¡çº¿ç¨‹æ¨æ–‡")
            return thread_tweets
            
        except Exception as e:
            self.logger.error(f"æå–æ¨æ–‡çº¿ç¨‹å¤±è´¥: {e}")
            return []
    
    async def extract_quoted_tweet(self) -> Optional[Dict[str, Any]]:
        """
        æå–å¼•ç”¨çš„æ¨æ–‡å†…å®¹
        
        Returns:
            å¼•ç”¨æ¨æ–‡æ•°æ®
        """
        try:
            # æŸ¥æ‰¾å¼•ç”¨æ¨æ–‡
            quoted_selectors = [
                '[data-testid="quoteTweet"]',
                'div[role="blockquote"]',
                'blockquote'
            ]
            
            for selector in quoted_selectors:
                try:
                    quoted_element = await self.page.query_selector(selector)
                    if quoted_element:
                        quoted_tweet = await self.parse_tweet_element(quoted_element)
                        if quoted_tweet:
                            quoted_tweet['is_quoted'] = True
                            self.logger.info("æå–åˆ°å¼•ç”¨æ¨æ–‡")
                            return quoted_tweet
                except Exception:
                    continue
            
            return None
            
        except Exception as e:
            self.logger.error(f"æå–å¼•ç”¨æ¨æ–‡å¤±è´¥: {e}")
            return None
    
    def is_content_truncated(self, content: str) -> bool:
        """
        æ£€æµ‹å†…å®¹æ˜¯å¦è¢«æˆªæ–­
        
        Args:
            content: æ¨æ–‡å†…å®¹
            
        Returns:
            æ˜¯å¦è¢«æˆªæ–­
        """
        truncation_indicators = [
            content.endswith('...'),
            content.endswith('â€¦'),
            len(content) > 280 and not content.endswith('.'),
            'æ˜¾ç¤ºæ›´å¤š' in content,
            'Show more' in content,
            'æŸ¥çœ‹æ›´å¤š' in content,
            'See more' in content
        ]
        return any(truncation_indicators)
    
    def has_rich_media(self, tweet: Dict[str, Any]) -> bool:
        """
        æ£€æµ‹æ˜¯å¦åŒ…å«ä¸°å¯Œåª’ä½“å†…å®¹
        
        Args:
            tweet: æ¨æ–‡æ•°æ®
            
        Returns:
            æ˜¯å¦åŒ…å«åª’ä½“å†…å®¹
        """
        content = tweet.get('content', '')
        media_indicators = [
            'ğŸ“·' in content,
            'ğŸ¥' in content,
            'ğŸ“¸' in content,
            'ğŸ¬' in content,
            'å›¾ç‰‡' in content,
            'è§†é¢‘' in content,
            'ç…§ç‰‡' in content,
            'photo' in content.lower(),
            'video' in content.lower(),
            'image' in content.lower(),
            tweet.get('media_count', 0) > 0
        ]
        return any(media_indicators)
    
    def is_thread_content(self, content: str) -> bool:
        """
        è¯†åˆ«æ¨æ–‡çº¿ç¨‹
        
        Args:
            content: æ¨æ–‡å†…å®¹
            
        Returns:
            æ˜¯å¦ä¸ºçº¿ç¨‹å†…å®¹
        """
        import re
        
        thread_patterns = [
            r'\d+/\d+',  # 1/5, 2/10 ç­‰æ ¼å¼
            r'\d+/',     # 1/, 2/ ç­‰æ ¼å¼
            r'\(\d+/\d+\)',  # (1/5) æ ¼å¼
        ]
        
        thread_indicators = [
            'ğŸ§µ', 'ğŸ“', 'ğŸ‘‡', 'â¬‡ï¸',
            'Thread', 'thread', 'çº¿ç¨‹',
            'æ¥ä¸‹æ¥', 'ç»§ç»­', 'continued',
            'ä¸‹ä¸€æ¡', 'next tweet'
        ]
        
        # æ£€æŸ¥æ­£åˆ™æ¨¡å¼
        for pattern in thread_patterns:
            if re.search(pattern, content):
                return True
        
        # æ£€æŸ¥çº¿ç¨‹æŒ‡ç¤ºè¯
        return any(indicator in content for indicator in thread_indicators)
    
    def is_high_value_content(self, tweet: Dict[str, Any]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºé«˜ä»·å€¼å†…å®¹
        
        Args:
            tweet: æ¨æ–‡æ•°æ®
            
        Returns:
            æ˜¯å¦ä¸ºé«˜ä»·å€¼å†…å®¹
        """
        content = tweet.get('content', '').lower()
        
        # è®¡ç®—äº’åŠ¨åˆ†æ•°
        engagement_score = (
            tweet.get('likes', 0) * 1 +
            tweet.get('retweets', 0) * 2 +
            tweet.get('comments', 0) * 3
        )
        
        # é«˜ä»·å€¼å…³é”®è¯
        value_keywords = [
            'æ•™ç¨‹', 'æ–¹æ³•', 'æŠ€å·§', 'ç»éªŒ', 'åˆ†äº«', 'å¹²è´§',
            'tutorial', 'guide', 'tips', 'how to', 'method',
            'æ”»ç•¥', 'ç§˜ç±', 'å¿ƒå¾—', 'æ€»ç»“', 'å¤ç›˜',
            'å·¥å…·', 'èµ„æº', 'æ¨è', 'tools', 'resources'
        ]
        
        value_indicators = [
            engagement_score > 50,  # é«˜äº’åŠ¨
            len(tweet.get('content', '')) > 200,  # é•¿å†…å®¹
            any(keyword in content for keyword in value_keywords),
            tweet.get('comments', 0) > 20,  # é«˜è¯„è®ºæ•°
            tweet.get('retweets', 0) > 10   # é«˜è½¬å‘æ•°
        ]
        
        return any(value_indicators)
    
    def get_scraping_strategy(self, account_type: str = 'general', follower_count: int = 0) -> Dict[str, Any]:
        """
        æ ¹æ®è´¦å·ç±»å‹è·å–æŠ“å–ç­–ç•¥
        
        Args:
            account_type: è´¦å·ç±»å‹
            follower_count: ç²‰ä¸æ•°é‡
            
        Returns:
            æŠ“å–ç­–ç•¥é…ç½®
        """
        strategies = {
            'æŠ€æœ¯åšä¸»': {
                'detail_threshold': 0.4,
                'engagement_threshold': 30,
                'content_length_threshold': 150,
                'priority_keywords': ['ä»£ç ', 'æŠ€æœ¯', 'å¼€å‘', 'code', 'tech']
            },
            'è¥é”€åšä¸»': {
                'detail_threshold': 0.6,
                'engagement_threshold': 15,
                'content_length_threshold': 100,
                'priority_keywords': ['è¥é”€', 'æ¨å¹¿', 'å˜ç°', 'marketing']
            },
            'æŠ•èµ„åšä¸»': {
                'detail_threshold': 0.5,
                'engagement_threshold': 25,
                'content_length_threshold': 120,
                'priority_keywords': ['æŠ•èµ„', 'ç†è´¢', 'è‚¡ç¥¨', 'investment']
            },
            'general': {
                'detail_threshold': 0.3,
                'engagement_threshold': 20,
                'content_length_threshold': 150,
                'priority_keywords': []
            }
        }
        
        # æ ¹æ®ç²‰ä¸æ•°è°ƒæ•´ç­–ç•¥
        strategy = strategies.get(account_type, strategies['general']).copy()
        if follower_count > 100000:  # å¤§Vè´¦å·
            strategy['detail_threshold'] *= 1.2
            strategy['engagement_threshold'] *= 0.8
        
        return strategy
    
    def should_scrape_details(self, tweet: Dict[str, Any], account_type: str = 'general') -> bool:
        """
        æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦æŠ“å–æ¨æ–‡è¯¦æƒ…
        
        Args:
            tweet: åŸºç¡€æ¨æ–‡æ•°æ®
            account_type: è´¦å·ç±»å‹
            
        Returns:
            æ˜¯å¦éœ€è¦æ·±åº¦æŠ“å–
        """
        content = tweet.get('content', '')
        strategy = self.get_scraping_strategy(account_type)
        
        # å¿…é¡»æ·±åº¦æŠ“å–çš„æ¡ä»¶
        must_scrape = [
            self.is_content_truncated(content),  # å†…å®¹è¢«æˆªæ–­
            self.is_thread_content(content),     # çº¿ç¨‹å†…å®¹
            self.has_rich_media(tweet)           # åŒ…å«å¤šåª’ä½“
        ]
        
        if any(must_scrape):
            return True
        
        # é€‰æ‹©æ€§æ·±åº¦æŠ“å–çš„æ¡ä»¶
        optional_scrape = [
            self.is_high_value_content(tweet),   # é«˜ä»·å€¼å†…å®¹
            len(content) > strategy['content_length_threshold'],  # é•¿å†…å®¹
            tweet.get('comments', 0) > strategy['engagement_threshold'],  # é«˜äº’åŠ¨
            any(keyword in content.lower() for keyword in strategy['priority_keywords'])  # ä¼˜å…ˆå…³é”®è¯
        ]
        
        # æ ¹æ®ç­–ç•¥é˜ˆå€¼å†³å®š
        score = sum(optional_scrape) / len(optional_scrape) if optional_scrape else 0
        return score >= strategy['detail_threshold']
    
    async def enhanced_tweet_scraping(self, max_tweets: int = 10, enable_details: bool = True) -> List[Dict[str, Any]]:
        """
        å¢å¼ºçš„æ¨æ–‡æŠ“å–ï¼ŒåŒ…å«è¯¦æƒ…é¡µå†…å®¹
        
        Args:
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_details: æ˜¯å¦å¯ç”¨è¯¦æƒ…é¡µæŠ“å–
            
        Returns:
            å¢å¼ºçš„æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            # å…ˆæŠ“å–æ—¶é—´çº¿ä¸Šçš„åŸºæœ¬æ¨æ–‡
            basic_tweets = await self.scrape_tweets(max_tweets)
            
            if not enable_details:
                return basic_tweets
            
            enhanced_tweets = []
            details_scraped = 0
            max_details = min(5, max_tweets // 2)  # é™åˆ¶è¯¦æƒ…é¡µæŠ“å–æ•°é‡
            
            for i, tweet in enumerate(basic_tweets):
                enhanced_tweet = tweet.copy()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·±åº¦æŠ“å–
                if (details_scraped < max_details and 
                    tweet.get('link') and 
                    self.should_scrape_details(tweet, 'general')):
                    
                    self.logger.info(f"å¯¹ç¬¬ {i+1} æ¡æ¨æ–‡è¿›è¡Œè¯¦æƒ…æŠ“å–")
                    
                    try:
                        # æŠ“å–è¯¦æƒ…é¡µå†…å®¹
                        details = await self.scrape_tweet_details(tweet['link'])
                        enhanced_tweet.update(details)
                        details_scraped += 1
                        
                        # æ¨¡æ‹Ÿäººå·¥æµè§ˆé—´éš”
                        if self.behavior_simulator:
                            await self.behavior_simulator.random_pause(2, 5)
                        else:
                            await asyncio.sleep(3)
                            
                    except Exception as e:
                        self.logger.warning(f"è¯¦æƒ…æŠ“å–å¤±è´¥: {e}")
                        enhanced_tweet['detail_error'] = str(e)
                
                enhanced_tweets.append(enhanced_tweet)
            
            self.logger.info(f"å¢å¼ºæŠ“å–å®Œæˆï¼Œå…±å¤„ç† {len(enhanced_tweets)} æ¡æ¨æ–‡ï¼Œå…¶ä¸­ {details_scraped} æ¡è¿›è¡Œäº†è¯¦æƒ…æŠ“å–")
            return enhanced_tweets
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºæ¨æ–‡æŠ“å–å¤±è´¥: {e}")
            return basic_tweets if 'basic_tweets' in locals() else []
    
    async def close(self):
        """
        å…³é—­æµè§ˆå™¨è¿æ¥
        """
        try:
            if self.browser:
                await self.browser.close()
                self.logger.info("æµè§ˆå™¨è¿æ¥å·²å…³é—­")
        except Exception as e:
            self.logger.error(f"å…³é—­æµè§ˆå™¨è¿æ¥å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    async def main():
        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        
        # è¿™é‡Œéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„è°ƒè¯•ç«¯å£
        debug_port = "ws://127.0.0.1:9222"
        
        parser = TwitterParser(debug_port)
        
        try:
            await parser.connect_browser()
            await parser.navigate_to_twitter()
            
            # æŠ“å–æŒ‡å®šç”¨æˆ·çš„æ¨æ–‡
            tweets = await parser.scrape_user_tweets('elonmusk', 5)
            
            for tweet in tweets:
                print(f"ç”¨æˆ·: @{tweet['username']}")
                print(f"å†…å®¹: {tweet['content'][:100]}...")
                print(f"ç‚¹èµ: {tweet['likes']}, è¯„è®º: {tweet['comments']}, è½¬å‘: {tweet['retweets']}")
                print(f"é“¾æ¥: {tweet['link']}")
                print("-" * 50)
                
        except Exception as e:
            print(f"é”™è¯¯: {e}")
        finally:
            await parser.close()
    
    asyncio.run(main())