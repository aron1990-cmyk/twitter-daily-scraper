# -*- coding: utf-8 -*-
"""
Twitter è§£æå™¨
ä½¿ç”¨ Playwright æ§åˆ¶æµè§ˆå™¨å¹¶æŠ“å–æ¨æ–‡æ•°æ®
"""

import asyncio
import logging
import re
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from playwright.async_api import async_playwright, Browser, Page
# é…ç½®å°†ä»è°ƒç”¨æ–¹ä¼ å…¥æˆ–ä½¿ç”¨é»˜è®¤é…ç½®
from human_behavior_simulator import HumanBehaviorSimulator
# from performance_optimizer import EnhancedSearchOptimizer

class TwitterParser:
    def __init__(self, debug_port: str = None):
        self.debug_port = debug_port
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.behavior_simulator = None
        # self.search_optimizer = EnhancedSearchOptimizer()
        self.logger = logging.getLogger(__name__)
        self.config = None
        
        # ä¼˜åŒ–åŠŸèƒ½å±æ€§
        self.seen_tweet_ids: Set[str] = set()
        self.content_cache: Dict[str, str] = {}
        self.optimization_enabled = True
    
    async def initialize(self, debug_port: str = None):
        """åˆå§‹åŒ–TwitterParser
        
        Args:
            debug_port: æµè§ˆå™¨è°ƒè¯•ç«¯å£
        """
        if debug_port:
            self.debug_port = debug_port
        
        if not self.debug_port:
            raise ValueError("debug_port is required for initialization")
        
        self.logger.info(f"Initializing TwitterParser with debug_port: {self.debug_port}")
        
        # è¿æ¥æµè§ˆå™¨
        await self.connect_browser()
        
        self.logger.info("TwitterParser initialization completed")
        
    async def connect_browser(self):
        """
        è¿æ¥åˆ° AdsPower æµè§ˆå™¨
        """
        try:
            self.logger.info("å¼€å§‹å¯åŠ¨ Playwright...")
            playwright = await async_playwright().start()
            
            self.logger.info(f"å¼€å§‹è¿æ¥åˆ°æµè§ˆå™¨è°ƒè¯•ç«¯å£: {self.debug_port}")
            # è¿æ¥åˆ°ç°æœ‰çš„æµè§ˆå™¨å®ä¾‹
            self.browser = await playwright.chromium.connect_over_cdp(self.debug_port)
            self.logger.info("æˆåŠŸè¿æ¥åˆ°æµè§ˆå™¨å®ä¾‹")
            
            # è·å–ç°æœ‰ä¸Šä¸‹æ–‡å’Œé¡µé¢
            contexts = self.browser.contexts
            self.logger.info(f"æ‰¾åˆ° {len(contexts)} ä¸ªæµè§ˆå™¨ä¸Šä¸‹æ–‡")
            
            if contexts:
                context = contexts[0]
                pages = context.pages
                self.logger.info(f"åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ° {len(pages)} ä¸ªé¡µé¢")
                
                if pages:
                    # ä½¿ç”¨ç°æœ‰é¡µé¢
                    self.page = pages[0]
                    try:
                        current_url = self.page.url
                        self.logger.info(f"ä½¿ç”¨ç°æœ‰é¡µé¢ï¼Œå½“å‰URL: {current_url}")
                    except Exception as url_error:
                        self.logger.warning(f"æ— æ³•è·å–é¡µé¢URL: {url_error}")
                        # åˆ›å»ºæ–°é¡µé¢ä½œä¸ºå¤‡é€‰
                        self.page = await context.new_page()
                        self.logger.info("åˆ›å»ºæ–°é¡µé¢ä½œä¸ºå¤‡é€‰")
                else:
                    # åœ¨ç°æœ‰ä¸Šä¸‹æ–‡ä¸­åˆ›å»ºæ–°é¡µé¢
                    self.page = await context.new_page()
                    self.logger.info("åœ¨ç°æœ‰ä¸Šä¸‹æ–‡ä¸­åˆ›å»ºæ–°é¡µé¢")
            else:
                # åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œè®¾ç½®ç”¨æˆ·ä»£ç†å’Œå…¶ä»–é€‰é¡¹
                self.logger.info("åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡...")
                context = await self.browser.new_context(
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080},
                    extra_http_headers={
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
                    }
                )
                self.page = await context.new_page()
                self.logger.info("åˆ›å»ºæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡å’Œé¡µé¢")
            
            # è®¾ç½®é¡µé¢é»˜è®¤è¶…æ—¶æ—¶é—´
            default_timeout = 30000
            self.page.set_default_timeout(default_timeout)
            self.logger.info(f"è®¾ç½®é¡µé¢è¶…æ—¶æ—¶é—´: {default_timeout}ms")
            
            # è®¾ç½®é¡µé¢å¯¼èˆªè¶…æ—¶æ—¶é—´
            navigation_timeout = 60000
            self.page.set_default_navigation_timeout(navigation_timeout)
            self.logger.info(f"è®¾ç½®å¯¼èˆªè¶…æ—¶æ—¶é—´: {navigation_timeout}ms")
            
            # åˆå§‹åŒ–äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨
            self.behavior_simulator = HumanBehaviorSimulator(self.page)
            self.logger.info("äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨åˆå§‹åŒ–å®Œæˆ")
            
            self.logger.info("æˆåŠŸè¿æ¥åˆ°æµè§ˆå™¨")
            
        except Exception as e:
            self.logger.error(f"è¿æ¥æµè§ˆå™¨å¤±è´¥: {e}")
            raise
    
    async def navigate_to_twitter(self, max_retries: int = 3):
        """
        å¯¼èˆªåˆ° Twitter ä¸»é¡µ
        
        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for attempt in range(max_retries):
            try:
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹
                await self.ensure_page_focus()
                
                current_url = self.page.url
                self.logger.info(f"å½“å‰é¡µé¢URL: {current_url}")
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ x.com
                if 'x.com' not in current_url:
                    self.logger.info(f"é¡µé¢ä¸åœ¨ x.comï¼Œå¼€å§‹å¯¼èˆª... (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
                    
                    # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´
                    await self.page.goto('https://x.com', timeout=60000)
                    
                    # åˆ†æ­¥ç­‰å¾…åŠ è½½
                    try:
                        await self.page.wait_for_load_state('domcontentloaded', timeout=30000)
                        await self.page.wait_for_load_state('networkidle', timeout=30000)
                    except Exception as load_error:
                        self.logger.warning(f"ç­‰å¾…åŠ è½½çŠ¶æ€å¤±è´¥: {load_error}ï¼Œç»§ç»­å°è¯•")
                    
                    self.logger.info("æˆåŠŸå¯¼èˆªåˆ° X (Twitter)")
                else:
                    self.logger.info("é¡µé¢å·²ç»åœ¨ X (Twitter)ï¼Œè·³è¿‡å¯¼èˆª")
                
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹åå†è¿›è¡Œæµ‹è¯•
                await self.ensure_page_focus()
                
                # æµ‹è¯•é¡µé¢äº¤äº’ - æ‰§è¡Œå‡ æ¬¡ä¸‹æ‹‰æ“ä½œ
                self.logger.info("å¼€å§‹æµ‹è¯•é¡µé¢ä¸‹æ‹‰åŠŸèƒ½...")
                for i in range(3):
                    # å‘ä¸‹æ»šåŠ¨
                    await self.page.evaluate('window.scrollBy(0, 500)')
                    await asyncio.sleep(2)
                    self.logger.info(f"æ‰§è¡Œç¬¬ {i+1} æ¬¡ä¸‹æ‹‰")
                
                # æ»šåŠ¨å›é¡¶éƒ¨
                await self.page.evaluate('window.scrollTo(0, 0)')
                await asyncio.sleep(2)
                self.logger.info("é¡µé¢ä¸‹æ‹‰æµ‹è¯•å®Œæˆï¼Œå·²æ»šåŠ¨å›é¡¶éƒ¨")
                return
                
            except Exception as e:
                self.logger.warning(f"ç¬¬{attempt + 1}æ¬¡å¯¼èˆªå°è¯•å¤±è´¥: {e}")
                
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5
                    self.logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    self.logger.error(f"å¯¼èˆªåˆ° X (Twitter) å¤±è´¥ï¼Œå·²å°è¯•{max_retries}æ¬¡")
                    raise Exception(f"å¯¼èˆªåˆ° X (Twitter) å¤±è´¥: {e}")
    
    async def navigate_to_profile(self, username: str, max_retries: int = 3):
        """
        å¯¼èˆªåˆ°æŒ‡å®šç”¨æˆ·çš„ä¸ªäººèµ„æ–™é¡µé¢
        
        Args:
            username: Twitter ç”¨æˆ·åï¼ˆä¸åŒ…å«@ç¬¦å·ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        profile_url = f'https://x.com/{username}'
        
        for attempt in range(max_retries):
            try:
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹
                await self.ensure_page_focus()
                
                self.logger.info(f"å°è¯•å¯¼èˆªåˆ° @{username} çš„ä¸ªäººèµ„æ–™é¡µé¢ (ç¬¬{attempt + 1}æ¬¡)")
                
                # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´è¿›è¡Œå¯¼èˆª
                await self.page.goto(profile_url, timeout=60000)
                
                # åˆ†æ­¥ç­‰å¾…åŠ è½½çŠ¶æ€
                try:
                    await self.page.wait_for_load_state('domcontentloaded', timeout=30000)
                    self.logger.info("DOMå†…å®¹åŠ è½½å®Œæˆ")
                    
                    await self.page.wait_for_load_state('networkidle', timeout=30000)
                    self.logger.info("ç½‘ç»œç©ºé—²çŠ¶æ€è¾¾åˆ°")
                except Exception as load_error:
                    self.logger.warning(f"ç­‰å¾…åŠ è½½çŠ¶æ€å¤±è´¥: {load_error}ï¼Œç»§ç»­å°è¯•")
                
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹åå†ç­‰å¾…
                await self.ensure_page_focus()
                
                # é¢å¤–ç­‰å¾…æ—¶é—´ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
                await asyncio.sleep(2)
                
                # éªŒè¯é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
                current_url = self.page.url
                if username.lower() in current_url.lower():
                    self.logger.info(f"æˆåŠŸå¯¼èˆªåˆ° @{username} çš„ä¸ªäººèµ„æ–™é¡µé¢")
                    return
                else:
                    raise Exception(f"é¡µé¢URLä¸åŒ¹é…ï¼ŒæœŸæœ›åŒ…å«'{username}'ï¼Œå®é™…ä¸º'{current_url}'")
                
            except Exception as e:
                self.logger.warning(f"ç¬¬{attempt + 1}æ¬¡å¯¼èˆªå°è¯•å¤±è´¥: {e}")
                
                if attempt < max_retries - 1:
                    # ç­‰å¾…åé‡è¯•ï¼ˆæé€Ÿæ¨¡å¼ï¼‰
                    wait_time = (attempt + 1) * 1  # æçŸ­é€’å¢ç­‰å¾…æ—¶é—´
                    self.logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                    self.logger.error(f"å¯¼èˆªåˆ° @{username} ä¸ªäººèµ„æ–™é¡µé¢å¤±è´¥ï¼Œå·²å°è¯•{max_retries}æ¬¡")
                    raise Exception(f"å¯¼èˆªåˆ° @{username} ä¸ªäººèµ„æ–™é¡µé¢å¤±è´¥: {e}")
    
    async def search_tweets(self, keyword: str, max_retries: int = 2):
        """
        æœç´¢åŒ…å«æŒ‡å®šå…³é”®è¯çš„æ¨æ–‡
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for attempt in range(max_retries + 1):
            try:
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹
                await self.ensure_page_focus()
                
                self.logger.info(f"å¼€å§‹æœç´¢å…³é”®è¯ '{keyword}' (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
                
                # æ„å»ºæœç´¢URLï¼Œä½¿ç”¨URLç¼–ç 
                import urllib.parse
                encoded_keyword = urllib.parse.quote(keyword)
                search_url = f'https://x.com/search?q={encoded_keyword}&src=typed_query&f=live'
                self.logger.info(f"æœç´¢URL: {search_url}")
                
                # å¯¼èˆªåˆ°æœç´¢é¡µé¢
                self.logger.info("æ­£åœ¨å¯¼èˆªåˆ°æœç´¢é¡µé¢...")
                await self.page.goto(search_url, timeout=30000)
                self.logger.info("å¯¼èˆªå®Œæˆï¼Œç­‰å¾…é¡µé¢åŠ è½½...")
                
                # ç®€åŒ–ç­‰å¾…ç­–ç•¥
                try:
                    await self.page.wait_for_load_state('domcontentloaded', timeout=10000)
                    self.logger.info("DOMå†…å®¹åŠ è½½å®Œæˆ")
                except Exception as load_error:
                    self.logger.warning(f"DOMåŠ è½½è¶…æ—¶: {load_error}")
                
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹åå†ç­‰å¾…æœç´¢ç»“æœ
                await self.ensure_page_focus()
                
                # ç­‰å¾…æœç´¢ç»“æœåŠ è½½ï¼ˆæé€Ÿæ¨¡å¼ï¼‰
                self.logger.info("ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")
                await asyncio.sleep(2)  # æçŸ­ç­‰å¾…æ—¶é—´
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
                try:
                    # ç­‰å¾…æ¨æ–‡å…ƒç´ å‡ºç°
                    await self.page.wait_for_selector('[data-testid="tweet"]', timeout=10000)
                    self.logger.info("æ‰¾åˆ°æ¨æ–‡å…ƒç´ ")
                except Exception:
                    self.logger.warning("æœªæ‰¾åˆ°æ¨æ–‡å…ƒç´ ï¼Œå¯èƒ½æ²¡æœ‰æœç´¢ç»“æœ")
                
                self.logger.info(f"æˆåŠŸæœç´¢å…³é”®è¯: {keyword}")
                return
                
            except Exception as e:
                self.logger.warning(f"ç¬¬{attempt + 1}æ¬¡æœç´¢å°è¯•å¤±è´¥: {e}")
                
                if attempt < max_retries:
                    wait_time = (attempt + 1) * 0.8  # æçŸ­é‡è¯•ç­‰å¾…æ—¶é—´
                    self.logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    self.logger.error(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥ï¼Œå·²å°è¯•{max_retries + 1}æ¬¡")
                    raise Exception(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
    
    async def ensure_page_focus(self):
        """
        ç¡®ä¿é¡µé¢è·å¾—ç„¦ç‚¹ï¼Œå¤„ç†é¡µé¢è¢«åˆ‡æ¢å‡ºå»çš„æƒ…å†µ
        """
        try:
            # æ£€æŸ¥é¡µé¢æ˜¯å¦å¯è§
            is_visible = await self.page.evaluate('!document.hidden')
            if not is_visible:
                self.logger.info("æ£€æµ‹åˆ°é¡µé¢å¤±å»ç„¦ç‚¹ï¼Œå°è¯•æ¢å¤...")
                
                # å°è¯•å°†é¡µé¢å¸¦åˆ°å‰å°
                await self.page.bring_to_front()
                await asyncio.sleep(0.3)              
                # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼æ¢å¤ç„¦ç‚¹ï¼Œé¿å…è¯¯ç‚¹å‡»é“¾æ¥
                try:
                    # æ–¹æ³•1ï¼šç›´æ¥èšç„¦åˆ°é¡µé¢
                    await self.page.evaluate('window.focus()')
                    await asyncio.sleep(0.1)
                    
                    # æ–¹æ³•2ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰ç„¦ç‚¹ï¼Œå°è¯•ç‚¹å‡»ä¸€ä¸ªå®‰å…¨çš„åŒºåŸŸï¼ˆé¡µé¢è¾¹ç¼˜ï¼‰
                    is_visible = await self.page.evaluate('!document.hidden')
                    if not is_visible:
                        # ç‚¹å‡»é¡µé¢å·¦ä¸Šè§’çš„å®‰å…¨åŒºåŸŸï¼Œé¿å…ç‚¹å‡»åˆ°é“¾æ¥
                        await self.page.mouse.click(10, 10)
                        await asyncio.sleep(0.1)
                except Exception as focus_error:
                    self.logger.debug(f"ç„¦ç‚¹æ¢å¤æ“ä½œå¤±è´¥: {focus_error}")
                
                # å†æ¬¡æ£€æŸ¥
                is_visible = await self.page.evaluate('!document.hidden')
                if is_visible:
                    self.logger.info("é¡µé¢ç„¦ç‚¹å·²æ¢å¤")
                else:
                    self.logger.warning("é¡µé¢ä»ç„¶å¤±å»ç„¦ç‚¹ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
                    
        except Exception as e:
            self.logger.warning(f"é¡µé¢ç„¦ç‚¹æ£€æŸ¥å¤±è´¥: {e}ï¼Œç»§ç»­æ‰§è¡Œ")
            
    async def dismiss_translate_popup(self):
        """
        æ£€æµ‹å¹¶å…³é—­é¡µé¢ä¸Šçš„ç¿»è¯‘å¼¹çª—ï¼Œç¡®ä¿æ»šåŠ¨å’ŒæŠ“å–ä¸è¢«ä¸­æ–­
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¿»è¯‘å¼¹çª—
            # å¸¸è§çš„ç¿»è¯‘å¼¹çª—é€‰æ‹©å™¨ï¼Œæ ¹æ®å®é™…æƒ…å†µå¯èƒ½éœ€è¦è°ƒæ•´
            popup_selectors = [
                'div[role="dialog"][aria-modal="true"]',  # é€šç”¨å¯¹è¯æ¡†
                'div.translate-dialog',                  # å¯èƒ½çš„ç¿»è¯‘å¯¹è¯æ¡†ç±»å
                'div[aria-label*="translate"]',          # å¸¦æœ‰translateå­—æ ·çš„å¯¹è¯æ¡†
                'div[data-testid="translatePrompt"]',    # Twitterç‰¹å®šçš„ç¿»è¯‘æç¤º
                'div.r-1upvrn0',                        # Twitterå¸¸ç”¨çš„å¼¹çª—ç±»
                'div[role="button"][tabindex="0"][aria-label*="å…³é—­"]',  # å…³é—­æŒ‰é’®
                'div[role="button"][tabindex="0"][aria-label*="Close"]'   # è‹±æ–‡å…³é—­æŒ‰é’®
            ]
            
            for selector in popup_selectors:
                popup_elements = await self.page.query_selector_all(selector)
                if popup_elements:
                    self.logger.info(f"æ£€æµ‹åˆ°å¯èƒ½çš„ç¿»è¯‘å¼¹çª—: {selector}ï¼Œå°è¯•å…³é—­")
                    
                    # å°è¯•æ–¹æ³•1ï¼šç‚¹å‡»å…³é—­æŒ‰é’®
                    close_button_selectors = [
                        'div[role="button"][aria-label*="å…³é—­"]',
                        'div[role="button"][aria-label*="Close"]',
                        'div[role="button"][aria-label*="Dismiss"]',
                        'div[role="button"][aria-label*="å–æ¶ˆ"]',
                        'div[role="button"][aria-label*="Cancel"]',
                        'button[aria-label*="å…³é—­"]',
                        'button[aria-label*="Close"]',
                        'svg[aria-label*="å…³é—­"]',
                        'svg[aria-label*="Close"]'
                    ]
                    
                    for close_selector in close_button_selectors:
                        try:
                            close_button = await self.page.query_selector(close_selector)
                            if close_button:
                                await close_button.click()
                                self.logger.info(f"æˆåŠŸç‚¹å‡»å…³é—­æŒ‰é’®: {close_selector}")
                                await asyncio.sleep(0.5)  # ç­‰å¾…å¼¹çª—æ¶ˆå¤±
                                return True
                        except Exception as click_error:
                            self.logger.debug(f"ç‚¹å‡»å…³é—­æŒ‰é’®å¤±è´¥: {click_error}")
                    
                    # å°è¯•æ–¹æ³•2ï¼šç‚¹å‡»å¼¹çª—å¤–éƒ¨åŒºåŸŸ
                    try:
                        # è·å–å¼¹çª—ä½ç½®
                        popup = popup_elements[0]
                        popup_box = await popup.bounding_box()
                        if popup_box:
                            # ç‚¹å‡»å¼¹çª—å¤–çš„åŒºåŸŸï¼ˆé¡µé¢å·¦ä¸Šè§’ï¼‰
                            await self.page.mouse.click(10, 10)
                            self.logger.info("å°è¯•é€šè¿‡ç‚¹å‡»é¡µé¢å…¶ä»–åŒºåŸŸå…³é—­å¼¹çª—")
                            await asyncio.sleep(0.5)  # ç­‰å¾…å¼¹çª—æ¶ˆå¤±
                            return True
                    except Exception as outside_click_error:
                        self.logger.debug(f"ç‚¹å‡»å¼¹çª—å¤–éƒ¨åŒºåŸŸå¤±è´¥: {outside_click_error}")
                    
                    # å°è¯•æ–¹æ³•3ï¼šä½¿ç”¨é”®ç›˜ESCé”®
                    try:
                        await self.page.keyboard.press('Escape')
                        self.logger.info("å°è¯•é€šè¿‡ESCé”®å…³é—­å¼¹çª—")
                        await asyncio.sleep(0.5)  # ç­‰å¾…å¼¹çª—æ¶ˆå¤±
                        return True
                    except Exception as key_error:
                        self.logger.debug(f"ä½¿ç”¨ESCé”®å…³é—­å¼¹çª—å¤±è´¥: {key_error}")
                    
                    # å°è¯•æ–¹æ³•4ï¼šä½¿ç”¨JavaScriptå…³é—­å¼¹çª—
                    try:
                        await self.page.evaluate('''
                        document.querySelectorAll('div[role="dialog"]').forEach(dialog => {
                            dialog.style.display = 'none';
                        });
                        ''')
                        self.logger.info("å°è¯•é€šè¿‡JavaScriptéšè—å¼¹çª—")
                        await asyncio.sleep(0.5)  # ç­‰å¾…å¼¹çª—æ¶ˆå¤±
                        return True
                    except Exception as js_error:
                        self.logger.debug(f"ä½¿ç”¨JavaScriptéšè—å¼¹çª—å¤±è´¥: {js_error}")
            
            return False  # æ²¡æœ‰å‘ç°å¼¹çª—æˆ–å…³é—­å¤±è´¥
        except Exception as e:
            self.logger.warning(f"å¤„ç†ç¿»è¯‘å¼¹çª—æ—¶å‡ºé”™: {e}")
            return False
    
    async def scroll_and_load_tweets(self, max_tweets: int = 10):
        """
        ä½¿ç”¨ä¼˜åŒ–çš„æ»šåŠ¨ç­–ç•¥åŠ è½½æ›´å¤šæ¨æ–‡
        
        Args:
            max_tweets: æœ€å¤§åŠ è½½æ¨æ–‡æ•°é‡
        """
        if self.optimization_enabled:
            self.logger.info(f"ğŸ”§ ä½¿ç”¨ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥ï¼Œç›®æ ‡æ¨æ–‡æ•°: {max_tweets}")
            result = await self.scroll_and_load_tweets_optimized(max_tweets)
            self.logger.info(f"ğŸ”§ ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥å®Œæˆï¼Œè¿”å›ç»“æœ: {result}")
            return result
        
        try:
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹
            await self.ensure_page_focus()
            
            scroll_attempts = 0
            current_tweets = 0
            last_tweet_count = 0
            stagnant_count = 0
            
            self.logger.info(f"å¼€å§‹ä¼˜åŒ–æ»šåŠ¨åŠ è½½æ¨æ–‡ï¼Œç›®æ ‡: {max_tweets} æ¡")
            
            while current_tweets < max_tweets:
                # è·å–å½“å‰æ¨æ–‡æ•°é‡
                tweets = await self.page.query_selector_all('[data-testid="tweet"]')
                current_tweets = len(tweets)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡åŠ è½½
                if current_tweets == last_tweet_count:
                    stagnant_count += 1
                else:
                    stagnant_count = 0
                last_tweet_count = current_tweets
                
                # æ£€æŸ¥å¹¶å¤„ç†å¯èƒ½å‡ºç°çš„ç¿»è¯‘å¼¹çª—
                popup_dismissed = await self.dismiss_translate_popup()
                if popup_dismissed:
                    self.logger.info("å·²å¤„ç†ç¿»è¯‘å¼¹çª—ï¼Œç»§ç»­æ»šåŠ¨")
                    # å¼¹çª—å¤„ç†åé‡ç½®åœæ»è®¡æ•°ï¼Œé¿å…å› å¼¹çª—å¯¼è‡´çš„è¯¯åˆ¤
                    stagnant_count = max(0, stagnant_count - 1)
                
                # ä½¿ç”¨ç®€åŒ–çš„æ»šåŠ¨ç­–ç•¥ï¼ˆä¿®å¤search_optimizerç¼ºå¤±é—®é¢˜ï¼‰
                scroll_distance = 800
                wait_time = 1.5
                max_scrolls = 20
                
                # å¦‚æœè¿ç»­å¤šæ¬¡æ²¡æœ‰æ–°å†…å®¹ï¼Œå¯ç”¨æ¿€è¿›æ¨¡å¼
                if stagnant_count >= 3:
                    scroll_distance = 3000
                    wait_time = 1.0
                    self.logger.debug("æ£€æµ‹åˆ°å†…å®¹åœæ»ï¼Œå¯ç”¨æ¿€è¿›æ»šåŠ¨æ¨¡å¼")
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­æ»šåŠ¨
                if scroll_attempts >= max_scrolls:
                    self.logger.info(f"è¾¾åˆ°æœ€å¤§æ»šåŠ¨æ¬¡æ•° {max_scrolls}ï¼Œåœæ­¢æ»šåŠ¨")
                    break
                
                if stagnant_count >= 8:
                    self.logger.info(f"å†…å®¹é•¿æ—¶é—´åœæ» ({stagnant_count} æ¬¡)ï¼Œåœæ­¢æ»šåŠ¨")
                    break
                
                # æ‰§è¡Œæ»šåŠ¨
                self.logger.debug(f"æ‰§è¡Œæ»šåŠ¨ï¼Œè·ç¦»: {scroll_distance}pxï¼Œç­‰å¾…æ—¶é—´: {wait_time}s")
                
                await self.ensure_page_focus()
                await self.page.evaluate(f'window.scrollBy(0, {scroll_distance})')
                await asyncio.sleep(wait_time)
                
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦æœ‰ç¿»è¯‘å¼¹çª—å‡ºç°
                await self.dismiss_translate_popup()
                
                scroll_attempts += 1
                
                # å¦‚æœé•¿æ—¶é—´æ²¡æœ‰æ–°å†…å®¹ï¼Œå°è¯•åˆ·æ–°é¡µé¢
                if stagnant_count >= 8:
                    self.logger.info("é•¿æ—¶é—´æ— æ–°å†…å®¹ï¼Œå°è¯•é¡µé¢åˆ·æ–°")
                    await self.page.reload()
                    await asyncio.sleep(2)
                    stagnant_count = 0
                    # åˆ·æ–°é¡µé¢åä¹Ÿæ£€æŸ¥æ˜¯å¦æœ‰ç¿»è¯‘å¼¹çª—
                    await self.dismiss_translate_popup()
            
            # å¦‚æœæœ‰äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨ä¸”æ•ˆæœä¸ä½³ï¼Œä½œä¸ºè¡¥å……
            if self.behavior_simulator and current_tweets < max_tweets * 0.7:
                self.logger.info("å½“å‰æ•ˆæœä¸ä½³ï¼Œä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨ä½œä¸ºè¡¥å……")
                try:
                    # ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨å‰å…ˆå¤„ç†å¯èƒ½çš„å¼¹çª—
                    await self.dismiss_translate_popup()
                    await self.behavior_simulator.smart_scroll_and_collect(
                        max_tweets=max_tweets - current_tweets,
                        target_selector='[data-testid="tweet"]'
                    )
                except Exception as e:
                    self.logger.warning(f"äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¡¥å……å¤±è´¥: {e}")
                    # å‡ºé”™æ—¶ä¹Ÿå°è¯•å¤„ç†å¯èƒ½çš„å¼¹çª—
                    await self.dismiss_translate_popup()
            
            # è·å–æœ€ç»ˆçš„æ¨æ–‡æ•°é‡
            final_tweets = await self.page.query_selector_all('[data-testid="tweet"]')
            efficiency = len(final_tweets) / max(scroll_attempts, 1)
            self.logger.info(f"ä¼˜åŒ–æ»šåŠ¨å®Œæˆï¼Œæ»šåŠ¨ {scroll_attempts} æ¬¡ï¼Œæœ€ç»ˆæ¨æ–‡æ•°é‡: {len(final_tweets)}ï¼Œæ•ˆç‡: {efficiency:.2f} æ¨æ–‡/æ»šåŠ¨")
            
        except Exception as e:
            self.logger.error(f"ä¼˜åŒ–æ»šåŠ¨å¤±è´¥: {e}")
            # å‡ºé”™æ—¶ä¹Ÿå°è¯•å¤„ç†å¯èƒ½çš„å¼¹çª—
            await self.dismiss_translate_popup()
            raise
    
    def extract_number(self, text: str) -> int:
        """
        ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆå¤„ç†Kã€Mç­‰å•ä½ï¼‰
        
        Args:
            text: åŒ…å«æ•°å­—çš„æ–‡æœ¬
            
        Returns:
            æå–çš„æ•°å­—
        """
        if not text:
            return 0
        
        # è®°å½•åŸå§‹æ–‡æœ¬ç”¨äºè°ƒè¯•
        original_text = text
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™æ•°å­—ã€å•ä½å’Œåˆ†éš”ç¬¦
        text = re.sub(r'[^\d\.KkMmBbä¸‡åƒç™¾å,\s]', '', text)
        
        # æŸ¥æ‰¾æ•°å­—æ¨¡å¼ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        patterns = [
            (r'([\d,]+(?:\.\d+)?)\s*[Bb]', 1000000000),  # 1.2B, 1,234B
            (r'([\d,]+(?:\.\d+)?)\s*[Mm]', 1000000),     # 1.2M, 1,234M
            (r'([\d,]+(?:\.\d+)?)\s*[Kk]', 1000),        # 1.2K, 1,234K
            (r'([\d,]+(?:\.\d+)?)\s*ä¸‡', 10000),          # 1.2ä¸‡
            (r'([\d,]+(?:\.\d+)?)\s*åƒ', 1000),           # 1.2åƒ
            (r'([\d,]+(?:\.\d+)?)\s*ç™¾', 100),            # 1.2ç™¾
            (r'([\d,]+(?:\.\d+)?)\s*å', 10),             # 1.2å
            (r'([\d,]+(?:\.\d+)?)', 1),                   # æ™®é€šæ•°å­—
        ]
        
        for pattern, multiplier in patterns:
            match = re.search(pattern, text)
            if match:
                number_str = match.group(1).replace(',', '')
                try:
                    number = float(number_str)
                    result = int(number * multiplier)
                    
                    # è°ƒè¯•æ—¥å¿—
                    if result > 0:
                        self.logger.debug(f"æ•°å­—æå–æˆåŠŸ: '{original_text}' -> '{number_str}' * {multiplier} = {result}")
                    
                    return result
                except ValueError:
                    continue
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…
        loose_match = re.search(r'(\d+)', text)
        if loose_match:
            try:
                result = int(loose_match.group(1))
                if result > 0:
                    self.logger.debug(f"å®½æ¾åŒ¹é…æ•°å­—æå–: '{original_text}' -> {result}")
                return result
            except ValueError:
                pass
        
        self.logger.debug(f"æ•°å­—æå–å¤±è´¥: '{original_text}' -> 0")
        return 0
    
    async def parse_tweet_element(self, tweet_element) -> Optional[Dict[str, Any]]:
        """
        è§£æå•ä¸ªæ¨æ–‡å…ƒç´ 
        
        Args:
            tweet_element: æ¨æ–‡DOMå…ƒç´ 
            
        Returns:
            æ¨æ–‡æ•°æ®å­—å…¸
        """
        # å¦‚æœå¯ç”¨äº†ä¼˜åŒ–åŠŸèƒ½ï¼Œä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
        if self.optimization_enabled:
            self.logger.info(f"ğŸ”§ è°ƒç”¨ä¼˜åŒ–ç‰ˆæœ¬è§£ææ–¹æ³•ï¼Œoptimization_enabled={self.optimization_enabled}")
            result = await self.parse_tweet_element_optimized(tweet_element)
            self.logger.info(f"ğŸ”§ ä¼˜åŒ–ç‰ˆæœ¬è§£æç»“æœ: {result is not None}")
            return result
        
        try:
            tweet_data = {}
            
            # æ£€æŸ¥å…ƒç´ æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            try:
                await tweet_element.is_visible()
            except Exception:
                self.logger.warning("æ¨æ–‡å…ƒç´ å·²å¤±æ•ˆï¼Œè·³è¿‡è§£æ")
                return None
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–ç”¨æˆ·å
            try:
                # å°è¯•å¤šç§é€‰æ‹©å™¨
                username_selectors = [
                    '[data-testid="User-Name"] a',
                    '[data-testid="User-Names"] a',
                    'a[href^="/"][role="link"]'
                ]
                
                username_element = None
                for selector in username_selectors:
                    try:
                        username_element = await tweet_element.query_selector(selector)
                        if username_element:
                            break
                    except Exception:
                        continue
                
                if username_element:
                    username_href = await username_element.get_attribute('href')
                    if username_href and '/' in username_href:
                        tweet_data['username'] = username_href.split('/')[-1]
            except Exception as e:
                self.logger.debug(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–æ¨æ–‡å†…å®¹
            try:
                content_selectors = [
                    '[data-testid="tweetText"]',
                    '[data-testid="tweetText"] span',
                    'div[lang] span',
                    'div[dir="auto"] span',
                    'div[dir="ltr"] span',
                    'div[dir="rtl"] span',
                    '[lang] span',
                    'span[dir="auto"]',
                    'div[data-testid="tweetText"] > span',
                    'article div[lang] span',
                    'article span[dir]'
                ]
                
                content_text = ''
                # å°è¯•å¤šç§æ–¹å¼æå–å†…å®¹
                for selector in content_selectors:
                    try:
                        content_elements = await tweet_element.query_selector_all(selector)
                        if content_elements:
                            # æ”¶é›†æ‰€æœ‰æ–‡æœ¬å†…å®¹
                            text_parts = []
                            for elem in content_elements:
                                text = await elem.inner_text()
                                if text and text.strip():
                                    text_parts.append(text.strip())
                            
                            if text_parts:
                                content_text = ' '.join(text_parts)
                                break
                    except Exception:
                        continue
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œå°è¯•è·å–æ•´ä¸ªæ¨æ–‡åŒºåŸŸçš„æ–‡æœ¬
                if not content_text:
                    try:
                        # å°è¯•ä»æ•´ä¸ªæ¨æ–‡å…ƒç´ ä¸­æå–æ–‡æœ¬ï¼Œä½†æ’é™¤ç”¨æˆ·åã€æ—¶é—´ç­‰
                        all_text = await tweet_element.inner_text()
                        if all_text:
                            # ç®€å•è¿‡æ»¤ï¼Œç§»é™¤æ˜æ˜¾çš„éå†…å®¹æ–‡æœ¬
                            lines = all_text.split('\n')
                            filtered_lines = []
                            for line in lines:
                                line = line.strip()
                                # è·³è¿‡ç©ºè¡Œã€ç”¨æˆ·åã€æ—¶é—´æˆ³ç­‰
                                if (line and 
                                    not line.startswith('@') and 
                                    not line.endswith('h') and 
                                    not line.endswith('m') and 
                                    not line.endswith('s') and
                                    not line.isdigit() and
                                    len(line) > 3):
                                    filtered_lines.append(line)
                            
                            if filtered_lines:
                                content_text = ' '.join(filtered_lines[:3])  # å–å‰3è¡Œä½œä¸ºå†…å®¹
                    except Exception:
                        pass
                
                if content_text and content_text.strip():
                    tweet_data['content'] = content_text.strip()
                    
            except Exception as e:
                self.logger.debug(f"æå–æ¨æ–‡å†…å®¹å¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–å‘å¸ƒæ—¶é—´
            try:
                time_element = await tweet_element.query_selector('time')
                if time_element:
                    datetime_attr = await time_element.get_attribute('datetime')
                    if datetime_attr:
                        tweet_data['publish_time'] = datetime_attr
            except Exception as e:
                self.logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–æ¨æ–‡é“¾æ¥
            try:
                link_elements = await tweet_element.query_selector_all('a[href*="/status/"]')
                if link_elements:
                    href = await link_elements[0].get_attribute('href')
                    if href:
                        if href.startswith('/'):
                            tweet_data['link'] = f"https://x.com{href}"
                        else:
                            tweet_data['link'] = href
            except Exception as e:
                self.logger.debug(f"æå–æ¨æ–‡é“¾æ¥å¤±è´¥: {e}")
            
            # ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼æå–äº’åŠ¨æ•°æ®
            interaction_data = {'likes': 0, 'comments': 0, 'retweets': 0}
            
            # ç‚¹èµæ•° - å¢å¼ºç‰ˆæå–
            try:
                like_selectors = [
                    '[data-testid="like"]',
                    '[aria-label*="like"]',
                    '[aria-label*="Like"]',
                    '[aria-label*="å–œæ¬¢"]',
                    '[aria-label*="èµ"]',
                    'div[role="group"] div:first-child button',
                    'div[role="group"] > div:nth-child(4) button',
                    '[data-testid="tweet"] div[role="group"] button:nth-child(3)',
                    'button[aria-label*="heart"]',
                    'button[aria-label*="favorite"]'
                ]
                
                for selector in like_selectors:
                    try:
                        like_element = await tweet_element.query_selector(selector)
                        if like_element:
                            # å°è¯•ä» aria-label è·å–
                            like_text = await like_element.get_attribute('aria-label') or ''
                            if like_text:
                                extracted_likes = self.extract_number(like_text)
                                if extracted_likes > 0:
                                    interaction_data['likes'] = extracted_likes
                                    self.logger.debug(f"ä»aria-labelæå–ç‚¹èµæ•°: {extracted_likes}")
                                    break
                            
                            # å°è¯•ä»å†…éƒ¨æ–‡æœ¬è·å–
                            inner_text = await like_element.inner_text()
                            if inner_text and inner_text.strip():
                                extracted_likes = self.extract_number(inner_text)
                                if extracted_likes > 0:
                                    interaction_data['likes'] = extracted_likes
                                    self.logger.debug(f"ä»å†…éƒ¨æ–‡æœ¬æå–ç‚¹èµæ•°: {extracted_likes}")
                                    break
                    except Exception:
                        continue
            except Exception as e:
                self.logger.debug(f"æå–ç‚¹èµæ•°å¤±è´¥: {e}")
            
            # è¯„è®ºæ•° - å¢å¼ºç‰ˆæå–
            try:
                reply_selectors = [
                    '[data-testid="reply"]',
                    '[aria-label*="repl"]',
                    '[aria-label*="Reply"]',
                    '[aria-label*="å›å¤"]',
                    '[aria-label*="è¯„è®º"]',
                    'div[role="group"] div:first-child button',
                    'div[role="group"] > div:nth-child(1) button',
                    '[data-testid="tweet"] div[role="group"] button:first-child',
                    'button[aria-label*="comment"]'
                ]
                
                for selector in reply_selectors:
                    try:
                        reply_element = await tweet_element.query_selector(selector)
                        if reply_element:
                            # å°è¯•ä» aria-label è·å–
                            reply_text = await reply_element.get_attribute('aria-label') or ''
                            if reply_text:
                                extracted_replies = self.extract_number(reply_text)
                                if extracted_replies > 0:
                                    interaction_data['comments'] = extracted_replies
                                    self.logger.debug(f"ä»aria-labelæå–è¯„è®ºæ•°: {extracted_replies}")
                                    break
                            
                            # å°è¯•ä»å†…éƒ¨æ–‡æœ¬è·å–
                            inner_text = await reply_element.inner_text()
                            if inner_text and inner_text.strip():
                                extracted_replies = self.extract_number(inner_text)
                                if extracted_replies > 0:
                                    interaction_data['comments'] = extracted_replies
                                    self.logger.debug(f"ä»å†…éƒ¨æ–‡æœ¬æå–è¯„è®ºæ•°: {extracted_replies}")
                                    break
                    except Exception:
                        continue
            except Exception as e:
                self.logger.debug(f"æå–è¯„è®ºæ•°å¤±è´¥: {e}")
            
            # è½¬å‘æ•° - å¢å¼ºç‰ˆæå–
            try:
                retweet_selectors = [
                    '[data-testid="retweet"]',
                    '[aria-label*="retweet"]',
                    '[aria-label*="Retweet"]',
                    '[aria-label*="è½¬å‘"]',
                    '[aria-label*="è½¬æ¨"]',
                    'div[role="group"] div:nth-child(2) button',
                    'div[role="group"] > div:nth-child(2) button',
                    '[data-testid="tweet"] div[role="group"] button:nth-child(2)',
                    'button[aria-label*="repost"]'
                ]
                
                for selector in retweet_selectors:
                    try:
                        retweet_element = await tweet_element.query_selector(selector)
                        if retweet_element:
                            # å°è¯•ä» aria-label è·å–
                            retweet_text = await retweet_element.get_attribute('aria-label') or ''
                            if retweet_text:
                                extracted_retweets = self.extract_number(retweet_text)
                                if extracted_retweets > 0:
                                    interaction_data['retweets'] = extracted_retweets
                                    self.logger.debug(f"ä»aria-labelæå–è½¬å‘æ•°: {extracted_retweets}")
                                    break
                            
                            # å°è¯•ä»å†…éƒ¨æ–‡æœ¬è·å–
                            inner_text = await retweet_element.inner_text()
                            if inner_text and inner_text.strip():
                                extracted_retweets = self.extract_number(inner_text)
                                if extracted_retweets > 0:
                                    interaction_data['retweets'] = extracted_retweets
                                    self.logger.debug(f"ä»å†…éƒ¨æ–‡æœ¬æå–è½¬å‘æ•°: {extracted_retweets}")
                                    break
                    except Exception:
                        continue
            except Exception as e:
                self.logger.debug(f"æå–è½¬å‘æ•°å¤±è´¥: {e}")
            
            # é€šç”¨æ–¹æ³•ï¼šå°è¯•ä»æ¨æ–‡åº•éƒ¨çš„ç»Ÿè®¡åŒºåŸŸæå–æ•°æ®
            try:
                # æŸ¥æ‰¾åŒ…å«ç»Ÿè®¡æ•°æ®çš„åŒºåŸŸ
                stats_elements = await tweet_element.query_selector_all('div[role="group"] button')
                if stats_elements and len(stats_elements) >= 3:
                    for i, button in enumerate(stats_elements[:4]):  # é€šå¸¸å‰4ä¸ªæŒ‰é’®æ˜¯å›å¤ã€è½¬å‘ã€ç‚¹èµã€åˆ†äº«
                        try:
                            aria_label = await button.get_attribute('aria-label') or ''
                            inner_text = await button.inner_text() or ''
                            
                            # ç»„åˆæ–‡æœ¬è¿›è¡Œåˆ†æ
                            combined_text = f"{aria_label} {inner_text}".lower()
                            
                            if any(keyword in combined_text for keyword in ['reply', 'repl', 'å›å¤', 'è¯„è®º']) and interaction_data['comments'] == 0:
                                extracted_num = self.extract_number(combined_text)
                                if extracted_num > 0:
                                    interaction_data['comments'] = extracted_num
                                    self.logger.debug(f"é€šç”¨æ–¹æ³•æå–è¯„è®ºæ•°: {extracted_num}")
                            
                            elif any(keyword in combined_text for keyword in ['retweet', 'repost', 'è½¬å‘', 'è½¬æ¨']) and interaction_data['retweets'] == 0:
                                extracted_num = self.extract_number(combined_text)
                                if extracted_num > 0:
                                    interaction_data['retweets'] = extracted_num
                                    self.logger.debug(f"é€šç”¨æ–¹æ³•æå–è½¬å‘æ•°: {extracted_num}")
                            
                            elif any(keyword in combined_text for keyword in ['like', 'heart', 'favorite', 'å–œæ¬¢', 'èµ']) and interaction_data['likes'] == 0:
                                extracted_num = self.extract_number(combined_text)
                                if extracted_num > 0:
                                    interaction_data['likes'] = extracted_num
                                    self.logger.debug(f"é€šç”¨æ–¹æ³•æå–ç‚¹èµæ•°: {extracted_num}")
                        except Exception:
                            continue
            except Exception as e:
                self.logger.debug(f"é€šç”¨ç»Ÿè®¡æ•°æ®æå–å¤±è´¥: {e}")
            
            # åˆå¹¶äº’åŠ¨æ•°æ®
            tweet_data.update(interaction_data)
            
            # æå–åª’ä½“å†…å®¹ï¼ˆå›¾ç‰‡å’Œè§†é¢‘ï¼‰
            try:
                media_content = await self.extract_media_content(tweet_element)
                if media_content:
                    tweet_data['media'] = media_content
                    self.logger.debug(f"æå–åˆ°åª’ä½“å†…å®¹: {len(media_content.get('images', []))} å¼ å›¾ç‰‡, {len(media_content.get('videos', []))} ä¸ªè§†é¢‘")
            except Exception as e:
                self.logger.debug(f"æå–åª’ä½“å†…å®¹å¤±è´¥: {e}")
            
            # è®¾ç½®é»˜è®¤å€¼
            tweet_data.setdefault('username', 'unknown')
            tweet_data.setdefault('content', '')
            tweet_data.setdefault('publish_time', datetime.now().isoformat())
            tweet_data.setdefault('link', '')
            tweet_data.setdefault('likes', 0)
            tweet_data.setdefault('comments', 0)
            tweet_data.setdefault('retweets', 0)
            tweet_data.setdefault('media', {'images': [], 'videos': []})
            
            # è¯†åˆ«å¸–å­ç±»å‹
            try:
                tweet_data['post_type'] = self.identify_tweet_type(tweet_data)
                self.logger.debug(f"è¯†åˆ«å¸–å­ç±»å‹: {tweet_data['post_type']}")
            except Exception as e:
                self.logger.debug(f"è¯†åˆ«å¸–å­ç±»å‹å¤±è´¥: {e}")
                tweet_data['post_type'] = 'æ–‡å­—'
            
            # éªŒè¯æ¨æ–‡æ•°æ®çš„æœ‰æ•ˆæ€§ - è¿›ä¸€æ­¥æ”¾å®½éªŒè¯æ¡ä»¶
            # åªè¦æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶å°±è®¤ä¸ºæ˜¯æœ‰æ•ˆæ¨æ–‡ï¼š
            # 1. æœ‰ç”¨æˆ·å
            # 2. æœ‰å†…å®¹
            # 3. æœ‰é“¾æ¥
            # 4. æœ‰åª’ä½“å†…å®¹
            # 5. æœ‰äº’åŠ¨æ•°æ®ï¼ˆç‚¹èµã€è¯„è®ºã€è½¬å‘ï¼‰
            has_username = tweet_data.get('username', '') and tweet_data.get('username', '') != 'unknown'
            has_content = tweet_data.get('content', '').strip()
            has_link = tweet_data.get('link', '').strip()
            has_media = (tweet_data.get('media', {}).get('images', []) or 
                        tweet_data.get('media', {}).get('videos', []))
            has_interactions = (tweet_data.get('likes', 0) > 0 or 
                              tweet_data.get('comments', 0) > 0 or 
                              tweet_data.get('retweets', 0) > 0)
            
            # åªè¦æœ‰ä»»ä½•ä¸€é¡¹æœ‰æ•ˆä¿¡æ¯å°±ä¿ç•™æ¨æ–‡
            if not (has_username or has_content or has_link or has_media or has_interactions):
                self.logger.debug(f"æ¨æ–‡æ•°æ®æ— æ•ˆï¼Œè·³è¿‡ - ç”¨æˆ·å: {tweet_data.get('username', 'None')}, å†…å®¹é•¿åº¦: {len(tweet_data.get('content', ''))}, é“¾æ¥: {bool(tweet_data.get('link', ''))}, åª’ä½“: {bool(has_media)}, äº’åŠ¨: {bool(has_interactions)}")
                return None
            
            self.logger.debug(f"æ¨æ–‡éªŒè¯é€šè¿‡ - ç”¨æˆ·å: {tweet_data.get('username', 'None')}, å†…å®¹é•¿åº¦: {len(tweet_data.get('content', ''))}, é“¾æ¥: {bool(tweet_data.get('link', ''))}")
            
            return tweet_data
            
        except Exception as e:
            self.logger.error(f"è§£ææ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return None
    
    async def scrape_tweets(self, max_tweets: int = 10, enable_enhanced: bool = False, filter_criteria: dict = None) -> List[Dict[str, Any]]:
        """
        æŠ“å–å½“å‰é¡µé¢çš„æ¨æ–‡æ•°æ®
        
        Args:
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            filter_criteria: ç­›é€‰æ¡ä»¶ {'min_likes': int, 'min_comments': int, 'min_retweets': int}
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        if enable_enhanced:
            return await self.enhanced_tweet_scraping(max_tweets, filter_criteria=filter_criteria)
        
        tweets_data = []
        max_scroll_attempts = 200  # å¢åŠ æœ€å¤§æ»šåŠ¨æ¬¡æ•°ä»¥ç¡®ä¿èƒ½æŠ“å–è¶³å¤Ÿæ»¡è¶³æ¡ä»¶çš„æ¨æ–‡
        scroll_attempts = 0
        last_tweet_count = 0
        no_new_tweets_count = 0
        consecutive_empty_scrolls = 0  # è¿ç»­ç©ºæ»šåŠ¨è®¡æ•°å™¨
        total_parsed_tweets = 0  # æ€»è§£ææ¨æ–‡æ•°ï¼ˆåŒ…æ‹¬ä¸æ»¡è¶³æ¡ä»¶çš„ï¼‰
        
        try:
            self.logger.info(f"å¼€å§‹æŠ“å–æ¨æ–‡ï¼Œç›®æ ‡æ•°é‡: {max_tweets}")
            
            while len(tweets_data) < max_tweets and scroll_attempts < max_scroll_attempts:
                # ç­‰å¾…æ¨æ–‡åŠ è½½
                try:
                    await self.page.wait_for_selector('[data-testid="tweet"]', timeout=5000)
                except Exception:
                    self.logger.warning(f"ç­‰å¾…æ¨æ–‡å…ƒç´ è¶…æ—¶ï¼Œå½“å‰æ»šåŠ¨æ¬¡æ•°: {scroll_attempts}")
                    if scroll_attempts == 0:  # ç¬¬ä¸€æ¬¡å°±æ²¡æ‰¾åˆ°æ¨æ–‡ï¼Œå¯èƒ½é¡µé¢æœ‰é—®é¢˜
                        break
                
                # è·å–å½“å‰é¡µé¢çš„æ¨æ–‡å…ƒç´ 
                tweet_elements = await self.page.query_selector_all('[data-testid="tweet"]')
                current_tweet_count = len(tweet_elements)
                
                self.logger.info(f"æ»šåŠ¨ç¬¬ {scroll_attempts + 1} æ¬¡ï¼Œé¡µé¢æ¨æ–‡æ•°: {current_tweet_count}ï¼Œå·²æŠ“å–: {len(tweets_data)}")
                
                # è§£ææ–°çš„æ¨æ–‡
                new_tweets_parsed = 0
                new_valid_tweets = 0  # æ–°å¢ï¼šæ»¡è¶³ç­›é€‰æ¡ä»¶çš„æ¨æ–‡æ•°
                for i, tweet_element in enumerate(tweet_elements):
                    if len(tweets_data) >= max_tweets:
                        break
                    
                    # æ¯éš”å‡ æ¡æ¨æ–‡æ£€æŸ¥é¡µé¢ç„¦ç‚¹
                    if i % 5 == 0:
                        await self.ensure_page_focus()
                    
                    tweet_data = await self.parse_tweet_element(tweet_element)
                    if tweet_data:
                        total_parsed_tweets += 1
                        
                        # æ£€æŸ¥æ˜¯å¦å·²ç»æŠ“å–è¿‡è¿™æ¡æ¨æ–‡ï¼ˆå»é‡ï¼‰
                        tweet_link = tweet_data.get('link', '')
                        if tweet_link:
                            # ä¼˜å…ˆä½¿ç”¨æ¨æ–‡é“¾æ¥ä½œä¸ºå”¯ä¸€æ ‡è¯†
                            tweet_id = tweet_link
                        else:
                            # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œä½¿ç”¨å†…å®¹çš„å“ˆå¸Œå€¼ä½œä¸ºæ ‡è¯†ï¼ˆæ›´å®½æ¾çš„å»é‡ï¼‰
                            content = tweet_data.get('content', '').strip()
                            if len(content) > 20:  # åªå¯¹æœ‰è¶³å¤Ÿå†…å®¹çš„æ¨æ–‡è¿›è¡Œå»é‡
                                content_hash = hash(content[:100])  # åªä½¿ç”¨å‰100ä¸ªå­—ç¬¦
                                tweet_id = f"content_{content_hash}"
                            else:
                                # å¯¹äºçŸ­å†…å®¹ï¼Œä½¿ç”¨æ›´è¯¦ç»†çš„æ ‡è¯†é¿å…è¯¯åˆ¤
                                detail_hash = hash(f"{tweet_data.get('username', '')}{content}{tweet_data.get('timestamp', '')}{i}")
                                tweet_id = f"detail_{detail_hash}"
                        
                        if tweet_id not in self.seen_tweet_ids:
                            self.seen_tweet_ids.add(tweet_id)
                            new_tweets_parsed += 1
                            
                            # åº”ç”¨ç­›é€‰æ¡ä»¶
                            if self._meets_filter_criteria(tweet_data, filter_criteria):
                                tweets_data.append(tweet_data)
                                new_valid_tweets += 1
                                self.logger.debug(f"æ–°æŠ“å–æœ‰æ•ˆæ¨æ–‡: @{tweet_data.get('username', 'unknown')} (ç‚¹èµ:{tweet_data.get('likes', 0)}, è½¬å‘:{tweet_data.get('retweets', 0)})")
                            else:
                                self.logger.debug(f"æ¨æ–‡ä¸æ»¡è¶³ç­›é€‰æ¡ä»¶: @{tweet_data.get('username', 'unknown')} (ç‚¹èµ:{tweet_data.get('likes', 0)}, è½¬å‘:{tweet_data.get('retweets', 0)})")
                        else:
                            self.logger.debug(f"è·³è¿‡é‡å¤æ¨æ–‡: {tweet_id[:50]}...")
                
                self.logger.info(f"æœ¬æ¬¡æ»šåŠ¨æ–°è§£ææ¨æ–‡: {new_tweets_parsed}ï¼Œæ»¡è¶³æ¡ä»¶: {new_valid_tweets}ï¼Œç´¯è®¡æœ‰æ•ˆ: {len(tweets_data)}/{max_tweets}ï¼Œæ€»è§£æ: {total_parsed_tweets}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡
                if current_tweet_count <= last_tweet_count:
                    no_new_tweets_count += 1
                    self.logger.info(f"é¡µé¢æ¨æ–‡æ•°é‡æœªå¢åŠ ï¼Œè¿ç»­æ¬¡æ•°: {no_new_tweets_count}")
                else:
                    no_new_tweets_count = 0
                
                # æ£€æŸ¥æœ¬æ¬¡æ»šåŠ¨æ˜¯å¦è§£æåˆ°æ–°çš„æœ‰æ•ˆæ¨æ–‡
                if new_valid_tweets == 0:
                    consecutive_empty_scrolls += 1
                    self.logger.info(f"æœ¬æ¬¡æ»šåŠ¨æœªè·å¾—æ»¡è¶³æ¡ä»¶çš„æ¨æ–‡ï¼Œè¿ç»­ç©ºæ»šåŠ¨æ¬¡æ•°: {consecutive_empty_scrolls}")
                    
                    # å¦‚æœè¿ç»­å¾ˆå¤šæ¬¡æ²¡æœ‰è·å¾—æœ‰æ•ˆæ¨æ–‡ï¼Œä¸”é¡µé¢æ¨æ–‡æ•°é‡ä¹Ÿä¸å¢åŠ ï¼Œåˆ™åœæ­¢
                    if consecutive_empty_scrolls >= 20 and no_new_tweets_count >= 10:
                        self.logger.info("è¿ç»­å¤šæ¬¡æ»šåŠ¨æœªè·å¾—æ»¡è¶³æ¡ä»¶çš„æ¨æ–‡ä¸”é¡µé¢æ— æ–°æ¨æ–‡ï¼Œå¯èƒ½å·²åˆ°é¡µé¢åº•éƒ¨æˆ–æ— æ›´å¤šæ»¡è¶³æ¡ä»¶çš„æ¨æ–‡")
                        break
                else:
                    consecutive_empty_scrolls = 0
                
                last_tweet_count = current_tweet_count
                
                # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œåœæ­¢æ»šåŠ¨
                if len(tweets_data) >= max_tweets:
                    self.logger.info(f"å·²è¾¾åˆ°ç›®æ ‡æ¨æ–‡æ•°é‡: {len(tweets_data)}/{max_tweets}")
                    break
                
                # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šæ¨æ–‡
                scroll_attempts += 1
                if scroll_attempts < max_scroll_attempts:
                    # æ ¹æ®è¿ç»­ç©ºæ»šåŠ¨æ¬¡æ•°è°ƒæ•´æ»šåŠ¨è·ç¦»
                    scroll_distance = 1500 if consecutive_empty_scrolls > 3 else 1200
                    await self.page.evaluate(f'window.scrollBy(0, {scroll_distance})')
                    
                    # æ ¹æ®æƒ…å†µè°ƒæ•´ç­‰å¾…æ—¶é—´
                    wait_time = 3 if consecutive_empty_scrolls > 5 else 2
                    await asyncio.sleep(wait_time)  # ç­‰å¾…åŠ è½½
                    
                    # å¤„ç†å¯èƒ½çš„å¼¹çª—
                    try:
                        await self.dismiss_translate_popup()
                    except Exception:
                        pass
            
            # åªè¿”å›ç›®æ ‡æ•°é‡çš„æ¨æ–‡
            final_tweets = tweets_data[:max_tweets]
            filter_info = f"ï¼ˆç­›é€‰æ¡ä»¶: {filter_criteria}ï¼‰" if filter_criteria else "ï¼ˆæ— ç­›é€‰æ¡ä»¶ï¼‰"
            self.logger.info(f"æ¨æ–‡æŠ“å–å®Œæˆ{filter_info}ï¼Œç›®æ ‡: {max_tweets}ï¼Œå®é™…è·å–: {len(final_tweets)}ï¼Œæ€»è§£æ: {total_parsed_tweets}ï¼Œæ»šåŠ¨æ¬¡æ•°: {scroll_attempts}")
            
            if len(final_tweets) < max_tweets:
                shortage = max_tweets - len(final_tweets)
                self.logger.warning(f"æ»¡è¶³æ¡ä»¶çš„æ¨æ–‡æ•°é‡ä¸è¶³ï¼Œç¼ºå°‘ {shortage} æ¡æ¨æ–‡ï¼ˆæ€»å…±è§£æäº† {total_parsed_tweets} æ¡æ¨æ–‡ï¼‰")
            
            return final_tweets
            
        except Exception as e:
            self.logger.error(f"æŠ“å–æ¨æ–‡å¤±è´¥: {e}")
            return tweets_data[:max_tweets] if tweets_data else []
    
    def _meets_filter_criteria(self, tweet_data: Dict[str, Any], filter_criteria: dict = None) -> bool:
        """
        æ£€æŸ¥æ¨æ–‡æ˜¯å¦æ»¡è¶³ç­›é€‰æ¡ä»¶
        
        Args:
            tweet_data: æ¨æ–‡æ•°æ®
            filter_criteria: ç­›é€‰æ¡ä»¶å­—å…¸
            
        Returns:
            æ˜¯å¦æ»¡è¶³ç­›é€‰æ¡ä»¶
        """
        if not filter_criteria:
            return True
        
        # è·å–æ¨æ–‡çš„äº’åŠ¨æ•°æ®
        likes = tweet_data.get('likes', 0)
        comments = tweet_data.get('comments', 0)
        retweets = tweet_data.get('retweets', 0)
        
        # æ£€æŸ¥æœ€å°ç‚¹èµæ•°
        min_likes = filter_criteria.get('min_likes', 0)
        if likes < min_likes:
            return False
        
        # æ£€æŸ¥æœ€å°è¯„è®ºæ•°
        min_comments = filter_criteria.get('min_comments', 0)
        if comments < min_comments:
            return False
        
        # æ£€æŸ¥æœ€å°è½¬å‘æ•°
        min_retweets = filter_criteria.get('min_retweets', 0)
        if retweets < min_retweets:
            return False
        
        return True
    
    async def scrape_user_tweets(self, username: str, max_tweets: int = 10, enable_enhanced: bool = False, filter_criteria: dict = None) -> List[Dict[str, Any]]:
        """
        æŠ“å–æŒ‡å®šç”¨æˆ·çš„æ¨æ–‡
        
        Args:
            username: Twitter ç”¨æˆ·å
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            filter_criteria: ç­›é€‰æ¡ä»¶ {'min_likes': int, 'min_comments': int, 'min_retweets': int}
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹
            await self.ensure_page_focus()
            
            await self.navigate_to_profile(username)
            
            # ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œé¡µé¢æ¢ç´¢
            if self.behavior_simulator:
                self.logger.info(f"å¼€å§‹æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆ @{username} çš„é¡µé¢")
                await self.behavior_simulator.simulate_page_exploration()
                
                # æ¨¡æ‹Ÿé˜…è¯»è¡Œä¸º
                await self.behavior_simulator.simulate_reading_behavior()
            else:
                await asyncio.sleep(0.8)  # æé€Ÿå›é€€ç­‰å¾…
            
            tweets = await self.scrape_tweets(max_tweets, enable_enhanced, filter_criteria)
            
            # æ¨¡æ‹Ÿç”¨æˆ·ä¼šè¯ç»“æŸè¡Œä¸º
            if self.behavior_simulator:
                await self.behavior_simulator.simulate_natural_browsing([f"https://twitter.com/{username}"])
            
            # ä¸ºæ¯æ¡æ¨æ–‡æ·»åŠ æ¥æºä¿¡æ¯
            for tweet in tweets:
                tweet['source'] = f'@{username}'
                tweet['source_type'] = 'user_profile'
            
            return tweets
            
        except Exception as e:
            self.logger.error(f"æŠ“å–ç”¨æˆ· @{username} çš„æ¨æ–‡å¤±è´¥: {e}")
            return []
    
    async def scrape_keyword_tweets(self, keyword: str, max_tweets: int = 10, enable_enhanced: bool = False, filter_criteria: dict = None) -> List[Dict[str, Any]]:
        """
        æŠ“å–åŒ…å«æŒ‡å®šå…³é”®è¯çš„æ¨æ–‡
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            filter_criteria: ç­›é€‰æ¡ä»¶ {'min_likes': int, 'min_comments': int, 'min_retweets': int}
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹
            await self.ensure_page_focus()
            
            await self.search_tweets(keyword)
            
            # ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œæœç´¢é¡µé¢æ¢ç´¢
            if self.behavior_simulator:
                self.logger.info(f"å¼€å§‹æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆå…³é”®è¯ '{keyword}' çš„æœç´¢ç»“æœ")
                await self.behavior_simulator.simulate_page_exploration()
                
                # æ¨¡æ‹Ÿé˜…è¯»æœç´¢ç»“æœ
                await self.behavior_simulator.simulate_reading_behavior()
            else:
                await asyncio.sleep(0.8)  # æé€Ÿå›é€€ç­‰å¾…
            
            tweets = await self.scrape_tweets(max_tweets, enable_enhanced, filter_criteria)
            
            # æ¨¡æ‹Ÿæœç´¢ä¼šè¯ç»“æŸè¡Œä¸º
            if self.behavior_simulator:
                await self.behavior_simulator.simulate_natural_browsing([f"https://twitter.com/search?q={keyword}"])
            
            # ä¸ºæ¯æ¡æ¨æ–‡æ·»åŠ æ¥æºä¿¡æ¯
            for tweet in tweets:
                tweet['source'] = keyword
                tweet['source_type'] = 'keyword_search'
            
            return tweets
            
        except Exception as e:
            self.logger.error(f"æŠ“å–å…³é”®è¯ '{keyword}' çš„æ¨æ–‡å¤±è´¥: {e}")
            return []
    
    async def scrape_user_keyword_tweets(self, username: str, keyword: str, max_tweets: int = 10, enable_enhanced: bool = False) -> List[Dict[str, Any]]:
        """
        åœ¨æŒ‡å®šç”¨æˆ·ä¸‹æœç´¢åŒ…å«å…³é”®è¯çš„æ¨æ–‡
        
        Args:
            username: Twitter ç”¨æˆ·å
            keyword: æœç´¢å…³é”®è¯
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹
            await self.ensure_page_focus()
            
            self.logger.info(f"å¼€å§‹åœ¨ç”¨æˆ· @{username} ä¸‹æœç´¢å…³é”®è¯ '{keyword}'")
            
            # æ„å»ºç”¨æˆ·ç‰¹å®šçš„æœç´¢URL
            import urllib.parse
            encoded_keyword = urllib.parse.quote(keyword)
            encoded_username = urllib.parse.quote(username)
            # ä½¿ç”¨ from:username è¯­æ³•åœ¨ç‰¹å®šç”¨æˆ·ä¸‹æœç´¢
            search_query = f"from:{encoded_username} {encoded_keyword}"
            search_url = f'https://x.com/search?q={urllib.parse.quote(search_query)}&src=typed_query&f=live'
            
            self.logger.info(f"ç”¨æˆ·å…³é”®è¯æœç´¢URL: {search_url}")
            
            # å¯¼èˆªåˆ°æœç´¢é¡µé¢
            await self.page.goto(search_url, timeout=BROWSER_CONFIG['timeout'])
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            try:
                await self.page.wait_for_load_state('domcontentloaded', timeout=10000)
                self.logger.info("DOMå†…å®¹åŠ è½½å®Œæˆ")
            except Exception as load_error:
                self.logger.warning(f"DOMåŠ è½½è¶…æ—¶: {load_error}")
            
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹åå†ç­‰å¾…æœç´¢ç»“æœ
            await self.ensure_page_focus()
            
            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½ï¼ˆæé€Ÿæ¨¡å¼ï¼‰
            self.logger.info("ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")
            await asyncio.sleep(0.8)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
            try:
                await self.page.wait_for_selector('[data-testid="tweet"]', timeout=10000)
                self.logger.info("æ‰¾åˆ°æ¨æ–‡å…ƒç´ ")
            except Exception:
                self.logger.warning("æœªæ‰¾åˆ°æ¨æ–‡å…ƒç´ ï¼Œå¯èƒ½æ²¡æœ‰æœç´¢ç»“æœ")
            
            # ä½¿ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œæœç´¢é¡µé¢æ¢ç´¢
            if self.behavior_simulator:
                self.logger.info(f"å¼€å§‹æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆ @{username} ä¸‹å…³é”®è¯ '{keyword}' çš„æœç´¢ç»“æœ")
                await self.behavior_simulator.simulate_page_exploration()
                await self.behavior_simulator.simulate_reading_behavior()
            else:
                await asyncio.sleep(0.8)  # æé€Ÿå›é€€ç­‰å¾…
            
            tweets = await self.scrape_tweets(max_tweets, enable_enhanced)
            
            # ä¸ºæ¯æ¡æ¨æ–‡æ·»åŠ æ¥æºä¿¡æ¯
            for tweet in tweets:
                tweet['source'] = f"@{username} + {keyword}"
                tweet['source_type'] = 'user_keyword_search'
                tweet['target_username'] = username
                tweet['target_keyword'] = keyword
            
            self.logger.info(f"åœ¨ç”¨æˆ· @{username} ä¸‹æœç´¢å…³é”®è¯ '{keyword}' å®Œæˆï¼Œè·å¾— {len(tweets)} æ¡æ¨æ–‡")
            return tweets
            
        except Exception as e:
            self.logger.error(f"åœ¨ç”¨æˆ· @{username} ä¸‹æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
            return []
    
    async def scrape_tweet_details(self, tweet_url: str) -> Dict[str, Any]:
        """
        æŠ“å–æ¨æ–‡è¯¦æƒ…é¡µçš„å®Œæ•´å†…å®¹
        
        Args:
            tweet_url: æ¨æ–‡è¯¦æƒ…é¡µURL
            
        Returns:
            åŒ…å«å®Œæ•´å†…å®¹çš„æ¨æ–‡æ•°æ®
        """
        try:
            self.logger.info(f"å¼€å§‹æŠ“å–æ¨æ–‡è¯¦æƒ…: {tweet_url}")
            
            # å¯¼èˆªåˆ°æ¨æ–‡è¯¦æƒ…é¡µ
            await self.page.goto(tweet_url, timeout=BROWSER_CONFIG['navigation_timeout'])
            await self.page.wait_for_load_state('domcontentloaded', timeout=30000)
            
            # ç­‰å¾…å†…å®¹åŠ è½½ï¼ˆæé€Ÿæ¨¡å¼ï¼‰
            await asyncio.sleep(2)
            
            # æŠ“å–å®Œæ•´çš„æ¨æ–‡å†…å®¹
            full_content = await self.extract_full_tweet_content()
            
            # æŠ“å–å¤šåª’ä½“å†…å®¹
            media_content = await self.extract_media_content()
            
            # æŠ“å–æ¨æ–‡çº¿ç¨‹
            thread_tweets = await self.extract_tweet_thread()
            
            # æŠ“å–å¼•ç”¨æ¨æ–‡
            quoted_tweet = await self.extract_quoted_tweet()
            
            return {
                'full_content': full_content,
                'media': media_content,
                'thread': thread_tweets,
                'quoted_tweet': quoted_tweet,
                'has_detailed_content': True
            }
            
        except Exception as e:
            self.logger.error(f"æŠ“å–æ¨æ–‡è¯¦æƒ…å¤±è´¥: {e}")
            return {'has_detailed_content': False}
    
    async def extract_full_tweet_content(self) -> str:
        """
        æå–æ¨æ–‡çš„å®Œæ•´å†…å®¹æ–‡æœ¬
        
        Returns:
            å®Œæ•´çš„æ¨æ–‡å†…å®¹
        """
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–å®Œæ•´å†…å®¹
            content_selectors = [
                '[data-testid="tweetText"]',
                '[lang] span',
                'div[dir="auto"] span',
                'article div[lang] span'
            ]
            
            full_content = ""
            for selector in content_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        content_parts = []
                        for element in elements:
                            text = await element.inner_text()
                            if text and text.strip():
                                content_parts.append(text.strip())
                        
                        if content_parts:
                            full_content = ' '.join(content_parts)
                            break
                except Exception:
                    continue
            
            return full_content
            
        except Exception as e:
            self.logger.error(f"æå–å®Œæ•´æ¨æ–‡å†…å®¹å¤±è´¥: {e}")
            return ""
    
    def identify_tweet_type(self, tweet_data: Dict[str, Any]) -> str:
        """
        è¯†åˆ«æ¨æ–‡çš„ç±»å‹
        
        Args:
            tweet_data: æ¨æ–‡æ•°æ®
            
        Returns:
            æ¨æ–‡ç±»å‹: 'æ–‡å­—', 'å›¾æ–‡', 'è§†é¢‘', 'åµŒå¥—è§†é¢‘', 'åµŒå¥—å›¾æ–‡'
        """
        try:
            content = tweet_data.get('content', '')
            media = tweet_data.get('media', {})
            quoted_tweet = tweet_data.get('quoted_tweet')
            
            # è·å–åª’ä½“æ•°é‡
            image_count = len(media.get('images', []))
            video_count = len(media.get('videos', []))
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼•ç”¨æ¨æ–‡ï¼ˆåµŒå¥—ï¼‰
            has_quoted = quoted_tweet is not None
            
            # åˆ¤æ–­ç±»å‹
            if has_quoted:
                # åµŒå¥—æ¨æ–‡
                if video_count > 0:
                    return 'åµŒå¥—è§†é¢‘'
                elif image_count > 0:
                    return 'åµŒå¥—å›¾æ–‡'
                else:
                    return 'åµŒå¥—æ–‡å­—'
            else:
                # æ™®é€šæ¨æ–‡
                if video_count > 0:
                    return 'è§†é¢‘'
                elif image_count > 0:
                    return 'å›¾æ–‡'
                else:
                    return 'æ–‡å­—'
                    
        except Exception as e:
            self.logger.error(f"è¯†åˆ«æ¨æ–‡ç±»å‹å¤±è´¥: {e}")
            return 'æ–‡å­—'  # é»˜è®¤è¿”å›æ–‡å­—ç±»å‹
    
    async def extract_media_content(self, tweet_element=None) -> Dict[str, List[Dict[str, Any]]]:
        """
        æå–æ¨æ–‡ä¸­çš„å›¾ç‰‡ã€è§†é¢‘ç­‰å¤šåª’ä½“å†…å®¹
        
        Args:
            tweet_element: æ¨æ–‡å…ƒç´ ï¼Œå¦‚æœä¸ºNoneåˆ™åœ¨æ•´ä¸ªé¡µé¢ä¸­æœç´¢
        
        Returns:
            åŒ…å«imageså’Œvideosåˆ—è¡¨çš„å­—å…¸
        """
        media_content = {'images': [], 'videos': []}
        
        try:
            # ç¡®å®šæœç´¢èŒƒå›´
            search_context = tweet_element if tweet_element else self.page
            
            # æŠ“å–å›¾ç‰‡
            image_selectors = [
                '[data-testid="tweetPhoto"] img',
                'img[src*="pbs.twimg.com"]',
                'div[data-testid="tweetPhoto"] img',
                '[data-testid="card.layoutLarge.media"] img'
            ]
            
            for selector in image_selectors:
                try:
                    images = await search_context.query_selector_all(selector)
                    for img in images:
                        src = await img.get_attribute('src')
                        alt = await img.get_attribute('alt') or ''
                        if src and ('pbs.twimg.com' in src or 'twimg.com' in src):
                            media_content['images'].append({
                                'type': 'image',
                                'url': src,
                                'description': alt,
                                'original_url': src.replace(':small', ':orig').replace(':medium', ':orig').replace(':large', ':orig')
                            })
                    if images:
                        break
                except Exception:
                    continue
            
            # æŠ“å–è§†é¢‘
            video_selectors = [
                'video',
                '[data-testid="videoPlayer"] video',
                'div[data-testid="videoComponent"] video',
                '[data-testid="previewInterstitial"] video'
            ]
            
            for selector in video_selectors:
                try:
                    videos = await search_context.query_selector_all(selector)
                    for video in videos:
                        poster = await video.get_attribute('poster')
                        src = await video.get_attribute('src')
                        media_content['videos'].append({
                            'type': 'video',
                            'poster': poster,
                            'url': src,
                            'description': 'è§†é¢‘å†…å®¹'
                        })
                    if videos:
                        break
                except Exception:
                    continue
            
            # æŠ“å–GIFï¼ˆä½œä¸ºç‰¹æ®Šçš„å›¾ç‰‡ç±»å‹ï¼‰
            gif_selectors = [
                'img[src*="video.twimg.com"]',
                '[data-testid="tweetPhoto"] img[src*=".gif"]'
            ]
            
            for selector in gif_selectors:
                try:
                    gifs = await search_context.query_selector_all(selector)
                    for gif in gifs:
                        src = await gif.get_attribute('src')
                        alt = await gif.get_attribute('alt') or ''
                        if src:
                            media_content['images'].append({
                                'type': 'gif',
                                'url': src,
                                'description': alt
                            })
                    if gifs:
                        break
                except Exception:
                    continue
            
            total_media = len(media_content['images']) + len(media_content['videos'])
            if total_media > 0:
                self.logger.debug(f"æå–åˆ° {len(media_content['images'])} å¼ å›¾ç‰‡, {len(media_content['videos'])} ä¸ªè§†é¢‘")
            
            return media_content
            
        except Exception as e:
            self.logger.error(f"æå–å¤šåª’ä½“å†…å®¹å¤±è´¥: {e}")
            return {'images': [], 'videos': []}
    
    async def extract_tweet_thread(self) -> List[Dict[str, Any]]:
        """
        æå–æ¨æ–‡çº¿ç¨‹ï¼ˆè¿ç»­çš„ç›¸å…³æ¨æ–‡ï¼‰
        
        Returns:
            æ¨æ–‡çº¿ç¨‹åˆ—è¡¨
        """
        thread_tweets = []
        
        try:
            # æŸ¥æ‰¾çº¿ç¨‹ä¸­çš„å…¶ä»–æ¨æ–‡
            thread_selectors = [
                'article[data-testid="tweet"]',
                '[data-testid="tweet"]'
            ]
            
            for selector in thread_selectors:
                try:
                    tweet_elements = await self.page.query_selector_all(selector)
                    
                    # å¦‚æœæ‰¾åˆ°å¤šæ¡æ¨æ–‡ï¼Œè¯´æ˜å¯èƒ½æ˜¯çº¿ç¨‹
                    if len(tweet_elements) > 1:
                        for i, element in enumerate(tweet_elements[1:], 1):  # è·³è¿‡ç¬¬ä¸€æ¡ï¼ˆä¸»æ¨æ–‡ï¼‰
                            thread_tweet = await self.parse_tweet_element(element)
                            if thread_tweet:
                                thread_tweet['thread_position'] = i
                                thread_tweets.append(thread_tweet)
                    
                    break
                except Exception:
                    continue
            
            self.logger.info(f"æå–åˆ° {len(thread_tweets)} æ¡çº¿ç¨‹æ¨æ–‡")
            return thread_tweets
            
        except Exception as e:
            self.logger.error(f"æå–æ¨æ–‡çº¿ç¨‹å¤±è´¥: {e}")
            return []
    
    async def extract_quoted_tweet(self) -> Optional[Dict[str, Any]]:
        """
        æå–å¼•ç”¨çš„æ¨æ–‡å†…å®¹
        
        Returns:
            å¼•ç”¨æ¨æ–‡æ•°æ®
        """
        try:
            # æŸ¥æ‰¾å¼•ç”¨æ¨æ–‡
            quoted_selectors = [
                '[data-testid="quoteTweet"]',
                'div[role="blockquote"]',
                'blockquote'
            ]
            
            for selector in quoted_selectors:
                try:
                    quoted_element = await self.page.query_selector(selector)
                    if quoted_element:
                        quoted_tweet = await self.parse_tweet_element(quoted_element)
                        if quoted_tweet:
                            quoted_tweet['is_quoted'] = True
                            self.logger.info("æå–åˆ°å¼•ç”¨æ¨æ–‡")
                            return quoted_tweet
                except Exception:
                    continue
            
            return None
            
        except Exception as e:
            self.logger.error(f"æå–å¼•ç”¨æ¨æ–‡å¤±è´¥: {e}")
            return None
    
    def is_content_truncated(self, content: str) -> bool:
        """
        æ£€æµ‹å†…å®¹æ˜¯å¦è¢«æˆªæ–­
        
        Args:
            content: æ¨æ–‡å†…å®¹
            
        Returns:
            æ˜¯å¦è¢«æˆªæ–­
        """
        truncation_indicators = [
            content.endswith('...'),
            content.endswith('â€¦'),
            len(content) > 280 and not content.endswith('.'),
            'æ˜¾ç¤ºæ›´å¤š' in content,
            'Show more' in content,
            'æŸ¥çœ‹æ›´å¤š' in content,
            'See more' in content
        ]
        return any(truncation_indicators)
    
    def has_rich_media(self, tweet: Dict[str, Any]) -> bool:
        """
        æ£€æµ‹æ˜¯å¦åŒ…å«ä¸°å¯Œåª’ä½“å†…å®¹
        
        Args:
            tweet: æ¨æ–‡æ•°æ®
            
        Returns:
            æ˜¯å¦åŒ…å«åª’ä½“å†…å®¹
        """
        content = tweet.get('content', '')
        media_indicators = [
            'ğŸ“·' in content,
            'ğŸ¥' in content,
            'ğŸ“¸' in content,
            'ğŸ¬' in content,
            'å›¾ç‰‡' in content,
            'è§†é¢‘' in content,
            'ç…§ç‰‡' in content,
            'photo' in content.lower(),
            'video' in content.lower(),
            'image' in content.lower(),
            tweet.get('media_count', 0) > 0
        ]
        return any(media_indicators)
    
    def is_thread_content(self, content: str) -> bool:
        """
        è¯†åˆ«æ¨æ–‡çº¿ç¨‹
        
        Args:
            content: æ¨æ–‡å†…å®¹
            
        Returns:
            æ˜¯å¦ä¸ºçº¿ç¨‹å†…å®¹
        """
        import re
        
        thread_patterns = [
            r'\d+/\d+',  # 1/5, 2/10 ç­‰æ ¼å¼
            r'\d+/',     # 1/, 2/ ç­‰æ ¼å¼
            r'\(\d+/\d+\)',  # (1/5) æ ¼å¼
        ]
        
        thread_indicators = [
            'ğŸ§µ', 'ğŸ“', 'ğŸ‘‡', 'â¬‡ï¸',
            'Thread', 'thread', 'çº¿ç¨‹',
            'æ¥ä¸‹æ¥', 'ç»§ç»­', 'continued',
            'ä¸‹ä¸€æ¡', 'next tweet'
        ]
        
        # æ£€æŸ¥æ­£åˆ™æ¨¡å¼
        for pattern in thread_patterns:
            if re.search(pattern, content):
                return True
        
        # æ£€æŸ¥çº¿ç¨‹æŒ‡ç¤ºè¯
        return any(indicator in content for indicator in thread_indicators)
    
    def is_high_value_content(self, tweet: Dict[str, Any]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºé«˜ä»·å€¼å†…å®¹
        
        Args:
            tweet: æ¨æ–‡æ•°æ®
            
        Returns:
            æ˜¯å¦ä¸ºé«˜ä»·å€¼å†…å®¹
        """
        content = tweet.get('content', '').lower()
        
        # è®¡ç®—äº’åŠ¨åˆ†æ•°
        engagement_score = (
            tweet.get('likes', 0) * 1 +
            tweet.get('retweets', 0) * 2 +
            tweet.get('comments', 0) * 3
        )
        
        # é«˜ä»·å€¼å…³é”®è¯
        value_keywords = [
            'æ•™ç¨‹', 'æ–¹æ³•', 'æŠ€å·§', 'ç»éªŒ', 'åˆ†äº«', 'å¹²è´§',
            'tutorial', 'guide', 'tips', 'how to', 'method',
            'æ”»ç•¥', 'ç§˜ç±', 'å¿ƒå¾—', 'æ€»ç»“', 'å¤ç›˜',
            'å·¥å…·', 'èµ„æº', 'æ¨è', 'tools', 'resources'
        ]
        
        value_indicators = [
            engagement_score > 50,  # é«˜äº’åŠ¨
            len(tweet.get('content', '')) > 200,  # é•¿å†…å®¹
            any(keyword in content for keyword in value_keywords),
            tweet.get('comments', 0) > 20,  # é«˜è¯„è®ºæ•°
            tweet.get('retweets', 0) > 10   # é«˜è½¬å‘æ•°
        ]
        
        return any(value_indicators)
    
    def get_scraping_strategy(self, account_type: str = 'general', follower_count: int = 0) -> Dict[str, Any]:
        """
        æ ¹æ®è´¦å·ç±»å‹è·å–æŠ“å–ç­–ç•¥
        
        Args:
            account_type: è´¦å·ç±»å‹
            follower_count: ç²‰ä¸æ•°é‡
            
        Returns:
            æŠ“å–ç­–ç•¥é…ç½®
        """
        strategies = {
            'æŠ€æœ¯åšä¸»': {
                'detail_threshold': 0.4,
                'engagement_threshold': 30,
                'content_length_threshold': 150,
                'priority_keywords': ['ä»£ç ', 'æŠ€æœ¯', 'å¼€å‘', 'code', 'tech']
            },
            'è¥é”€åšä¸»': {
                'detail_threshold': 0.6,
                'engagement_threshold': 15,
                'content_length_threshold': 100,
                'priority_keywords': ['è¥é”€', 'æ¨å¹¿', 'å˜ç°', 'marketing']
            },
            'æŠ•èµ„åšä¸»': {
                'detail_threshold': 0.5,
                'engagement_threshold': 25,
                'content_length_threshold': 120,
                'priority_keywords': ['æŠ•èµ„', 'ç†è´¢', 'è‚¡ç¥¨', 'investment']
            },
            'general': {
                'detail_threshold': 0.3,
                'engagement_threshold': 20,
                'content_length_threshold': 150,
                'priority_keywords': []
            }
        }
        
        # æ ¹æ®ç²‰ä¸æ•°è°ƒæ•´ç­–ç•¥
        strategy = strategies.get(account_type, strategies['general']).copy()
        if follower_count > 100000:  # å¤§Vè´¦å·
            strategy['detail_threshold'] *= 1.2
            strategy['engagement_threshold'] *= 0.8
        
        return strategy
    
    def should_scrape_details(self, tweet: Dict[str, Any], account_type: str = 'general') -> bool:
        """
        æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦æŠ“å–æ¨æ–‡è¯¦æƒ…
        
        Args:
            tweet: åŸºç¡€æ¨æ–‡æ•°æ®
            account_type: è´¦å·ç±»å‹
            
        Returns:
            æ˜¯å¦éœ€è¦æ·±åº¦æŠ“å–
        """
        content = tweet.get('content', '')
        strategy = self.get_scraping_strategy(account_type)
        
        # å¿…é¡»æ·±åº¦æŠ“å–çš„æ¡ä»¶
        must_scrape = [
            self.is_content_truncated(content),  # å†…å®¹è¢«æˆªæ–­
            self.is_thread_content(content),     # çº¿ç¨‹å†…å®¹
            self.has_rich_media(tweet)           # åŒ…å«å¤šåª’ä½“
        ]
        
        if any(must_scrape):
            return True
        
        # é€‰æ‹©æ€§æ·±åº¦æŠ“å–çš„æ¡ä»¶
        optional_scrape = [
            self.is_high_value_content(tweet),   # é«˜ä»·å€¼å†…å®¹
            len(content) > strategy['content_length_threshold'],  # é•¿å†…å®¹
            tweet.get('comments', 0) > strategy['engagement_threshold'],  # é«˜äº’åŠ¨
            any(keyword in content.lower() for keyword in strategy['priority_keywords'])  # ä¼˜å…ˆå…³é”®è¯
        ]
        
        # æ ¹æ®ç­–ç•¥é˜ˆå€¼å†³å®š
        score = sum(optional_scrape) / len(optional_scrape) if optional_scrape else 0
        return score >= strategy['detail_threshold']
    
    async def enhanced_tweet_scraping(self, max_tweets: int = 10, enable_details: bool = True) -> List[Dict[str, Any]]:
        """
        å¢å¼ºçš„æ¨æ–‡æŠ“å–ï¼ŒåŒ…å«è¯¦æƒ…é¡µå†…å®¹
        
        Args:
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_details: æ˜¯å¦å¯ç”¨è¯¦æƒ…é¡µæŠ“å–
            
        Returns:
            å¢å¼ºçš„æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        try:
            # å…ˆæŠ“å–æ—¶é—´çº¿ä¸Šçš„åŸºæœ¬æ¨æ–‡
            basic_tweets = await self.scrape_tweets(max_tweets)
            
            if not enable_details:
                return basic_tweets
            
            enhanced_tweets = []
            details_scraped = 0
            max_details = min(5, max_tweets // 2)  # é™åˆ¶è¯¦æƒ…é¡µæŠ“å–æ•°é‡
            
            for i, tweet in enumerate(basic_tweets):
                enhanced_tweet = tweet.copy()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·±åº¦æŠ“å–
                if (details_scraped < max_details and 
                    tweet.get('link') and 
                    self.should_scrape_details(tweet, 'general')):
                    
                    self.logger.info(f"å¯¹ç¬¬ {i+1} æ¡æ¨æ–‡è¿›è¡Œè¯¦æƒ…æŠ“å–")
                    
                    try:
                        # æŠ“å–è¯¦æƒ…é¡µå†…å®¹
                        details = await self.scrape_tweet_details(tweet['link'])
                        enhanced_tweet.update(details)
                        details_scraped += 1
                        
                        # æ¨¡æ‹Ÿäººå·¥æµè§ˆé—´éš”
                        if self.behavior_simulator:
                            await self.behavior_simulator.random_pause(2, 5)
                        else:
                            await asyncio.sleep(3)
                            
                    except Exception as e:
                        self.logger.warning(f"è¯¦æƒ…æŠ“å–å¤±è´¥: {e}")
                        enhanced_tweet['detail_error'] = str(e)
                
                enhanced_tweets.append(enhanced_tweet)
            
            self.logger.info(f"å¢å¼ºæŠ“å–å®Œæˆï¼Œå…±å¤„ç† {len(enhanced_tweets)} æ¡æ¨æ–‡ï¼Œå…¶ä¸­ {details_scraped} æ¡è¿›è¡Œäº†è¯¦æƒ…æŠ“å–")
            return enhanced_tweets
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºæ¨æ–‡æŠ“å–å¤±è´¥: {e}")
            return basic_tweets if 'basic_tweets' in locals() else []
    
    def parse_tweets(self, tweet_elements: List[Any]) -> List[Dict[str, Any]]:
        """
        è§£ææ¨æ–‡å…ƒç´ åˆ—è¡¨
        
        Args:
            tweet_elements: æ¨æ–‡DOMå…ƒç´ åˆ—è¡¨
            
        Returns:
            è§£æåçš„æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        parsed_tweets = []
        
        for element in tweet_elements:
            try:
                tweet_data = self.extract_tweet_data(element)
                if tweet_data:
                    parsed_tweets.append(tweet_data)
            except Exception as e:
                self.logger.warning(f"è§£ææ¨æ–‡å¤±è´¥: {e}")
                continue
        
        return parsed_tweets
    
    def parse_user_profile(self, profile_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æç”¨æˆ·ä¸ªäººèµ„æ–™æ•°æ®
        
        Args:
            profile_data: åŸå§‹ä¸ªäººèµ„æ–™æ•°æ®
            
        Returns:
            è§£æåçš„ç”¨æˆ·èµ„æ–™
        """
        try:
            parsed_profile = {
                'username': profile_data.get('username', ''),
                'display_name': profile_data.get('display_name', ''),
                'bio': profile_data.get('bio', ''),
                'followers_count': self._parse_count(profile_data.get('followers', '0')),
                'following_count': self._parse_count(profile_data.get('following', '0')),
                'tweets_count': self._parse_count(profile_data.get('tweets', '0')),
                'verified': profile_data.get('verified', False),
                'profile_image': profile_data.get('profile_image', ''),
                'banner_image': profile_data.get('banner_image', ''),
                'location': profile_data.get('location', ''),
                'website': profile_data.get('website', ''),
                'joined_date': profile_data.get('joined_date', ''),
                'parsed_at': datetime.now().isoformat()
            }
            
            return parsed_profile
            
        except Exception as e:
            self.logger.error(f"è§£æç”¨æˆ·èµ„æ–™å¤±è´¥: {e}")
            return {}
    
    def extract_tweet_data(self, tweet_element: Any) -> Dict[str, Any]:
        """
        ä»æ¨æ–‡å…ƒç´ ä¸­æå–æ•°æ®
        
        Args:
            tweet_element: æ¨æ–‡DOMå…ƒç´ 
            
        Returns:
            æå–çš„æ¨æ–‡æ•°æ®
        """
        try:
            # è¿™é‡Œåº”è¯¥æ ¹æ®å®é™…çš„DOMç»“æ„æ¥æå–æ•°æ®
            # ç”±äºè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ–¹æ³•ï¼Œè¿”å›åŸºæœ¬ç»“æ„
            tweet_data = {
                'id': '',
                'username': '',
                'display_name': '',
                'content': '',
                'timestamp': '',
                'likes': 0,
                'retweets': 0,
                'comments': 0,
                'link': '',
                'images': [],
                'videos': [],
                'extracted_at': datetime.now().isoformat()
            }
            
            # å¦‚æœtweet_elementæ˜¯å­—å…¸ç±»å‹ï¼ˆå·²è§£æçš„æ•°æ®ï¼‰
            if isinstance(tweet_element, dict):
                tweet_data.update(tweet_element)
            
            return tweet_data
            
        except Exception as e:
            self.logger.error(f"æå–æ¨æ–‡æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def _parse_count(self, count_str: str) -> int:
        """
        è§£æè®¡æ•°å­—ç¬¦ä¸²ï¼ˆå¦‚1.2K, 5.6Mç­‰ï¼‰
        
        Args:
            count_str: è®¡æ•°å­—ç¬¦ä¸²
            
        Returns:
            è§£æåçš„æ•°å­—
        """
        try:
            if not count_str or count_str == '-':
                return 0
            
            count_str = count_str.strip().replace(',', '')
            
            if count_str.endswith('K'):
                return int(float(count_str[:-1]) * 1000)
            elif count_str.endswith('M'):
                return int(float(count_str[:-1]) * 1000000)
            elif count_str.endswith('B'):
                return int(float(count_str[:-1]) * 1000000000)
            else:
                return int(count_str)
                
        except (ValueError, AttributeError):
            return 0

    # ==================== ä¼˜åŒ–åŠŸèƒ½æ–¹æ³• ====================
    
    def clean_tweet_content(self, content: str) -> str:
        """ä¼˜åŒ–çš„æ¨æ–‡å†…å®¹æ¸…ç† - ä¿®å¤è¿‡åº¦æ¸…ç†é—®é¢˜"""
        if not content:
            return ""
        
        # ç¼“å­˜æ£€æŸ¥
        if content in self.content_cache:
            return self.content_cache[content]
        
        original_content = content
        
        # åŸºç¡€æ¸…ç†ï¼šå»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content.strip())
        
        # åªè¿›è¡Œå¿…è¦çš„æ¸…ç†ï¼Œé¿å…è¿‡åº¦åˆ é™¤å†…å®¹
        # 1. å»é™¤æ˜æ˜¾çš„é‡å¤ç”¨æˆ·åæ¨¡å¼ (å®Œå…¨ç›¸åŒçš„è¿ç»­é‡å¤)
        content = re.sub(r'\b(\w+)\s+\1\b', r'\1', content)
        
        # 2. å»é™¤æœ«å°¾çš„ç»Ÿè®¡æ•°æ®æ¨¡å¼ï¼ˆä½†ä¿ç•™ä¸»è¦å†…å®¹ï¼‰
        content = re.sub(r'\s*Â·\s*[\d,KMB.\s]+$', '', content)
        
        # 3. å»é™¤å¼€å¤´å¯èƒ½çš„ç”¨æˆ·åé‡å¤ï¼ˆä½†åªåˆ é™¤æ˜æ˜¾é‡å¤çš„éƒ¨åˆ†ï¼‰
        content = re.sub(r'^(@?\w+)\s+\1\s+', r'\1 ', content)
        
        # 4. æ¸…ç†æœ«å°¾çš„å¤šä½™ç¬¦å·
        content = re.sub(r'\s*[Â·â€¦]+\s*$', '', content)
        
        # ä¿ç•™åŸå§‹å†…å®¹çš„ä¸»ä½“éƒ¨åˆ†ï¼Œåªåšæœ€å°å¿…è¦çš„æ¸…ç†
        cleaned_content = content.strip()
        
        # å¦‚æœæ¸…ç†åå†…å®¹å¤ªçŸ­ä¸”åŸå†…å®¹è¾ƒé•¿ï¼Œåˆ™è¿”å›åŸå†…å®¹
        if len(cleaned_content) < len(original_content) * 0.3 and len(original_content) > 20:
            cleaned_content = original_content.strip()
        
        # ç¼“å­˜ç»“æœ
        self.content_cache[original_content] = cleaned_content
        
        return cleaned_content
    
    def extract_tweet_id(self, tweet_link: str) -> str:
        """ä»æ¨æ–‡é“¾æ¥ä¸­æå–ID"""
        try:
            if '/status/' in tweet_link:
                return tweet_link.split('/status/')[-1].split('?')[0]
            return ''
        except Exception:
            return ''
    
    def is_duplicate_tweet(self, tweet_link: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ¨æ–‡"""
        if not tweet_link:
            return False
            
        # ç»Ÿä¸€ä½¿ç”¨seen_tweet_idsè¿›è¡Œå»é‡
        if tweet_link in self.seen_tweet_ids:
            return True
            
        # æå–æ¨æ–‡IDè¿›è¡Œæ›´ç²¾ç¡®çš„å»é‡
        tweet_id = self.extract_tweet_id(tweet_link)
        if tweet_id:
            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç›¸åŒIDçš„æ¨æ–‡
            for seen_id in self.seen_tweet_ids:
                if tweet_id in seen_id:
                    return True
                    
        return False
    
    def parse_engagement_number(self, num_str: str) -> int:
        """è§£æäº’åŠ¨æ•°å­— (å¦‚: 1.2K -> 1200)"""
        try:
            if not num_str:
                return 0
            
            num_str = num_str.replace(',', '').replace(' ', '')
            
            if num_str.endswith('K'):
                return int(float(num_str[:-1]) * 1000)
            elif num_str.endswith('M'):
                return int(float(num_str[:-1]) * 1000000)
            elif num_str.endswith('B'):
                return int(float(num_str[:-1]) * 1000000000)
            else:
                return int(num_str)
        except (ValueError, IndexError):
            return 0
    
    async def scroll_and_load_tweets_optimized(self, target_tweets: int = 15, max_attempts: int = 20) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„æ»šåŠ¨ç­–ç•¥"""
        self.logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥ï¼Œç›®æ ‡: {target_tweets} æ¡æ¨æ–‡")
        
        scroll_attempt = 0
        stagnant_rounds = 0
        last_unique_count = 0
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°
        base_scroll_distance = 800
        base_wait_time = 1.5
        
        while scroll_attempt < max_attempts:
            # è·å–å½“å‰æ¨æ–‡æ•°é‡
            try:
                await self.page.wait_for_selector('[data-testid="tweet"]', timeout=5000)
                current_elements = await self.page.query_selector_all('[data-testid="tweet"]')
                current_unique_tweets = len(self.seen_tweet_ids)
            except Exception:
                current_elements = []
                current_unique_tweets = 0
            
            self.logger.debug(f"ğŸ“Š æ»šåŠ¨å°è¯• {scroll_attempt + 1}/{max_attempts}ï¼Œå½“å‰å”¯ä¸€æ¨æ–‡: {current_unique_tweets}/{target_tweets}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if current_unique_tweets >= target_tweets:
                self.logger.info(f"ğŸ¯ è¾¾åˆ°ç›®æ ‡æ¨æ–‡æ•°é‡: {current_unique_tweets}")
                break
            
            # æ£€æŸ¥åœæ»æƒ…å†µ
            if current_unique_tweets == last_unique_count:
                stagnant_rounds += 1
            else:
                stagnant_rounds = 0
            
            last_unique_count = current_unique_tweets
            
            # æ£€æŸ¥å¹¶å¤„ç†å¯èƒ½å‡ºç°çš„ç¿»è¯‘å¼¹çª—
            popup_dismissed = await self.dismiss_translate_popup()
            if popup_dismissed:
                self.logger.info("å·²å¤„ç†ç¿»è¯‘å¼¹çª—ï¼Œç»§ç»­æ»šåŠ¨")
                # å¼¹çª—å¤„ç†åé‡ç½®åœæ»è®¡æ•°ï¼Œé¿å…å› å¼¹çª—å¯¼è‡´çš„è¯¯åˆ¤
                stagnant_rounds = max(0, stagnant_rounds - 1)
            
            # æ ¹æ®åœæ»æƒ…å†µè°ƒæ•´ç­–ç•¥
            if stagnant_rounds >= 3:
                # æ¿€è¿›æ¨¡å¼ï¼šå¢åŠ æ»šåŠ¨è·ç¦»ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
                scroll_distance = base_scroll_distance * 2
                wait_time = base_wait_time * 0.7
                self.logger.debug(f"ğŸ”¥ æ¿€è¿›æ¨¡å¼ï¼šæ»šåŠ¨è·ç¦» {scroll_distance}ï¼Œç­‰å¾…æ—¶é—´ {wait_time:.1f}s")
            elif stagnant_rounds >= 6:
                # è¶…æ¿€è¿›æ¨¡å¼ï¼šå¤§å¹…æ»šåŠ¨
                scroll_distance = base_scroll_distance * 3
                wait_time = base_wait_time * 0.5
                self.logger.debug(f"âš¡ è¶…æ¿€è¿›æ¨¡å¼ï¼šæ»šåŠ¨è·ç¦» {scroll_distance}ï¼Œç­‰å¾…æ—¶é—´ {wait_time:.1f}s")
            else:
                # æ­£å¸¸æ¨¡å¼
                scroll_distance = base_scroll_distance
                wait_time = base_wait_time
            
            # æ‰§è¡Œæ»šåŠ¨
            try:
                # ç¡®ä¿é¡µé¢ç„¦ç‚¹
                await self.ensure_page_focus()
                
                # å¹³æ»‘æ»šåŠ¨
                current_scroll = await self.page.evaluate('window.pageYOffset')
                await self.page.evaluate(f'''
                    window.scrollTo({{
                        top: {current_scroll + scroll_distance},
                        behavior: 'smooth'
                    }});
                ''')
                
                # ç­‰å¾…æ»šåŠ¨å®Œæˆå’Œå†…å®¹åŠ è½½
                await asyncio.sleep(wait_time)
                
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦æœ‰ç¿»è¯‘å¼¹çª—å‡ºç°
                await self.dismiss_translate_popup()
                
                # æ£€æŸ¥æ–°æ¨æ–‡å¹¶æ›´æ–°seen_tweet_ids
                await self.update_seen_tweets()
                
            except Exception as e:
                self.logger.warning(f"æ»šåŠ¨å¤±è´¥: {e}")
                await asyncio.sleep(2)
                # å‡ºé”™æ—¶ä¹Ÿå°è¯•å¤„ç†å¯èƒ½çš„å¼¹çª—
                await self.dismiss_translate_popup()
            
            # å¦‚æœè¿ç»­å¤šè½®æ— æ–°å†…å®¹ï¼Œè€ƒè™‘åˆ·æ–°
            if stagnant_rounds >= 8:
                self.logger.info("ğŸ”„ é•¿æ—¶é—´æ— æ–°å†…å®¹ï¼Œå°è¯•åˆ·æ–°é¡µé¢")
                try:
                    await self.page.reload(wait_until='domcontentloaded')
                    await asyncio.sleep(3)
                    stagnant_rounds = 0
                    # é‡æ–°æ”¶é›†å·²è§è¿‡çš„æ¨æ–‡ID
                    await self.rebuild_seen_tweets()
                except Exception as e:
                    self.logger.warning(f"é¡µé¢åˆ·æ–°å¤±è´¥: {e}")
            
            scroll_attempt += 1
        
        final_unique_tweets = len(self.seen_tweet_ids)
        self.logger.info(f"ğŸ“Š æ»šåŠ¨ç­–ç•¥å®Œæˆ: {final_unique_tweets} æ¡å”¯ä¸€æ¨æ–‡ï¼Œ{scroll_attempt} æ¬¡æ»šåŠ¨")
        
        return {
            'final_tweet_count': final_unique_tweets,
            'scroll_attempts': scroll_attempt,
            'target_reached': final_unique_tweets >= target_tweets,
            'efficiency': final_unique_tweets / max(scroll_attempt, 1)
        }
    
    async def update_seen_tweets(self):
        """æ›´æ–°å·²è§æ¨æ–‡IDé›†åˆ"""
        try:
            current_elements = await self.page.query_selector_all('[data-testid="tweet"]')
            
            for element in current_elements:
                try:
                    link_element = await element.query_selector('a[href*="/status/"]')
                    if link_element:
                        href = await link_element.get_attribute('href')
                        if href:
                            tweet_id = self.extract_tweet_id(href)
                            if tweet_id:
                                self.seen_tweet_ids.add(tweet_id)
                except Exception:
                    continue
        except Exception as e:
            self.logger.debug(f"æ›´æ–°å·²è§æ¨æ–‡IDå¤±è´¥: {e}")
    
    async def rebuild_seen_tweets(self):
        """é‡æ–°æ„å»ºå·²è§æ¨æ–‡IDé›†åˆ"""
        try:
            self.seen_tweet_ids.clear()
            await self.update_seen_tweets()
            self.logger.debug(f"é‡å»ºå·²è§æ¨æ–‡IDé›†åˆ: {len(self.seen_tweet_ids)} æ¡")
        except Exception as e:
            self.logger.warning(f"é‡å»ºå·²è§æ¨æ–‡IDå¤±è´¥: {e}")
    
    
    async def parse_tweet_element_enhanced(self, element) -> Optional[Dict[str, Any]]:
        """å¢å¼ºçš„æ¨æ–‡å…ƒç´ è§£æ - è§£å†³æ•°é‡ä¸è¶³é—®é¢˜"""
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ¨æ–‡å…ƒç´ 
            if not await self.is_valid_tweet_element(element):
                return None
            
            # æå–åŸºç¡€ä¿¡æ¯
            tweet_data = {
                'username': await self.extract_username_enhanced(element),
                'content': await self.extract_content_enhanced(element),
                'link': await self.extract_tweet_link_enhanced(element),
                'publish_time': await self.extract_publish_time_enhanced(element),
                'likes': 0,
                'comments': 0,
                'retweets': 0,
                'media': {'images': [], 'videos': []}
            }
            
            # æå–äº’åŠ¨æ•°æ®
            engagement = await self.extract_engagement_enhanced(element)
            tweet_data.update(engagement)
            
            # æå–åª’ä½“å†…å®¹
            media = await self.extract_media_content_enhanced(element)
            tweet_data['media'] = media
            
            # æ”¹è¿›çš„å»é‡æ£€æŸ¥
            if await self.is_duplicate_tweet_enhanced(tweet_data):
                self.logger.debug(f"æ¨æ–‡é‡å¤ï¼Œè·³è¿‡: {tweet_data.get('link', 'no_link')}")
                return None
            
            # æ”¾å®½éªŒè¯æ¡ä»¶ - åªè¦æœ‰åŸºæœ¬ä¿¡æ¯å°±ä¿ç•™
            if self.is_valid_tweet_data_enhanced(tweet_data):
                return tweet_data
            
            return None
            
        except Exception as e:
            self.logger.debug(f"è§£ææ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return None
    
    async def is_valid_tweet_element(self, element) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ¨æ–‡å…ƒç´ """
        try:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨æ–‡çš„åŸºæœ¬ç»“æ„
            has_user_info = await element.query_selector('[data-testid="User-Name"]') is not None
            has_content_area = await element.query_selector('[data-testid="tweetText"]') is not None
            has_time = await element.query_selector('time') is not None
            has_actions = await element.query_selector('[role="group"]') is not None
            
            # æ’é™¤å¹¿å‘Šå’Œæ¨èå†…å®¹
            element_text = await element.text_content()
            is_ad = any(keyword in element_text.lower() for keyword in ['promoted', 'æ¨å¹¿', 'ad', 'å¹¿å‘Š'])
            
            # è‡³å°‘è¦æœ‰ç”¨æˆ·ä¿¡æ¯æˆ–å†…å®¹åŒºåŸŸï¼Œä¸”ä¸æ˜¯å¹¿å‘Š
            return (has_user_info or has_content_area or has_time or has_actions) and not is_ad
            
        except Exception:
            return True  # å‡ºé”™æ—¶ä¿å®ˆå¤„ç†ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆå…ƒç´ 
    
    async def extract_username_enhanced(self, element) -> str:
        """å¢å¼ºçš„ç”¨æˆ·åæå–"""
        try:
            # æ‰©å±•é€‰æ‹©å™¨åˆ—è¡¨
            selectors = [
                '[data-testid="User-Name"] [dir="ltr"]',
                '[data-testid="User-Name"] span',
                '[data-testid="User-Names"] [dir="ltr"]',
                '[data-testid="User-Names"] span',
                'a[href^="/"][role="link"] span',
                '[dir="ltr"] span',
                'div[dir="ltr"] span',
                'span[dir="ltr"]'
            ]
            
            for selector in selectors:
                try:
                    elements = await element.query_selector_all(selector)
                    for elem in elements:
                        text = await elem.text_content()
                        if text and text.strip():
                            username = self.clean_username(text.strip())
                            if username and username != 'unknown':
                                return username
                except Exception:
                    continue
            
            # ä»é“¾æ¥ä¸­æå–
            try:
                link_elem = await element.query_selector('a[href^="/"][role="link"]')
                if link_elem:
                    href = await link_elem.get_attribute('href')
                    if href:
                        match = re.match(r'^/([^/]+)', href)
                        if match:
                            return match.group(1)
            except Exception:
                pass
            
            return 'unknown'
            
        except Exception:
            return 'unknown'
    
    def clean_username(self, text: str) -> str:
        """æ¸…ç†ç”¨æˆ·å"""
        if not text:
            return 'unknown'
        
        # ç§»é™¤@ç¬¦å·
        text = re.sub(r'^@+', '', text)
        
        # åªä¿ç•™ç¬¬ä¸€ä¸ªå•è¯
        text = text.split()[0] if text.split() else text
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯æ•°å­—å’Œä¸‹åˆ’çº¿
        text = re.sub(r'[^a-zA-Z0-9_]', '', text)
        
        # æ’é™¤æ˜æ˜¾çš„æ•°å­—ï¼ˆå¦‚ç‚¹èµæ•°ç­‰ï¼‰
        if re.match(r'^\d+[KMB]?$', text):
            return 'unknown'
        
        return text if text else 'unknown'
    
    async def extract_content_enhanced(self, element) -> str:
        """å¢å¼ºçš„å†…å®¹æå–"""
        try:
            content_parts = []
            
            # æ‰©å±•å†…å®¹é€‰æ‹©å™¨
            selectors = [
                '[data-testid="tweetText"]',
                '[data-testid="tweetText"] span',
                '[lang] span',
                'div[dir="auto"] span',
                'div[dir="ltr"] span',
                'div[dir="rtl"] span',
                'span[dir="auto"]',
                'span[dir="ltr"]',
                'span[dir="rtl"]',
                'div[lang] span'
            ]
            
            for selector in selectors:
                try:
                    elements = await element.query_selector_all(selector)
                    for elem in elements:
                        text = await elem.text_content()
                        if text and text.strip():
                            clean_text = text.strip()
                            if clean_text not in content_parts and len(clean_text) > 2:
                                content_parts.append(clean_text)
                except Exception:
                    continue
            
            if content_parts:
                content = ' '.join(content_parts)
                return self.clean_tweet_content_enhanced(content)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•ä»æ•´ä¸ªå…ƒç´ æå–
            try:
                full_text = await element.text_content()
                if full_text:
                    lines = [line.strip() for line in full_text.split('\n') if line.strip()]
                    # è¿‡æ»¤æ‰ç”¨æˆ·åã€æ—¶é—´ç­‰ä¿¡æ¯ï¼Œä¿ç•™ä¸»è¦å†…å®¹
                    content_lines = []
                    for line in lines:
                        if (len(line) > 10 and 
                            not line.startswith('@') and 
                            not re.match(r'^\d+[hms]$', line) and
                            not re.match(r'^\d+[KMB]?$', line)):
                            content_lines.append(line)
                    
                    if content_lines:
                        return ' '.join(content_lines[:3])  # å–å‰3è¡Œ
            except Exception:
                pass
            
            return 'No content available'
            
        except Exception:
            return 'No content available'
    
    def clean_tweet_content_enhanced(self, content: str) -> str:
        """å¢å¼ºçš„å†…å®¹æ¸…ç†"""
        if not content:
            return ""
        
        # åŸºç¡€æ¸…ç†
        content = re.sub(r'\s+', ' ', content.strip())
        
        # ç§»é™¤æ˜æ˜¾çš„é‡å¤æ¨¡å¼
        content = re.sub(r'(\w+)\s+', r'', content)
        
        # ç§»é™¤æœ«å°¾çš„ç»Ÿè®¡ä¿¡æ¯
        content = re.sub(r'\s*[Â·â€¦]+\s*\d+[KMB]?\s*$', '', content)
        
        # ç§»é™¤å¼€å¤´çš„é‡å¤ç”¨æˆ·å
        content = re.sub(r'^(@?\w+)\s+\s+', r' ', content)
        
        return content.strip()
    
    async def extract_tweet_link_enhanced(self, element) -> str:
        """å¢å¼ºçš„é“¾æ¥æå–"""
        try:
            # å¤šç§é“¾æ¥é€‰æ‹©å™¨
            selectors = [
                'a[href*="/status/"]',
                'a[href*="/status/"][role="link"]',
                'time[datetime] a',
                'time a'
            ]
            
            for selector in selectors:
                try:
                    link_elem = await element.query_selector(selector)
                    if link_elem:
                        href = await link_elem.get_attribute('href')
                        if href and '/status/' in href:
                            if href.startswith('/'):
                                return f'https://x.com{href}'
                            return href
                except Exception:
                    continue
            
            return ''
            
        except Exception:
            return ''
    
    async def is_duplicate_tweet_enhanced(self, tweet_data: Dict[str, Any]) -> bool:
        """å¢å¼ºçš„å»é‡æ£€æŸ¥"""
        try:
            # ä¼˜å…ˆä½¿ç”¨é“¾æ¥å»é‡
            link = tweet_data.get('link', '')
            if link:
                tweet_id = self.extract_tweet_id(link)
                if tweet_id:
                    if not hasattr(self, 'seen_tweet_ids_enhanced'):
                        self.seen_tweet_ids_enhanced = set()
                    
                    if tweet_id in self.seen_tweet_ids_enhanced:
                        return True
                    self.seen_tweet_ids_enhanced.add(tweet_id)
                    return False
            
            # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œä½¿ç”¨å†…å®¹å»é‡ï¼ˆæ›´å®½æ¾çš„ç­–ç•¥ï¼‰
            content = tweet_data.get('content', '')
            if content and len(content) > 20:
                # ä½¿ç”¨å†…å®¹çš„å“ˆå¸Œå€¼è€Œä¸æ˜¯å‰50å­—ç¬¦
                import hashlib
                content_hash = hashlib.md5(content.encode()).hexdigest()
                
                if not hasattr(self, 'seen_content_hashes'):
                    self.seen_content_hashes = set()
                
                if content_hash in self.seen_content_hashes:
                    return True
                self.seen_content_hashes.add(content_hash)
            
            return False
            
        except Exception:
            return False
    
    def is_valid_tweet_data_enhanced(self, tweet_data: Dict[str, Any]) -> bool:
        """å¢å¼ºçš„æ¨æ–‡æ•°æ®éªŒè¯ - æ›´å®½æ¾çš„æ¡ä»¶"""
        try:
            username = tweet_data.get('username', '')
            content = tweet_data.get('content', '')
            link = tweet_data.get('link', '')
            
            # åªè¦æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶å°±è®¤ä¸ºæœ‰æ•ˆï¼š
            # 1. æœ‰ç”¨æˆ·åä¸”ä¸æ˜¯unknown
            # 2. æœ‰å†…å®¹ä¸”é•¿åº¦å¤§äº5
            # 3. æœ‰æœ‰æ•ˆé“¾æ¥
            # 4. æœ‰åª’ä½“å†…å®¹
            # 5. æœ‰ä»»ä½•äº’åŠ¨æ•°æ®
            
            has_username = username and username != 'unknown'
            has_content = content and content != 'No content available' and len(content.strip()) > 5
            has_link = link and '/status/' in link
            has_media = (tweet_data.get('media', {}).get('images') or 
                        tweet_data.get('media', {}).get('videos'))
            has_engagement = (tweet_data.get('likes', 0) > 0 or 
                            tweet_data.get('comments', 0) > 0 or 
                            tweet_data.get('retweets', 0) > 0)
            
            return has_username or has_content or has_link or has_media or has_engagement
            
        except Exception:
            return False


    async def parse_tweet_element_optimized(self, element) -> Optional[Dict[str, Any]]:
        """ä¼˜åŒ–çš„æ¨æ–‡å…ƒç´ è§£æ"""
        try:
            self.logger.info("ğŸ”§ å¼€å§‹è§£ææ¨æ–‡å…ƒç´ ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
            # æå–ç”¨æˆ·å
            username = await self.extract_clean_username(element)
            
            # æå–å†…å®¹
            content = await self.extract_clean_content(element)
            
            # æå–é“¾æ¥
            link = await self.extract_tweet_link(element)
            
            # æ£€æŸ¥é‡å¤
            if self.is_duplicate_tweet(link):
                self.logger.info(f"ğŸ”§ æ¨æ–‡é‡å¤ï¼Œè·³è¿‡ - é“¾æ¥: {link}")
                return None
            
            self.logger.info(f"ğŸ”§ æ¨æ–‡ä¸é‡å¤ï¼Œç»§ç»­è§£æ - é“¾æ¥: {link}")
            
            # æå–æ—¶é—´
            publish_time = await self.extract_publish_time(element)
            
            # æå–äº’åŠ¨æ•°æ®
            engagement = await self.extract_engagement_data(element)
            
            # æå–åª’ä½“å†…å®¹
            media = await self.extract_media_content(element)
            
            # ç¡®å®šå¸–å­ç±»å‹
            post_type = 'çº¯æ–‡æœ¬'
            if media['images']:
                post_type = 'å›¾æ–‡'
            elif media['videos']:
                post_type = 'è§†é¢‘'
            
            # æ„å»ºæ¨æ–‡æ•°æ®
            tweet_data = {
                'username': username,
                'content': content,
                'publish_time': publish_time,
                'link': link,
                'likes': engagement['likes'],
                'comments': engagement['comments'],
                'retweets': engagement['retweets'],
                'media': media,
                'post_type': post_type
            }
            
            # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§ - è¿›ä¸€æ­¥æ”¾å®½éªŒè¯æ¡ä»¶
            # åªè¦æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶å°±è®¤ä¸ºæ˜¯æœ‰æ•ˆæ¨æ–‡ï¼š
            # 1. æœ‰æœ‰æ•ˆç”¨æˆ·å
            # 2. æœ‰å†…å®¹ï¼ˆé•¿åº¦å¤§äº3ä¸ªå­—ç¬¦ï¼‰
            # 3. æœ‰é“¾æ¥
            # 4. æœ‰åª’ä½“å†…å®¹
            # 5. æœ‰äº’åŠ¨æ•°æ®
            has_username = username and username != 'unknown'
            has_content = content and content != 'No content available' and len(content.strip()) > 3
            has_link = link and link.strip()
            has_media = media['images'] or media['videos']
            has_engagement = engagement['likes'] > 0 or engagement['comments'] > 0 or engagement['retweets'] > 0
            
            # è¯¦ç»†è®°å½•éªŒè¯ä¿¡æ¯
            self.logger.info(f"ğŸ”§ æ¨æ–‡éªŒè¯è¯¦æƒ…:")
            self.logger.info(f"   - ç”¨æˆ·å: '{username}' (æœ‰æ•ˆ: {has_username})")
            self.logger.info(f"   - å†…å®¹: '{content[:50] if content else 'None'}...' (é•¿åº¦: {len(content) if content else 0}, æœ‰æ•ˆ: {has_content})")
            self.logger.info(f"   - é“¾æ¥: '{link}' (æœ‰æ•ˆ: {has_link})")
            self.logger.info(f"   - åª’ä½“: {media} (æœ‰æ•ˆ: {has_media})")
            self.logger.info(f"   - äº’åŠ¨: {engagement} (æœ‰æ•ˆ: {has_engagement})")
            
            # åªè¦æœ‰ä»»ä½•ä¸€é¡¹æœ‰æ•ˆä¿¡æ¯å°±ä¿ç•™æ¨æ–‡
            if has_username or has_content or has_link or has_media or has_engagement:
                self.logger.info(f"ğŸ”§ æ¨æ–‡éªŒè¯é€šè¿‡ - è‡³å°‘æœ‰ä¸€é¡¹æœ‰æ•ˆä¿¡æ¯")
                return tweet_data
            
            self.logger.info(f"ğŸ”§ æ¨æ–‡æ•°æ®æ— æ•ˆï¼Œè·³è¿‡ - æ‰€æœ‰éªŒè¯é¡¹éƒ½å¤±è´¥")
            return None
            
        except Exception as e:
            self.logger.debug(f"è§£ææ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return None
    
    async def extract_clean_username(self, element) -> str:
        """æå–å¹²å‡€çš„ç”¨æˆ·å"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            username_selectors = [
                '[data-testid="User-Name"] [dir="ltr"]',
                '[data-testid="User-Name"] span',
                'a[href^="/"][role="link"] span',
                '[dir="ltr"] span'
            ]
            
            for selector in username_selectors:
                username_element = await element.query_selector(selector)
                if username_element:
                    username = await username_element.text_content()
                    username = username.strip()
                    self.logger.info(f"æ‰¾åˆ°ç”¨æˆ·ååŸå§‹æ–‡æœ¬: '{username}' (é€‰æ‹©å™¨: {selector})")
                    # æ¸…ç†ç”¨æˆ·å
                    username = re.sub(r'^@', '', username)
                    username = re.sub(r'\s.*', '', username)  # åªä¿ç•™ç¬¬ä¸€ä¸ªè¯
                    if username and not re.match(r'^\d+[KMB]?$', username):
                        self.logger.info(f"æå–åˆ°æœ‰æ•ˆç”¨æˆ·å: '{username}'")
                        return username
            
            # ä»é“¾æ¥ä¸­æå–ç”¨æˆ·å
            link_element = await element.query_selector('a[href^="/"][role="link"]')
            if link_element:
                href = await link_element.get_attribute('href')
                if href:
                    self.logger.info(f"æ‰¾åˆ°é“¾æ¥: {href}")
                    match = re.match(r'^/([^/]+)', href)
                    if match:
                        username = match.group(1)
                        self.logger.info(f"ä»é“¾æ¥æå–ç”¨æˆ·å: '{username}'")
                        return username
            
            self.logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆç”¨æˆ·åï¼Œè¿”å› 'unknown'")
            return 'unknown'
            
        except Exception as e:
            self.logger.debug(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
            return 'unknown'
    
    async def extract_clean_content(self, element) -> str:
        """æå–å¹²å‡€çš„æ¨æ–‡å†…å®¹"""
        try:
            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                '[data-testid="tweetText"]',
                '[lang] span',
                'div[dir="auto"] span'
            ]
            
            content_parts = []
            
            for selector in content_selectors:
                content_elements = await element.query_selector_all(selector)
                self.logger.info(f"é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(content_elements)} ä¸ªå†…å®¹å…ƒç´ ")
                for content_element in content_elements:
                    text = await content_element.text_content()
                    text = text.strip()
                    if text and text not in content_parts:
                        self.logger.info(f"æ‰¾åˆ°å†…å®¹ç‰‡æ®µ: '{text[:50]}...'")
                        content_parts.append(text)
            
            # åˆå¹¶å†…å®¹
            raw_content = ' '.join(content_parts)
            self.logger.info(f"åˆå¹¶åçš„åŸå§‹å†…å®¹: '{raw_content[:100]}...'")
            
            # æ¸…ç†å†…å®¹
            clean_content = self.clean_tweet_content(raw_content)
            self.logger.info(f"æ¸…ç†åçš„å†…å®¹: '{clean_content[:100]}...'")
            
            result = clean_content if clean_content else 'No content available'
            self.logger.info(f"æœ€ç»ˆè¿”å›å†…å®¹: '{result[:50]}...'")
            return result
            
        except Exception as e:
            self.logger.debug(f"æå–æ¨æ–‡å†…å®¹å¤±è´¥: {e}")
            return 'No content available'
    
    async def extract_tweet_link(self, element) -> str:
        """æå–æ¨æ–‡é“¾æ¥"""
        try:
            link_element = await element.query_selector('a[href*="/status/"]')
            if link_element:
                href = await link_element.get_attribute('href')
                if href:
                    if href.startswith('/'):
                        return f'https://x.com{href}'
                    else:
                        return href
            return ''
        except Exception:
            return ''
    
    async def extract_publish_time(self, element) -> str:
        """æå–å‘å¸ƒæ—¶é—´"""
        try:
            time_element = await element.query_selector('time')
            if time_element:
                datetime_attr = await time_element.get_attribute('datetime')
                if datetime_attr:
                    return datetime_attr
            return ''
        except Exception:
            return ''
    
    async def extract_engagement_data(self, element) -> Dict[str, int]:
        """æå–äº’åŠ¨æ•°æ®"""
        engagement = {'likes': 0, 'comments': 0, 'retweets': 0}
        
        try:
            # æŸ¥æ‰¾äº’åŠ¨æ•°æ®
            engagement_selectors = {
                'likes': ['[data-testid="like"]', '[aria-label*="like"]'],
                'comments': ['[data-testid="reply"]', '[aria-label*="repl"]'],
                'retweets': ['[data-testid="retweet"]', '[aria-label*="repost"]']
            }
            
            for metric, selectors in engagement_selectors.items():
                for selector in selectors:
                    metric_element = await element.query_selector(selector)
                    if metric_element:
                        # æŸ¥æ‰¾æ•°å­—
                        text = await metric_element.text_content()
                        numbers = re.findall(r'[\d,]+[KMB]?', text)
                        if numbers:
                            engagement[metric] = self.parse_engagement_number(numbers[0])
                            break
            
            return engagement
            
        except Exception as e:
            self.logger.debug(f"æå–äº’åŠ¨æ•°æ®å¤±è´¥: {e}")
            return engagement
    
    async def extract_media_content(self, element) -> Dict[str, List[Dict]]:
        """æå–åª’ä½“å†…å®¹"""
        media = {'images': [], 'videos': []}
        
        try:
            # æå–å›¾ç‰‡
            img_elements = await element.query_selector_all('img[src*="pbs.twimg.com"]')
            for img in img_elements:
                src = await img.get_attribute('src')
                alt = await img.get_attribute('alt') or 'Image'
                if src:
                    media['images'].append({
                        'type': 'image',
                        'url': src,
                        'description': alt,
                        'original_url': src
                    })
            
            # æå–è§†é¢‘
            video_elements = await element.query_selector_all('video, [data-testid="videoPlayer"]')
            for video in video_elements:
                poster = await video.get_attribute('poster')
                if poster:
                    media['videos'].append({
                        'type': 'video',
                        'poster': poster,
                        'description': 'Video content'
                    })
            
            return media
            
        except Exception as e:
            self.logger.debug(f"æå–åª’ä½“å†…å®¹å¤±è´¥: {e}")
            return media
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æ‘˜è¦"""
        return {
            'unique_tweets_processed': len(self.seen_tweet_ids),
            'content_cache_size': len(self.content_cache),
            'optimization_enabled': self.optimization_enabled,
            'optimizations_applied': [
                'intelligent_scroll_strategy',
                'content_deduplication',
                'enhanced_text_cleaning',
                'improved_element_extraction',
                'engagement_data_parsing',
                'media_content_detection'
            ]
        }
    
    def enable_optimizations(self):
        """å¯ç”¨ä¼˜åŒ–åŠŸèƒ½"""
        self.optimization_enabled = True
        # æ¸…ç©ºä¹‹å‰çš„ç¼“å­˜ï¼Œç¡®ä¿æ¯æ¬¡ä»»åŠ¡éƒ½æ˜¯å…¨æ–°å¼€å§‹
        self.seen_tweet_ids.clear()
        self.content_cache.clear()
        # æ¸…ç©ºè§£æé˜¶æ®µçš„å»é‡é›†åˆ
        if hasattr(self, 'parsed_tweet_ids'):
            self.parsed_tweet_ids.clear()
        self.logger.info("âœ… ä¼˜åŒ–åŠŸèƒ½å·²å¯ç”¨")
        self.logger.info("ğŸ”„ å·²æ¸…ç©ºæ¨æ–‡IDç¼“å­˜ã€å†…å®¹ç¼“å­˜å’Œè§£æå»é‡é›†åˆ")
    
    def disable_optimizations(self):
        """ç¦ç”¨ä¼˜åŒ–åŠŸèƒ½"""
        self.optimization_enabled = False
        self.logger.info("âŒ ä¼˜åŒ–åŠŸèƒ½å·²ç¦ç”¨")
    
    def clear_optimization_cache(self):
        """æ¸…ç†ä¼˜åŒ–ç¼“å­˜"""
        self.seen_tweet_ids.clear()
        self.content_cache.clear()
        self.logger.info("ğŸ§¹ ä¼˜åŒ–ç¼“å­˜å·²æ¸…ç†")

    async def close(self):
        """
        å…³é—­æµè§ˆå™¨è¿æ¥
        """
        try:
            if self.browser:
                await self.browser.close()
                self.logger.info("æµè§ˆå™¨è¿æ¥å·²å…³é—­")
        except Exception as e:
            self.logger.error(f"å…³é—­æµè§ˆå™¨è¿æ¥å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    async def main():
        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        
        # è¿™é‡Œéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„è°ƒè¯•ç«¯å£
        debug_port = "ws://127.0.0.1:9222"
        
        parser = TwitterParser(debug_port)
        
        try:
            await parser.connect_browser()
            await parser.navigate_to_twitter()
            
            # æŠ“å–æŒ‡å®šç”¨æˆ·çš„æ¨æ–‡
            tweets = await parser.scrape_user_tweets('elonmusk', 5)
            
            # æ¨æ–‡æŠ“å–å®Œæˆ
            pass
                
        except Exception as e:
            pass
        finally:
            await parser.close()
    
    asyncio.run(main())
    def detect_tweet_quality(self, tweet_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æµ‹æ¨æ–‡è´¨é‡å¹¶æ·»åŠ è´¨é‡æ ‡è®°"""
        try:
            quality_score = 0
            quality_issues = []
            
            # æ£€æŸ¥ç”¨æˆ·å
            username = tweet_data.get('username', '')
            if username and username != 'unknown':
                quality_score += 20
            else:
                quality_issues.append('ç¼ºå°‘ç”¨æˆ·å')
            
            # æ£€æŸ¥å†…å®¹
            content = tweet_data.get('content', '')
            if content and content != 'No content available':
                if len(content) > 10:
                    quality_score += 30
                elif len(content) > 5:
                    quality_score += 15
                    quality_issues.append('å†…å®¹è¿‡çŸ­')
                else:
                    quality_issues.append('å†…å®¹å¤ªçŸ­')
            else:
                quality_issues.append('ç¼ºå°‘å†…å®¹')
            
            # æ£€æŸ¥é“¾æ¥
            link = tweet_data.get('link', '')
            if link and '/status/' in link:
                quality_score += 25
            else:
                quality_issues.append('ç¼ºå°‘æœ‰æ•ˆé“¾æ¥')
            
            # æ£€æŸ¥æ—¶é—´
            if tweet_data.get('publish_time'):
                quality_score += 10
            else:
                quality_issues.append('ç¼ºå°‘å‘å¸ƒæ—¶é—´')
            
            # æ£€æŸ¥äº’åŠ¨æ•°æ®
            has_engagement = (tweet_data.get('likes', 0) > 0 or 
                            tweet_data.get('comments', 0) > 0 or 
                            tweet_data.get('retweets', 0) > 0)
            if has_engagement:
                quality_score += 15
            else:
                quality_issues.append('ç¼ºå°‘äº’åŠ¨æ•°æ®')
            
            # æ·»åŠ è´¨é‡ä¿¡æ¯
            tweet_data['quality_score'] = quality_score
            tweet_data['quality_issues'] = quality_issues
            tweet_data['quality_level'] = (
                'high' if quality_score >= 80 else
                'medium' if quality_score >= 50 else
                'low'
            )
            
            return tweet_data
            
        except Exception as e:
            self.logger.debug(f"è´¨é‡æ£€æµ‹å¤±è´¥: {e}")
            tweet_data['quality_score'] = 0
            tweet_data['quality_issues'] = ['è´¨é‡æ£€æµ‹å¤±è´¥']
            tweet_data['quality_level'] = 'unknown'
            return tweet_data

