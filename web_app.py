#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TwitteræŠ“å–Webç®¡ç†ç³»ç»Ÿ
æä¾›Webç•Œé¢è¿›è¡Œå…³é”®è¯é…ç½®ã€ä»»åŠ¡ç®¡ç†å’Œæ•°æ®æŸ¥çœ‹
"""

import os
import json
import sqlite3
import subprocess
import tempfile
import time
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from flask import Flask, render_template, request, jsonify, redirect, url_for, flash
from flask_sqlalchemy import SQLAlchemy
import asyncio
import threading
from dataclasses import asdict
import re

# å¯¼å…¥ç°æœ‰æ¨¡å—

# é»˜è®¤é…ç½®å®šä¹‰ï¼ˆå°†ä»æ•°æ®åº“åŠ è½½è¦†ç›–ï¼‰
TWITTER_TARGETS = {
    'accounts': [],
    'keywords': []
}

FILTER_CONFIG = {
    'min_likes': 50,
    'min_comments': 10,
    'min_retweets': 20,
    'keywords_filter': [],
    'max_tweets_per_target': 8,
    'max_total_tweets': 200,
    'min_content_length': 20,
    'max_content_length': 1000,
    'max_age_hours': 72,
}

OUTPUT_CONFIG = {
    'data_dir': './data',
    'excel_filename_format': 'twitter_daily_{date}.xlsx',
    'sheet_name': 'Twitteræ•°æ®',
}

BROWSER_CONFIG = {
    'headless': False,
    'timeout': 8000,
    'wait_time': 0.3,
    'scroll_pause_time': 0.3,
    'navigation_timeout': 10000,
    'load_state_timeout': 4000,
    'fast_mode': True,
    'skip_images': True,
    'disable_animations': True,
}

LOG_CONFIG = {
    'level': 'INFO',
    'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    'filename': 'twitter_scraper.log'
}

CLOUD_SYNC_CONFIG = {
    'google_sheets': {
        'enabled': False,
        'credentials_file': './credentials/google-credentials.json',
        'spreadsheet_id': '',
        'worksheet_name': 'Twitteræ•°æ®',
    },
    'feishu': {
        'enabled': False,
        'app_id': '',
        'app_secret': '',
        'spreadsheet_token': '',
        'sheet_id': '',
    }
}

# é£ä¹¦é…ç½®ä¿¡æ¯
FEISHU_CONFIG = {
    'app_id': '',
    'app_secret': '',
    'spreadsheet_token': '',
    'table_id': '',
    'enabled': True  # é»˜è®¤å¯ç”¨é£ä¹¦åŒæ­¥
}

# AdsPoweré…ç½®ä¿¡æ¯
ADS_POWER_CONFIG = {
    'local_api_url': 'http://local.adspower.net:50325',
    'user_id': 'k11p9ypc',
    'multi_user_ids': [],
    'max_concurrent_tasks': 2,
    'task_timeout': 900,
    'browser_startup_delay': 2,
    'headless': False,
    'health_check': True
}
from models import TweetModel, ScrapingConfig
from ads_browser_launcher import AdsPowerLauncher
from twitter_parser import TwitterParser
# from enhanced_twitter_parser import MultiWindowEnhancedScraper
# from optimized_scraping_engine import OptimizedScrapingEngine
from cloud_sync import CloudSyncManager
from excel_writer import ExcelWriter
from refactored_task_manager import RefactoredTaskManager

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
app.debug = True
app.config['SECRET_KEY'] = 'twitter-scraper-web-2024'
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////Users/aron/twitter-daily-scraper/instance/twitter_scraper.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

# è®¾ç½®å­—ç¬¦ç¼–ç 
app.config['JSON_AS_ASCII'] = False

# é…ç½®æ—¥å¿—
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
app.logger.setLevel(logging.INFO)

# å‡å°‘werkzeug HTTPè¯·æ±‚æ—¥å¿—è¾“å‡º
logging.getLogger('werkzeug').setLevel(logging.WARNING)

@app.after_request
def after_request(response):
    """è®¾ç½®å“åº”å¤´ï¼Œç¡®ä¿æ­£ç¡®å¤„ç†ä¸­æ–‡å­—ç¬¦"""
    response.headers['Content-Type'] = 'text/html; charset=utf-8'
    return response

# åˆå§‹åŒ–Flaskæ‰©å±•
db = SQLAlchemy(app)

def load_config_from_database():
    """ä»æ•°æ®åº“åŠ è½½é…ç½®"""
    global ADS_POWER_CONFIG, FEISHU_CONFIG, TWITTER_TARGETS, FILTER_CONFIG, OUTPUT_CONFIG, BROWSER_CONFIG, LOG_CONFIG, CLOUD_SYNC_CONFIG
    
    try:
        configs = SystemConfig.query.all()
        config_dict = {cfg.key: cfg.value for cfg in configs}
        
        # åŠ è½½AdsPoweré…ç½®
        if 'adspower_api_url' in config_dict:
            ADS_POWER_CONFIG['local_api_url'] = config_dict['adspower_api_url']
        if 'adspower_api_status' in config_dict:
            ADS_POWER_CONFIG['api_status'] = config_dict['adspower_api_status']
        if 'adspower_api_key' in config_dict:
            ADS_POWER_CONFIG['api_key'] = config_dict['adspower_api_key']
        if 'adspower_user_id' in config_dict:
            ADS_POWER_CONFIG['user_id'] = config_dict['adspower_user_id']
        if 'adspower_group_id' in config_dict:
            ADS_POWER_CONFIG['group_id'] = config_dict['adspower_group_id']
        if 'adspower_multi_user_ids' in config_dict:
            multi_ids = config_dict['adspower_multi_user_ids']
            ADS_POWER_CONFIG['multi_user_ids'] = [uid.strip() for uid in multi_ids.split('\n') if uid.strip()] if multi_ids else []
            # è®¾ç½®user_idsç”¨äºä»»åŠ¡ç®¡ç†å™¨
            if ADS_POWER_CONFIG['multi_user_ids']:
                ADS_POWER_CONFIG['user_ids'] = ADS_POWER_CONFIG['multi_user_ids']
            else:
                ADS_POWER_CONFIG['user_ids'] = [ADS_POWER_CONFIG['user_id']]
        if 'max_concurrent_tasks' in config_dict:
            ADS_POWER_CONFIG['max_concurrent_tasks'] = int(config_dict['max_concurrent_tasks'])
        if 'task_timeout' in config_dict:
            ADS_POWER_CONFIG['task_timeout'] = int(config_dict['task_timeout'])
        if 'request_interval' in config_dict:
            ADS_POWER_CONFIG['request_interval'] = float(config_dict['request_interval'])
        if 'user_rotation_enabled' in config_dict:
            ADS_POWER_CONFIG['user_rotation_enabled'] = config_dict['user_rotation_enabled'].lower() == 'true'
        if 'user_switch_interval' in config_dict:
            ADS_POWER_CONFIG['user_switch_interval'] = int(config_dict['user_switch_interval'])
        if 'api_retry_delay' in config_dict:
            ADS_POWER_CONFIG['api_retry_delay'] = float(config_dict['api_retry_delay'])
        if 'browser_startup_delay' in config_dict:
            ADS_POWER_CONFIG['browser_startup_delay'] = float(config_dict['browser_startup_delay'])
        if 'adspower_timeout' in config_dict:
            ADS_POWER_CONFIG['timeout'] = int(config_dict['adspower_timeout'])
        if 'adspower_retry_count' in config_dict:
            ADS_POWER_CONFIG['retry_count'] = int(config_dict['adspower_retry_count'])
        if 'adspower_retry_delay' in config_dict:
            ADS_POWER_CONFIG['retry_delay'] = int(config_dict['adspower_retry_delay'])
        if 'adspower_headless' in config_dict:
            ADS_POWER_CONFIG['headless'] = config_dict['adspower_headless'].lower() == 'true'
        if 'adspower_health_check' in config_dict:
            ADS_POWER_CONFIG['health_check'] = config_dict['adspower_health_check'].lower() == 'true'
        if 'adspower_window_visible' in config_dict:
            ADS_POWER_CONFIG['window_visible'] = config_dict['adspower_window_visible'].lower() == 'true'
        
        # åŠ è½½é£ä¹¦é…ç½®
        if 'feishu_app_id' in config_dict:
            FEISHU_CONFIG['app_id'] = config_dict['feishu_app_id']
        if 'feishu_app_secret' in config_dict:
            FEISHU_CONFIG['app_secret'] = config_dict['feishu_app_secret']
        if 'feishu_spreadsheet_token' in config_dict:
            FEISHU_CONFIG['spreadsheet_token'] = config_dict['feishu_spreadsheet_token']
        if 'feishu_table_id' in config_dict:
            FEISHU_CONFIG['table_id'] = config_dict['feishu_table_id']
        if 'feishu_enabled' in config_dict:
            FEISHU_CONFIG['enabled'] = config_dict['feishu_enabled'].lower() == 'true'
        if 'feishu_auto_sync' in config_dict:
            FEISHU_CONFIG['auto_sync'] = config_dict['feishu_auto_sync'].lower() == 'true'
        
        print("âœ… é…ç½®å·²ä»æ•°æ®åº“åŠ è½½å®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸ é…ç½®åŠ è½½å¤±è´¥: {e}")

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    with app.app_context():
        db.create_all()
        
        # ç¡®ä¿noteså­—æ®µå­˜åœ¨
        try:
            # å°è¯•æ·»åŠ noteså­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            with db.engine.connect() as conn:
                conn.execute(db.text('ALTER TABLE scraping_task ADD COLUMN notes TEXT'))
                conn.commit()
        except Exception:
            # å­—æ®µå·²å­˜åœ¨æˆ–å…¶ä»–é”™è¯¯ï¼Œå¿½ç•¥
            pass
        
        # å¼ºåˆ¶åˆ·æ–°æ•°æ®åº“è¿æ¥å’Œå…ƒæ•°æ®
        db.session.commit()
        db.session.close()
        
        # é‡ç½®æ‰€æœ‰runningçŠ¶æ€çš„ä»»åŠ¡ä¸ºpendingçŠ¶æ€
        # è¿™æ˜¯ä¸ºäº†è§£å†³ç³»ç»Ÿé‡å¯åä»»åŠ¡çŠ¶æ€ä¸ä¸€è‡´çš„é—®é¢˜
        # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œç­‰åº”ç”¨å¯åŠ¨åå†å¤„ç†
        # try:
        #     running_tasks = ScrapingTask.query.filter_by(status='running').all()
        #     if running_tasks:
        #         for task in running_tasks:
        #             task.status = 'pending'
        #         db.session.commit()
        # except Exception as e:
        #     print(f"âš ï¸ é‡ç½®ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        
        # ä»æ•°æ®åº“åŠ è½½é…ç½®
        try:
            load_config_from_database()
        except Exception as e:
            print(f"âš ï¸ é…ç½®åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        # æ³¨æ„ï¼šä»»åŠ¡ç®¡ç†å™¨å·²åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤åˆå§‹åŒ–

# æ•°æ®åº“æ¨¡å‹
class ScrapingTask(db.Model):
    """æŠ“å–ä»»åŠ¡æ¨¡å‹"""
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    target_accounts = db.Column(db.Text)  # JSONæ ¼å¼å­˜å‚¨
    target_keywords = db.Column(db.Text)  # JSONæ ¼å¼å­˜å‚¨
    max_tweets = db.Column(db.Integer, default=50)
    min_likes = db.Column(db.Integer, default=0)
    min_retweets = db.Column(db.Integer, default=0)
    min_comments = db.Column(db.Integer, default=0)
    status = db.Column(db.String(20), default='pending')  # pending, running, completed, failed, queued
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    started_at = db.Column(db.DateTime)
    completed_at = db.Column(db.DateTime)
    result_count = db.Column(db.Integer, default=0)
    error_message = db.Column(db.Text)
    notes = db.Column(db.Text)  # ä»»åŠ¡å¤‡æ³¨ï¼Œç”¨äºå­˜å‚¨å†…å®¹ä¸è¶³ç­‰æé†’ä¿¡æ¯
    
    @property
    def keywords(self):
        """è·å–å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºæ¨¡æ¿å…¼å®¹æ€§"""
        try:
            keywords_list = json.loads(self.target_keywords or '[]')
            return ','.join(keywords_list) if keywords_list else ''
        except:
            return self.target_keywords or ''
    
    @property
    def accounts(self):
        """è·å–è´¦å·åˆ—è¡¨ï¼Œç”¨äºæ¨¡æ¿å…¼å®¹æ€§"""
        try:
            accounts_list = json.loads(self.target_accounts or '[]')
            return ','.join(accounts_list) if accounts_list else ''
        except:
            return self.target_accounts or ''
    
    @property
    def tweets_collected(self):
        """è·å–å·²æ”¶é›†çš„æ¨æ–‡æ•°é‡"""
        return self.result_count or 0
    
    def to_dict(self):
        return {
            'id': self.id,
            'name': self.name,
            'target_accounts': json.loads(self.target_accounts or '[]'),
            'target_keywords': json.loads(self.target_keywords or '[]'),
            'max_tweets': self.max_tweets,
            'min_likes': self.min_likes,
            'min_retweets': self.min_retweets,
            'min_comments': self.min_comments,
            'status': self.status,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'started_at': self.started_at.isoformat() if self.started_at else None,
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'result_count': self.result_count,
            'error_message': self.error_message,
            'notes': self.notes
        }

class TweetData(db.Model):
    """æ¨æ–‡æ•°æ®æ¨¡å‹"""
    id = db.Column(db.Integer, primary_key=True)
    task_id = db.Column(db.Integer, db.ForeignKey('scraping_task.id'), nullable=False)
    username = db.Column(db.String(50), nullable=False)
    content = db.Column(db.Text, nullable=False)
    likes = db.Column(db.Integer, default=0)
    comments = db.Column(db.Integer, default=0)
    retweets = db.Column(db.Integer, default=0)
    publish_time = db.Column(db.String(100))
    link = db.Column(db.Text)
    hashtags = db.Column(db.Text)  # è¯é¢˜æ ‡ç­¾ï¼ŒJSONæ ¼å¼å­˜å‚¨
    content_type = db.Column(db.String(50))  # ç±»å‹æ ‡ç­¾ï¼šæé’±ã€æŠ•æ”¾ã€å‰¯ä¸šå¹²è´§ã€æƒ…ç»ªç±»ç­‰
    scraped_at = db.Column(db.DateTime, default=datetime.utcnow)
    synced_to_feishu = db.Column(db.Boolean, default=False)
    
    # å¢å¼ºå†…å®¹å­—æ®µ
    full_content = db.Column(db.Text)  # å®Œæ•´æ¨æ–‡å†…å®¹ï¼ˆè¯¦æƒ…é¡µæŠ“å–ï¼‰
    media_content = db.Column(db.Text)  # å¤šåª’ä½“å†…å®¹ï¼ŒJSONæ ¼å¼å­˜å‚¨
    thread_tweets = db.Column(db.Text)  # æ¨æ–‡çº¿ç¨‹ï¼ŒJSONæ ¼å¼å­˜å‚¨
    quoted_tweet = db.Column(db.Text)  # å¼•ç”¨æ¨æ–‡ï¼ŒJSONæ ¼å¼å­˜å‚¨
    has_detailed_content = db.Column(db.Boolean, default=False)  # æ˜¯å¦åŒ…å«è¯¦æƒ…é¡µå†…å®¹
    detail_error = db.Column(db.Text)  # è¯¦æƒ…æŠ“å–é”™è¯¯ä¿¡æ¯
    
    def to_dict(self):
        return {
            'id': self.id,
            'task_id': self.task_id,
            'username': self.username,
            'content': self.content,
            'likes': self.likes,
            'comments': self.comments,
            'retweets': self.retweets,
            'publish_time': self.publish_time,
            'link': self.link,
            'hashtags': json.loads(self.hashtags or '[]'),
            'content_type': self.content_type,
            'scraped_at': self.scraped_at.isoformat() if self.scraped_at else None,
            'synced_to_feishu': self.synced_to_feishu,
            'full_content': self.full_content,
            'media_content': json.loads(self.media_content) if self.media_content else [],
            'thread_tweets': json.loads(self.thread_tweets) if self.thread_tweets else [],
            'quoted_tweet': json.loads(self.quoted_tweet) if self.quoted_tweet else None,
            'has_detailed_content': self.has_detailed_content,
            'detail_error': self.detail_error
        }

class SystemConfig(db.Model):
    """ç³»ç»Ÿé…ç½®æ¨¡å‹"""
    id = db.Column(db.Integer, primary_key=True)
    key = db.Column(db.String(100), unique=True, nullable=False)
    value = db.Column(db.Text)
    description = db.Column(db.Text)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow)

class TwitterInfluencer(db.Model):
    """Twitteråšä¸»ç®¡ç†æ¨¡å‹"""
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)  # åšä¸»åç§°
    username = db.Column(db.String(50), nullable=False)  # Twitterç”¨æˆ·å
    profile_url = db.Column(db.Text, nullable=False)  # åšä¸»ä¸»é¡µURL
    description = db.Column(db.Text)  # åšä¸»æè¿°
    category = db.Column(db.String(50))  # åˆ†ç±»ï¼šæé’±ã€æŠ•æ”¾ã€å‰¯ä¸šå¹²è´§ã€æƒ…ç»ªç±»ç­‰
    followers_count = db.Column(db.Integer, default=0)  # ç²‰ä¸æ•°
    is_active = db.Column(db.Boolean, default=True)  # æ˜¯å¦å¯ç”¨
    last_scraped = db.Column(db.DateTime)  # æœ€åæŠ“å–æ—¶é—´
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    def to_dict(self):
        return {
            'id': self.id,
            'name': self.name,
            'username': self.username,
            'profile_url': self.profile_url,
            'description': self.description,
            'category': self.category,
            'followers_count': self.followers_count,
            'is_active': self.is_active,
            'last_scraped': self.last_scraped.isoformat() if self.last_scraped else None,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }

# å…¨å±€å˜é‡
current_task = None
task_thread = None
task_executor = None

# å†…å®¹åˆ†ç±»å‡½æ•°
def classify_content_type(content: str) -> str:
    """
    æ ¹æ®æ¨æ–‡å†…å®¹è‡ªåŠ¨åˆ†ç±»ï¼ˆä»…ä½œä¸ºå»ºè®®ï¼Œç”¨æˆ·å¯è‡ªå®šä¹‰ï¼‰
    
    Args:
        content: æ¨æ–‡å†…å®¹
        
    Returns:
        å†…å®¹ç±»å‹åˆ†ç±»å»ºè®®ï¼Œå¦‚æœæ²¡æœ‰æ˜ç¡®åŒ¹é…åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not content:
        return ''
    
    content_lower = content.lower()
    
    # æé’±ç±»å…³é”®è¯
    money_keywords = ['èµšé’±', 'æ”¶å…¥', 'ç›ˆåˆ©', 'æŠ•èµ„', 'ç†è´¢', 'å‰¯ä¸š', 'åˆ›ä¸š', 'å•†æœº', 'å˜ç°', 'æ”¶ç›Š', 
                     'è´¢å¯Œ', 'é‡‘é’±', 'æŒ£é’±', 'æœˆå…¥', 'å¹´å…¥', 'è¢«åŠ¨æ”¶å…¥', 'ç°é‡‘æµ', 'æŠ•èµ„å›æŠ¥']
    
    # æŠ•æ”¾ç±»å…³é”®è¯
    ads_keywords = ['æŠ•æ”¾', 'å¹¿å‘Š', 'æ¨å¹¿', 'è¥é”€', 'roi', 'cpm', 'cpc', 'ctr', 'è½¬åŒ–ç‡', 
                   'è·å®¢', 'å¼•æµ', 'æŠ•æ”¾ç­–ç•¥', 'å¹¿å‘Šä¼˜åŒ–', 'ç´ æ', 'åˆ›æ„', 'æŠ•æ”¾æ•ˆæœ']
    
    # å‰¯ä¸šå¹²è´§ç±»å…³é”®è¯
    side_hustle_keywords = ['å‰¯ä¸š', 'å…¼èŒ', 'è‡ªåª’ä½“', 'å†…å®¹åˆ›ä½œ', 'çŸ¥è¯†ä»˜è´¹', 'åœ¨çº¿æ•™è‚²', 
                           'æŠ€èƒ½å˜ç°', 'ä¸ªäººå“ç‰Œ', 'æµé‡', 'ç²‰ä¸', 'è¿è¥', 'å¢é•¿', 'å¹²è´§']
    
    # æƒ…ç»ªç±»å…³é”®è¯
    emotion_keywords = ['ç„¦è™‘', 'å‹åŠ›', 'è¿·èŒ«', 'å›°æƒ‘', 'å¼€å¿ƒ', 'å¿«ä¹', 'æ„ŸåŠ¨', 'æ¿€åŠ¨', 
                       'æ²®ä¸§', 'å¤±æœ›', 'æ„¤æ€’', 'æ— å¥ˆ', 'æ„Ÿæ…¨', 'æ€è€ƒ', 'åæ€', 'æ„Ÿæ‚Ÿ']
    
    # æŠ€æœ¯ç±»å…³é”®è¯
    tech_keywords = ['ai', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç®—æ³•', 'ç¼–ç¨‹', 'ä»£ç ', 
                    'å¼€å‘', 'æŠ€æœ¯', 'å·¥å…·', 'è½¯ä»¶', 'åº”ç”¨', 'chatgpt', 'gpt', 'è‡ªåŠ¨åŒ–']
    
    # æ£€æŸ¥å„ç±»å…³é”®è¯ï¼Œåªæœ‰æ˜ç¡®åŒ¹é…æ‰è¿”å›åˆ†ç±»
    if any(keyword in content_lower for keyword in money_keywords):
        return 'æé’±'
    elif any(keyword in content_lower for keyword in ads_keywords):
        return 'æŠ•æ”¾'
    elif any(keyword in content_lower for keyword in side_hustle_keywords):
        return 'å‰¯ä¸šå¹²è´§'
    elif any(keyword in content_lower for keyword in emotion_keywords):
        return 'æƒ…ç»ªç±»'
    elif any(keyword in content_lower for keyword in tech_keywords):
        return 'æŠ€æœ¯ç±»'
    else:
        return ''  # è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè®©ç”¨æˆ·è‡ªå®šä¹‰

def detect_account_type(account_name: str, account_description: str = '') -> str:
    """
    æ ¹æ®è´¦å·ä¿¡æ¯æ£€æµ‹è´¦å·ç±»å‹
    
    Args:
        account_name: è´¦å·åç§°
        account_description: è´¦å·æè¿°
        
    Returns:
        è´¦å·ç±»å‹
    """
    combined_text = f"{account_name} {account_description}".lower()
    
    # æŠ€æœ¯åšä¸»å…³é”®è¯
    tech_keywords = [
        'ç¨‹åºå‘˜', 'å¼€å‘è€…', 'å·¥ç¨‹å¸ˆ', 'developer', 'engineer', 'programmer',
        'å‰ç«¯', 'åç«¯', 'å…¨æ ˆ', 'frontend', 'backend', 'fullstack',
        'python', 'javascript', 'java', 'go', 'rust', 'ai', 'äººå·¥æ™ºèƒ½',
        'ç®—æ³•', 'æ¶æ„å¸ˆ', 'cto', 'æŠ€æœ¯', 'tech', 'code', 'ç¼–ç¨‹'
    ]
    
    # è¥é”€åšä¸»å…³é”®è¯
    marketing_keywords = [
        'è¥é”€', 'æ¨å¹¿', 'å¢é•¿', 'è¿è¥', 'å¹¿å‘Š', 'marketing', 'growth',
        'æŠ•æ”¾', 'è·å®¢', 'è½¬åŒ–', 'roi', 'æµé‡', 'å¼•æµ', 'å˜ç°',
        'ç”µå•†', 'ç›´æ’­', 'å¸¦è´§', 'ç½‘çº¢', 'kol', 'åšä¸»'
    ]
    
    # æŠ•èµ„åšä¸»å…³é”®è¯
    investment_keywords = [
        'æŠ•èµ„', 'ç†è´¢', 'è‚¡ç¥¨', 'åŸºé‡‘', 'æœŸè´§', 'å¤–æ±‡', 'investment',
        'è´¢ç»', 'é‡‘è', 'åˆ¸å•†', 'åˆ†æå¸ˆ', 'äº¤æ˜“å‘˜', 'trader',
        'å¸åœˆ', 'åŒºå—é“¾', 'crypto', 'bitcoin', 'é‡åŒ–', 'ä»·å€¼æŠ•èµ„'
    ]
    
    # æ£€æŸ¥å„ç±»å‹å…³é”®è¯
    if any(keyword in combined_text for keyword in tech_keywords):
        return 'æŠ€æœ¯åšä¸»'
    elif any(keyword in combined_text for keyword in marketing_keywords):
        return 'è¥é”€åšä¸»'
    elif any(keyword in combined_text for keyword in investment_keywords):
        return 'æŠ•èµ„åšä¸»'
    
    return 'general'


# é‡æ„TaskManagerçš„å¯¼å…¥
import queue
from enum import Enum
from dataclasses import dataclass
from typing import Optional, Dict, List, Tuple

class TaskState(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    QUEUED = "queued"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    STOPPED = "stopped"

@dataclass
class TaskRequest:
    """ä»»åŠ¡è¯·æ±‚æ•°æ®ç»“æ„"""
    task_id: int
    use_background_process: bool = True
    priority: int = 0
    retry_count: int = 0
    max_retries: int = 3

@dataclass
class TaskSlot:
    """ä»»åŠ¡æ§½ä½æ•°æ®ç»“æ„"""
    task_id: int
    user_id: str
    process: Optional[subprocess.Popen] = None
    thread: Optional[threading.Thread] = None
    config_file: Optional[str] = None
    start_time: Optional[datetime] = None
    is_background: bool = True


# å•ä¸ªä»»åŠ¡æ‰§è¡Œå™¨ï¼ˆä¿®æ”¹ä¸ºæ”¯æŒæŒ‡å®šç”¨æˆ·IDï¼‰
class ScrapingTaskExecutor:
    def __init__(self, user_id=None):
        self.is_running = False
        self.current_task_id = None
        self.user_id = user_id or ADS_POWER_CONFIG['user_id']
        
    async def execute_task(self, task_id: int):
        """æ‰§è¡ŒæŠ“å–ä»»åŠ¡"""
        global current_task
        
        try:
            print(f"[DEBUG] å¼€å§‹æ‰§è¡Œä»»åŠ¡ {task_id}")
            
            # è·å–ä»»åŠ¡
            task = ScrapingTask.query.get(task_id)
            if not task:
                raise Exception(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
            
            print(f"[DEBUG] ä»»åŠ¡ä¿¡æ¯: {task.name}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = 'running'
            task.started_at = datetime.utcnow()
            db.session.commit()
            
            current_task = task
            self.is_running = True
            self.current_task_id = task_id
            
            # è§£æé…ç½®
            target_accounts = json.loads(task.target_accounts or '[]')
            target_keywords = json.loads(task.target_keywords or '[]')
            
            print(f"[DEBUG] ç›®æ ‡è´¦å·: {target_accounts}")
            print(f"[DEBUG] å…³é”®è¯: {target_keywords}")
            
            # å¯åŠ¨æµè§ˆå™¨
            print(f"[DEBUG] æ­£åœ¨å¯åŠ¨AdsPoweræµè§ˆå™¨...")
            app.logger.info(f"å¼€å§‹å¯åŠ¨AdsPoweræµè§ˆå™¨ï¼Œç”¨æˆ·ID: {self.user_id}")
            
            browser_manager = AdsPowerLauncher(ADS_POWER_CONFIG)
            user_id = self.user_id  # ä½¿ç”¨åˆ†é…çš„ç”¨æˆ·ID
            
            try:
                # è¿›è¡Œå®Œæ•´çš„å¥åº·æ£€æŸ¥å’Œæµè§ˆå™¨å¯åŠ¨
                app.logger.info("æ­£åœ¨è¿›è¡ŒAdsPowerå¥åº·æ£€æŸ¥...")
                browser_info = browser_manager.start_browser(user_id, skip_health_check=False)
                if not browser_info:
                    raise Exception("æµè§ˆå™¨å¯åŠ¨å¤±è´¥ï¼šæœªè¿”å›æµè§ˆå™¨ä¿¡æ¯")
                
                app.logger.info(f"æµè§ˆå™¨å¯åŠ¨æˆåŠŸ: {browser_info}")
                print(f"[DEBUG] æµè§ˆå™¨å¯åŠ¨æˆåŠŸ: {browser_info}")
                
            except Exception as e:
                app.logger.error(f"AdsPoweræµè§ˆå™¨å¯åŠ¨å¤±è´¥: {str(e)}")
                
                # è·å–è¯¦ç»†çš„å¥åº·æŠ¥å‘Š
                try:
                    health_report = browser_manager.get_health_report()
                    app.logger.error(f"ç³»ç»Ÿå¥åº·æŠ¥å‘Š: {health_report}")
                    
                    # å°è¯•è‡ªåŠ¨ä¿®å¤
                    app.logger.info("å°è¯•è‡ªåŠ¨ä¿®å¤ç³»ç»Ÿé—®é¢˜...")
                    if browser_manager.auto_optimize_system():
                        app.logger.info("ç³»ç»Ÿä¼˜åŒ–å®Œæˆï¼Œé‡æ–°å°è¯•å¯åŠ¨æµè§ˆå™¨...")
                        browser_info = browser_manager.start_browser(user_id, skip_health_check=True)
                        if browser_info:
                            app.logger.info("æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼ˆä¿®å¤åï¼‰")
                        else:
                            raise Exception("æµè§ˆå™¨å¯åŠ¨å¤±è´¥ï¼ˆä¿®å¤åä»ç„¶å¤±è´¥ï¼‰")
                    else:
                        raise Exception(f"AdsPoweræµè§ˆå™¨å¯åŠ¨å¤±è´¥ä¸”è‡ªåŠ¨ä¿®å¤å¤±è´¥: {str(e)}")
                        
                except Exception as repair_error:
                    app.logger.error(f"è‡ªåŠ¨ä¿®å¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(repair_error)}")
                    raise Exception(f"AdsPoweræµè§ˆå™¨å¯åŠ¨å¤±è´¥: {str(e)}ã€‚ä¿®å¤å°è¯•ä¹Ÿå¤±è´¥: {str(repair_error)}")
            
            debug_port = browser_info.get('ws', {}).get('puppeteer')
            print(f"[DEBUG] è°ƒè¯•ç«¯å£: {debug_port}")
            
            # è¿æ¥è§£æå™¨
            print(f"[DEBUG] æ­£åœ¨è¿æ¥Twitterè§£æå™¨...")
            parser = TwitterParser(debug_port)
            await parser.connect_browser()
            print(f"[DEBUG] Twitterè§£æå™¨è¿æ¥æˆåŠŸ")
            
            all_tweets = []
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦ç»„åˆæœç´¢ï¼ˆåŒæ—¶æœ‰è´¦å·å’Œå…³é”®è¯ï¼‰
            if target_accounts and target_keywords:
                print(f"[DEBUG] æ£€æµ‹åˆ°ç»„åˆæœç´¢æ¨¡å¼ï¼šåœ¨æŒ‡å®šåšä¸»ä¸‹æœç´¢å…³é”®è¯")
                print(f"[DEBUG] ç›®æ ‡åšä¸»: {target_accounts}")
                print(f"[DEBUG] æœç´¢å…³é”®è¯: {target_keywords}")
                
                # ç»„åˆæœç´¢ï¼šåœ¨æ¯ä¸ªæŒ‡å®šåšä¸»ä¸‹æœç´¢æ¯ä¸ªå…³é”®è¯
                for account in target_accounts:
                    if not self.is_running:
                        break
                    
                    # æ¸…ç†ç”¨æˆ·åï¼Œå»é™¤@ç¬¦å·
                    clean_username = account.lstrip('@')
                    
                    for keyword in target_keywords:
                        if not self.is_running:
                            break
                        
                        try:
                            print(f"[DEBUG] åœ¨åšä¸» @{clean_username} ä¸‹æœç´¢å…³é”®è¯ '{keyword}'")
                            tweets = await parser.scrape_user_keyword_tweets(
                                username=clean_username, 
                                keyword=keyword, 
                                max_tweets=task.max_tweets,
                                enable_enhanced=True
                            )
                            
                            # è¿‡æ»¤æ¨æ–‡
                            filtered_tweets = self._filter_tweets(tweets, task)
                            all_tweets.extend(filtered_tweets)
                            
                            print(f"[DEBUG] åœ¨åšä¸» @{clean_username} ä¸‹æœç´¢å…³é”®è¯ '{keyword}' å®Œæˆï¼Œè·å¾— {len(filtered_tweets)} æ¡æœ‰æ•ˆæ¨æ–‡")
                            
                        except Exception as e:
                            print(f"åœ¨åšä¸» @{clean_username} ä¸‹æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                            continue
            else:
                # åˆ†åˆ«æŠ“å–è´¦å·æ¨æ–‡å’Œå…³é”®è¯æ¨æ–‡ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                
                # æŠ“å–è´¦å·æ¨æ–‡
                for account in target_accounts:
                    if not self.is_running:  # æ£€æŸ¥æ˜¯å¦è¢«åœæ­¢
                        break
                    
                    # æ¸…ç†ç”¨æˆ·åï¼Œå»é™¤@ç¬¦å·
                    clean_username = account.lstrip('@')
                        
                    try:
                        print(f"[DEBUG] æŠ“å–åšä¸» @{clean_username} çš„æ¨æ–‡")
                        tweets = await parser.scrape_user_tweets(username=clean_username, max_tweets=task.max_tweets, enable_enhanced=True)
                        
                        # è¿‡æ»¤æ¨æ–‡
                        filtered_tweets = self._filter_tweets(tweets, task)
                        all_tweets.extend(filtered_tweets)
                        
                        print(f"[DEBUG] åšä¸» @{clean_username} æŠ“å–å®Œæˆï¼Œè·å¾— {len(filtered_tweets)} æ¡æœ‰æ•ˆæ¨æ–‡")
                        
                    except Exception as e:
                        print(f"æŠ“å–è´¦å· {clean_username} å¤±è´¥: {e}")
                        continue
                
                # æŠ“å–å…³é”®è¯æ¨æ–‡
                for keyword in target_keywords:
                    if not self.is_running:
                        break
                        
                    try:
                        print(f"[DEBUG] å…¨å±€æœç´¢å…³é”®è¯ '{keyword}'")
                        tweets = await parser.scrape_keyword_tweets(keyword, max_tweets=task.max_tweets, enable_enhanced=True)
                        filtered_tweets = self._filter_tweets(tweets, task)
                        all_tweets.extend(filtered_tweets)
                        
                        print(f"[DEBUG] å…³é”®è¯ '{keyword}' æœç´¢å®Œæˆï¼Œè·å¾— {len(filtered_tweets)} æ¡æœ‰æ•ˆæ¨æ–‡")
                        
                    except Exception as e:
                        print(f"æœç´¢å…³é”®è¯ {keyword} å¤±è´¥: {e}")
                        continue
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            saved_count = self._save_tweets_to_db(all_tweets, task_id)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = 'completed'
            task.completed_at = datetime.utcnow()
            task.result_count = saved_count
            db.session.commit()
            
            # å…³é—­æµè§ˆå™¨
            await parser.close()
            
            print(f"ä»»åŠ¡ {task_id} å®Œæˆï¼Œå…±æŠ“å– {saved_count} æ¡æ¨æ–‡")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨åŒæ­¥åˆ°é£ä¹¦
            self._check_auto_sync_feishu(task_id)
            
        except Exception as e:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            task = ScrapingTask.query.get(task_id)
            if task:
                task.status = 'failed'
                task.error_message = str(e)
                task.completed_at = datetime.utcnow()
                db.session.commit()
            
            print(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {e}")
            
        finally:
            self.is_running = False
            self.current_task_id = None
            current_task = None
    
    def _filter_tweets(self, tweets: List[Dict], task: ScrapingTask) -> List[Dict]:
        """è¿‡æ»¤æ¨æ–‡"""
        filtered = []
        for tweet in tweets:
            if (tweet.get('likes', 0) >= task.min_likes and
                tweet.get('retweets', 0) >= task.min_retweets and
                tweet.get('comments', 0) >= task.min_comments):
                filtered.append(tweet)
        return filtered
    
    def _save_tweets_to_db(self, tweets: List[Dict], task_id: int) -> int:
        """ä¿å­˜æ¨æ–‡åˆ°æ•°æ®åº“"""
        saved_count = 0
        for tweet in tweets:
            try:
                tweet_data = TweetData(
                    task_id=task_id,
                    username=tweet.get('username', ''),
                    content=tweet.get('content', ''),
                    likes=tweet.get('likes', 0),
                    comments=tweet.get('comments', 0),
                    retweets=tweet.get('retweets', 0),
                    publish_time=tweet.get('publish_time', ''),
                    link=tweet.get('link', ''),
                    hashtags=json.dumps(tweet.get('hashtags', [])),
                    content_type=classify_content_type(tweet.get('content', '')),
                    full_content=tweet.get('full_content', ''),
                    media_content=json.dumps(tweet.get('media', {'images': [], 'videos': []})),
                    thread_tweets=json.dumps(tweet.get('thread_tweets', [])),
                    quoted_tweet=json.dumps(tweet.get('quoted_tweet')) if tweet.get('quoted_tweet') else None,
                    has_detailed_content=tweet.get('has_detailed_content', False),
                    detail_error=tweet.get('detail_error')
                )
                db.session.add(tweet_data)
                saved_count += 1
            except Exception as e:
                print(f"ä¿å­˜æ¨æ–‡å¤±è´¥: {e}")
                continue
        
        db.session.commit()
        return saved_count
    
    def _check_auto_sync_feishu(self, task_id: int):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨åŒæ­¥åˆ°é£ä¹¦"""
        try:
            print(f"[è°ƒè¯•] å¼€å§‹æ£€æŸ¥ä»»åŠ¡ {task_id} çš„è‡ªåŠ¨åŒæ­¥...")
            
            # æ£€æŸ¥é£ä¹¦é…ç½®æ˜¯å¦å¯ç”¨
            if not FEISHU_CONFIG.get('enabled'):
                print(f"[è°ƒè¯•] é£ä¹¦é…ç½®æœªå¯ç”¨ï¼Œè·³è¿‡åŒæ­¥")
                return
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨åŒæ­¥
            if not FEISHU_CONFIG.get('auto_sync', False):
                print(f"[è°ƒè¯•] è‡ªåŠ¨åŒæ­¥æœªå¯ç”¨ï¼Œè·³è¿‡åŒæ­¥ (å½“å‰å€¼: {FEISHU_CONFIG.get('auto_sync', False)})")
                return
            
            # æ£€æŸ¥é£ä¹¦é…ç½®å®Œæ•´æ€§
            required_fields = ['app_id', 'app_secret', 'spreadsheet_token', 'table_id']
            missing_fields = [field for field in required_fields if not FEISHU_CONFIG.get(field)]
            if missing_fields:
                print(f"é£ä¹¦è‡ªåŠ¨åŒæ­¥è·³è¿‡ï¼šé…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {', '.join(missing_fields)}")
                return
            
            print(f"å¼€å§‹è‡ªåŠ¨åŒæ­¥ä»»åŠ¡ {task_id} çš„æ•°æ®åˆ°é£ä¹¦...")
            
            # è·å–ä»»åŠ¡æ•°æ®
            tweets = TweetData.query.filter_by(task_id=task_id).all()
            if not tweets:
                print("æ²¡æœ‰æ•°æ®éœ€è¦åŒæ­¥")
                return
            
            # å‡†å¤‡åŒæ­¥æ•°æ®
            sync_data = []
            for tweet in tweets:
                # è§£æhashtags
                try:
                    hashtags = json.loads(tweet.hashtags) if tweet.hashtags else []
                except:
                    hashtags = []
                
                # ä¸å†éœ€è¦æ—¶é—´æˆ³è½¬æ¢ï¼Œè®©é£ä¹¦è‡ªåŠ¨å¤„ç†æ—¶é—´å­—æ®µ
                
                sync_data.append({
                    'æ¨æ–‡åŸæ–‡å†…å®¹': tweet.content or '',
                    'ä½œè€…ï¼ˆè´¦å·ï¼‰': tweet.username or '',
                    'æ¨æ–‡é“¾æ¥': tweet.link or '',
                    'è¯é¢˜æ ‡ç­¾ï¼ˆHashtagï¼‰': ', '.join(hashtags),
                    'ç±»å‹æ ‡ç­¾': tweet.content_type or '',
                    'è¯„è®º': tweet.comments or 0,
                    'ç‚¹èµ': tweet.likes or 0,
                    'è½¬å‘': tweet.retweets or 0
                })
            
            # åˆ›å»ºäº‘åŒæ­¥ç®¡ç†å™¨å¹¶åŒæ­¥ï¼ˆç»Ÿä¸€åˆå§‹åŒ–æ–¹å¼ï¼Œä¸APIåŒæ­¥ä¿æŒä¸€è‡´ï¼‰
            from cloud_sync import CloudSyncManager
            sync_config = {
                'feishu': {
                    'enabled': True,
                    'app_id': FEISHU_CONFIG['app_id'],
                    'app_secret': FEISHU_CONFIG['app_secret'],
                    'spreadsheet_token': FEISHU_CONFIG['spreadsheet_token'],
                    'table_id': FEISHU_CONFIG['table_id'],
                    'base_url': 'https://open.feishu.cn/open-apis'
                }
            }
            sync_manager = CloudSyncManager(sync_config)
            
            # ç›´æ¥æ‰§è¡ŒåŒæ­¥ï¼ˆä¸è°ƒç”¨setup_feishuï¼Œä¿æŒé…ç½®å®Œæ•´æ€§ï¼‰
            success = sync_manager.sync_to_feishu(
                sync_data,
                FEISHU_CONFIG['spreadsheet_token'],
                FEISHU_CONFIG['table_id']
            )
            
            if success:
                # æ›´æ–°åŒæ­¥çŠ¶æ€
                for tweet in tweets:
                    tweet.synced_to_feishu = True
                db.session.commit()
                print(f"ä»»åŠ¡ {task_id} è‡ªåŠ¨åŒæ­¥åˆ°é£ä¹¦æˆåŠŸï¼Œå·²æ›´æ–° {len(tweets)} æ¡è®°å½•çš„åŒæ­¥çŠ¶æ€")
                
                # æ‰§è¡Œæ•°æ®éªŒè¯
                print(f"ğŸ” [AUTO_SYNC] å¼€å§‹æ•°æ®éªŒè¯...")
                try:
                    from feishu_data_validator import FeishuDataValidator
                    validator = FeishuDataValidator()
                    validation_result = validator.validate_sync_data(task_id=task_id)
                    
                    if validation_result.get('success'):
                        comparison = validation_result['comparison_result']
                        summary = comparison['summary']
                        print(f"âœ… [AUTO_SYNC] æ•°æ®éªŒè¯å®Œæˆ")
                        print(f"ğŸ“Š [AUTO_SYNC] éªŒè¯ç»“æœ: åŒæ­¥å‡†ç¡®ç‡ {summary['sync_accuracy']:.2f}%")
                        print(f"ğŸ“Š [AUTO_SYNC] åŒ¹é…è®°å½•: {summary['matched_count']}/{summary['total_local']}")
                        
                        if summary['sync_accuracy'] < 95:
                            print(f"âš ï¸ [AUTO_SYNC] å‘ç° {summary['field_mismatch_count']} æ¡å­—æ®µä¸åŒ¹é…ï¼Œå»ºè®®æ£€æŸ¥")
                    else:
                        print(f"âš ï¸ [AUTO_SYNC] æ•°æ®éªŒè¯å¤±è´¥: {validation_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                        
                except Exception as e:
                    print(f"âŒ [AUTO_SYNC] æ•°æ®éªŒè¯å¼‚å¸¸: {e}")
            else:
                print(f"ä»»åŠ¡ {task_id} è‡ªåŠ¨åŒæ­¥åˆ°é£ä¹¦å¤±è´¥")
                
        except Exception as e:
            print(f"è‡ªåŠ¨åŒæ­¥åˆ°é£ä¹¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    def stop_task(self):
        """åœæ­¢å½“å‰ä»»åŠ¡"""
        self.is_running = False

# å…¨å±€å¹¶è¡Œä»»åŠ¡ç®¡ç†å™¨ï¼ˆå°†åœ¨é…ç½®åŠ è½½ååˆå§‹åŒ–ï¼‰
task_manager = None
optimized_scraper = None

def init_task_manager():
    """åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨"""
    global task_manager, optimized_scraper
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
    if task_manager is not None:
        print("âš ï¸ TaskManagerå·²ç»åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
        return
    
    max_concurrent = ADS_POWER_CONFIG.get('max_concurrent_tasks', 2)
    
    # è·å–ç”¨æˆ·IDåˆ—è¡¨ï¼Œä¼˜å…ˆä½¿ç”¨user_idsï¼Œç„¶åæ˜¯multi_user_idsï¼Œæœ€åæ˜¯å•ä¸ªuser_id
    user_ids = ADS_POWER_CONFIG.get('user_ids')
    if not user_ids:
        user_ids = ADS_POWER_CONFIG.get('multi_user_ids')
    if not user_ids:
        user_ids = [ADS_POWER_CONFIG.get('user_id', 'default')]
    
    print(f"[TaskManager] é…ç½®ä¿¡æ¯:")
    print(f"  - æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°: {max_concurrent}")
    print(f"  - ç”¨æˆ·IDåˆ—è¡¨: {user_ids}")
    print(f"  - ç”¨æˆ·IDæ•°é‡: {len(user_ids)}")
    
    # ç¡®ä¿ç”¨æˆ·IDæ•°é‡è¶³å¤Ÿæ”¯æŒå¹¶å‘ä»»åŠ¡
    if len(user_ids) < max_concurrent:
        print(f"âš ï¸ è­¦å‘Š: ç”¨æˆ·IDæ•°é‡({len(user_ids)})å°‘äºæœ€å¤§å¹¶å‘ä»»åŠ¡æ•°({max_concurrent})")
        print(f"âš ï¸ å»ºè®®é…ç½®è‡³å°‘ {max_concurrent} ä¸ªç”¨æˆ·IDä»¥æ”¯æŒå®Œå…¨å¹¶è¡Œ")
    
    task_manager = RefactoredTaskManager(max_concurrent_tasks=max_concurrent, user_ids=user_ids)
    
    print(f"[RefactoredTaskManager] åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å¹¶å‘: {max_concurrent}")
    
    # åˆå§‹åŒ–ä¼˜åŒ–æŠ“å–å™¨
    # optimized_scraper = MultiWindowEnhancedScraper(max_workers=max_concurrent)
    
    print(f"âœ… TaskManagerå·²åˆå§‹åŒ–ï¼Œæœ€å¤§å¹¶å‘ä»»åŠ¡æ•°: {max_concurrent}")
    print(f"âœ… ç”¨æˆ·IDæ± å¤§å°: {len(user_ids)}")
    print(f"âœ… OptimizedScraperå·²åˆå§‹åŒ–ï¼Œæ”¯æŒå¤šçª—å£å¹¶å‘æŠ“å–")

# åœ¨æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–
try:
    init_database()
except Exception as e:
    print(f"âš ï¸ åˆå§‹åŒ–å¤±è´¥: {e}")

# è·¯ç”±å®šä¹‰
@app.route('/')
def index():
    """é¦–é¡µ"""
    from datetime import datetime, date
    import sys
    import psutil
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    today = date.today()
    stats = {
        'total_tasks': ScrapingTask.query.count(),
        'total_tweets': TweetData.query.count(),
        'running_tasks': ScrapingTask.query.filter_by(status='running').count(),
        'completed_tasks': ScrapingTask.query.filter_by(status='completed').count(),
        'today_tweets': TweetData.query.filter(db.func.date(TweetData.scraped_at) == today).count()
    }
    
    # è·å–æœ€è¿‘çš„ä»»åŠ¡
    recent_tasks = ScrapingTask.query.order_by(ScrapingTask.created_at.desc()).limit(5).all()
    
    # è·å–ç³»ç»Ÿä¿¡æ¯
    try:
        # è®¡ç®—è¿è¡Œæ—¶é—´
        import time
        start_time = getattr(app, 'start_time', time.time())
        uptime_seconds = int(time.time() - start_time)
        uptime_hours = uptime_seconds // 3600
        uptime_minutes = (uptime_seconds % 3600) // 60
        uptime = f"{uptime_hours}å°æ—¶{uptime_minutes}åˆ†é’Ÿ"
        
        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory = psutil.virtual_memory()
        memory_usage = f"{memory.percent}% ({memory.used // (1024**3):.1f}GB/{memory.total // (1024**3):.1f}GB)"
        
        # è·å–Pythonç‰ˆæœ¬
        python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        
        # è·å–æ•°æ®åº“å¤§å°
        db_path = app.config['SQLALCHEMY_DATABASE_URI'].replace('sqlite:///', '')
        if os.path.exists(db_path):
            db_size_bytes = os.path.getsize(db_path)
            if db_size_bytes < 1024**2:
                db_size = f"{db_size_bytes / 1024:.1f} KB"
            elif db_size_bytes < 1024**3:
                db_size = f"{db_size_bytes / (1024**2):.1f} MB"
            else:
                db_size = f"{db_size_bytes / (1024**3):.1f} GB"
        else:
            db_size = "æœªçŸ¥"
        
        system_info = {
            'uptime': uptime,
            'memory_usage': memory_usage,
            'python_version': python_version,
            'db_size': db_size
        }
    except Exception as e:
        app.logger.error(f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")
        system_info = {
            'uptime': 'æœªçŸ¥',
            'memory_usage': 'æœªçŸ¥',
            'python_version': 'æœªçŸ¥',
            'db_size': 'æœªçŸ¥'
        }
    
    return render_template('index.html', stats=stats, recent_tasks=recent_tasks, system_info=system_info)

@app.route('/tasks')
def tasks():
    """ä»»åŠ¡ç®¡ç†é¡µé¢"""
    tasks = ScrapingTask.query.order_by(ScrapingTask.created_at.desc()).all()
    
    # è®¡ç®—ä»»åŠ¡ç»Ÿè®¡æ•°æ®
    task_stats = {
        'total': len(tasks),
        'pending': len([t for t in tasks if t.status == 'pending']),
        'running': len([t for t in tasks if t.status == 'running']),
        'completed': len([t for t in tasks if t.status == 'completed']),
        'failed': len([t for t in tasks if t.status == 'failed']),
        'queued': len([t for t in tasks if t.status == 'queued'])
    }
    
    return render_template('tasks.html', tasks=tasks, task_stats=task_stats)

@app.route('/create_task', methods=['GET', 'POST'])
def create_task():
    """åˆ›å»ºä»»åŠ¡é¡µé¢å’Œå¤„ç†è¡¨å•æäº¤"""
    if request.method == 'POST':
        try:
            app.logger.info("æ”¶åˆ°åˆ›å»ºä»»åŠ¡è¯·æ±‚")
            
            # å¤„ç†è¡¨å•æ•°æ®
            task_name = request.form.get('task_name', '').strip()
            keywords = request.form.get('keywords', '').strip()
            target_accounts = request.form.get('target_accounts', '').strip()
            max_tweets = int(request.form.get('max_tweets', 100))
            min_likes = int(request.form.get('min_likes', 0))
            min_retweets = int(request.form.get('min_retweets', 0))
            min_comments = int(request.form.get('min_comments', 0))
            
            app.logger.info(f"ä»»åŠ¡å‚æ•°: name={task_name}, keywords={keywords}, accounts={target_accounts}, max_tweets={max_tweets}, min_likes={min_likes}, min_retweets={min_retweets}, min_comments={min_comments}")
            
            if not task_name:
                app.logger.warning("ä»»åŠ¡åç§°ä¸ºç©º")
                flash('ä»»åŠ¡åç§°ä¸èƒ½ä¸ºç©º', 'error')
                return redirect(url_for('index'))
            
            # éªŒè¯å…³é”®è¯å’Œç›®æ ‡è´¦å·è‡³å°‘å¡«å†™ä¸€ä¸ª
            if not keywords and not target_accounts:
                app.logger.warning("å…³é”®è¯å’Œç›®æ ‡è´¦å·éƒ½ä¸ºç©º")
                flash('å…³é”®è¯å’Œç›®æ ‡è´¦å·è‡³å°‘éœ€è¦å¡«å†™ä¸€ä¸ª', 'error')
                return redirect(url_for('index'))
            
            # è§£æå…³é”®è¯å’Œè´¦å·
            keywords_list = [k.strip() for k in keywords.split(',') if k.strip()]
            accounts_list = [a.strip() for a in target_accounts.split(',') if a.strip()] if target_accounts else []
            
            app.logger.info(f"è§£æåçš„å‚æ•°: keywords_list={keywords_list}, accounts_list={accounts_list}")
            
            # åˆ›å»ºä»»åŠ¡
            task = ScrapingTask(
                name=task_name,
                target_accounts=json.dumps(accounts_list),
                target_keywords=json.dumps(keywords_list),
                max_tweets=max_tweets,
                min_likes=min_likes,
                min_retweets=min_retweets,
                min_comments=min_comments
            )
            
            app.logger.info("æ­£åœ¨ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“")
            db.session.add(task)
            db.session.commit()
            app.logger.info(f"ä»»åŠ¡å·²ä¿å­˜ï¼ŒID: {task.id}")
            
            # è‡ªåŠ¨å¯åŠ¨ä»»åŠ¡
            app.logger.info("æ£€æŸ¥æ˜¯å¦å¯ä»¥å¯åŠ¨ä»»åŠ¡")
            if task_manager.can_start_task():
                app.logger.info(f"å°è¯•å¯åŠ¨ä»»åŠ¡ {task.id}")
                success, message = task_manager.start_task(task.id)
                if success:
                    app.logger.info(f"ä»»åŠ¡ {task.id} å¯åŠ¨æˆåŠŸ")
                    flash(f'ä»»åŠ¡ "{task_name}" åˆ›å»ºæˆåŠŸå¹¶å·²å¼€å§‹æ‰§è¡Œï¼', 'success')
                else:
                    app.logger.warning(f"ä»»åŠ¡ {task.id} å¯åŠ¨å¤±è´¥: {message}")
                    flash(f'ä»»åŠ¡ "{task_name}" åˆ›å»ºæˆåŠŸï¼Œä½†å¯åŠ¨å¤±è´¥: {message}', 'warning')
            else:
                status = task_manager.get_task_status()
                app.logger.info(f"æ— æ³•å¯åŠ¨ä»»åŠ¡ï¼Œå½“å‰çŠ¶æ€: {status}")
                flash(f'ä»»åŠ¡ "{task_name}" åˆ›å»ºæˆåŠŸï¼å½“å‰æœ‰ {status["running_count"]} ä¸ªä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œè¯·ç¨åæ‰‹åŠ¨å¯åŠ¨ã€‚', 'info')
            
            app.logger.info("é‡å®šå‘åˆ°ä»»åŠ¡é¡µé¢")
            return redirect(url_for('tasks'))
            
        except Exception as e:
            app.logger.error(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
            flash(f'åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}', 'error')
            return redirect(url_for('index'))
    
    return render_template('create_task.html')

@app.route('/data')
def data():
    """æ•°æ®æŸ¥çœ‹é¡µé¢"""
    from datetime import datetime, date
    from sqlalchemy import func
    
    page = request.args.get('page', 1, type=int)
    per_page = 20
    search = request.args.get('search', '')
    task_id = request.args.get('task_id', type=int)
    min_likes = request.args.get('min_likes', type=int)
    min_retweets = request.args.get('min_retweets', type=int)
    sort = request.args.get('sort', 'created_desc')
    
    # æ„å»ºæŸ¥è¯¢
    query = TweetData.query
    
    # æœç´¢è¿‡æ»¤
    if search:
        query = query.filter(
            db.or_(
                TweetData.content.contains(search),
                TweetData.username.contains(search)
            )
        )
    
    # ä»»åŠ¡è¿‡æ»¤
    if task_id:
        query = query.filter(TweetData.task_id == task_id)
    
    # ç‚¹èµæ•°è¿‡æ»¤
    if min_likes is not None:
        query = query.filter(TweetData.likes >= min_likes)
    
    # è½¬å‘æ•°è¿‡æ»¤
    if min_retweets is not None:
        query = query.filter(TweetData.retweets >= min_retweets)
    
    # æ’åº
    if sort == 'created_asc':
        query = query.order_by(TweetData.scraped_at.asc())
    elif sort == 'likes_desc':
        query = query.order_by(TweetData.likes.desc())
    elif sort == 'retweets_desc':
        query = query.filter(TweetData.retweets.isnot(None)).order_by(TweetData.retweets.desc())
    else:  # created_desc
        query = query.order_by(TweetData.scraped_at.desc())
    
    # åˆ†é¡µ
    tweets = query.paginate(
        page=page, per_page=per_page, error_out=False
    )
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    today = date.today()
    data_stats = {
        'total_tweets': TweetData.query.count(),
        'today_tweets': TweetData.query.filter(func.date(TweetData.scraped_at) == today).count(),
        'avg_likes': db.session.query(func.avg(TweetData.likes)).filter(TweetData.likes.isnot(None)).scalar() or 0,
        'avg_retweets': db.session.query(func.avg(TweetData.retweets)).filter(TweetData.retweets.isnot(None)).scalar() or 0
    }
    
    # æ ¼å¼åŒ–å¹³å‡æ•°
    data_stats['avg_likes'] = round(data_stats['avg_likes'], 1)
    data_stats['avg_retweets'] = round(data_stats['avg_retweets'], 1)
    
    # è·å–æ‰€æœ‰ä»»åŠ¡ç”¨äºç­›é€‰
    tasks = ScrapingTask.query.order_by(ScrapingTask.created_at.desc()).all()
    
    return render_template('data.html', 
                         tweets=tweets.items, 
                         pagination=tweets, 
                         data_stats=data_stats, 
                         tasks=tasks)

@app.route('/about')
def about():
    """å…³äºæˆ‘é¡µé¢"""
    return render_template('about.html')

@app.route('/config')
def config():
    """é…ç½®é¡µé¢"""
    # è·å–å½“å‰é…ç½®
    config_data = {}
    
    # ä»æ•°æ®åº“è·å–é…ç½®
    configs = SystemConfig.query.all()
    for cfg in configs:
        config_data[cfg.key] = cfg.value
    
    # å¤„ç†AdsPower APIåœ°å€çš„å‘åå…¼å®¹æ€§
    if 'adspower_api_url' in config_data and ('adspower_api_host' not in config_data or 'adspower_api_port' not in config_data):
        # ä»å®Œæ•´URLä¸­è§£æä¸»æœºå’Œç«¯å£
        api_url = config_data['adspower_api_url']
        if api_url.startswith('http://'):
            url_parts = api_url.replace('http://', '').split(':')
            if len(url_parts) == 2:
                config_data['adspower_api_host'] = url_parts[0]
                config_data['adspower_api_port'] = url_parts[1]
            else:
                config_data['adspower_api_host'] = 'localhost'
                config_data['adspower_api_port'] = '50325'
    
    # è®¾ç½®é»˜è®¤å€¼
    if 'adspower_api_host' not in config_data:
        config_data['adspower_api_host'] = 'localhost'
    if 'adspower_api_port' not in config_data:
        config_data['adspower_api_port'] = '50325'
    
    # å¤„ç†å¯¼å‡ºå­—æ®µé…ç½®
    if 'export_fields' in config_data:
        config_data['export_fields'] = config_data['export_fields'].split(',') if config_data['export_fields'] else []
    else:
        config_data['export_fields'] = ['content', 'username', 'created_at', 'likes_count', 'retweets_count', 'hashtags']
    
    return render_template('config.html', config=config_data)

@app.route('/update_config', methods=['POST'])
def update_config():
    """æ›´æ–°é…ç½®"""
    try:
        config_type = request.form.get('config_type')
        
        if config_type == 'adspower':
            # å¤„ç†AdsPoweré…ç½®
            api_host = request.form.get('adspower_api_host', 'local.adspower.net')
            api_port = request.form.get('adspower_api_port', '50325')
            api_url = f'http://{api_host}:{api_port}'
            
            adspower_configs = {
                'adspower_api_host': api_host,
                'adspower_api_port': api_port,
                'adspower_api_url': api_url,  # ä¿æŒå‘åå…¼å®¹
                'adspower_api_status': request.form.get('adspower_api_status', ''),
                'adspower_api_key': request.form.get('adspower_api_key', ''),
                'adspower_user_id': request.form.get('adspower_user_id', ''),
                'adspower_multi_user_ids': request.form.get('adspower_multi_user_ids', ''),
                'max_concurrent_tasks': request.form.get('max_concurrent_tasks', '2'),
                'task_timeout': request.form.get('task_timeout', '900'),
                'browser_startup_delay': request.form.get('browser_startup_delay', '2'),
                'request_interval': request.form.get('request_interval', '2.0'),
                'user_switch_interval': request.form.get('user_switch_interval', '30'),
                'adspower_headless': 'adspower_headless' in request.form,
                'adspower_health_check': 'adspower_health_check' in request.form,
                'user_rotation_enabled': 'user_rotation_enabled' in request.form
            }
            
            # æ›´æ–°æˆ–åˆ›å»ºé…ç½®è®°å½•
            for key, value in adspower_configs.items():
                config = SystemConfig.query.filter_by(key=key).first()
                if config:
                    config.value = str(value)
                    config.updated_at = datetime.utcnow()
                else:
                    config = SystemConfig(
                        key=key,
                        value=str(value),
                        description=f'AdsPoweré…ç½®: {key}'
                    )
                    db.session.add(config)
            
            # æ ¹æ®å¤šçª—å£ç”¨æˆ·IDåˆ—è¡¨è‡ªåŠ¨è®¡ç®—å¹¶å‘ä»»åŠ¡æ•°
            multi_user_ids_list = [uid.strip() for uid in adspower_configs['adspower_multi_user_ids'].split('\n') if uid.strip()]
            auto_concurrent_tasks = max(1, len(multi_user_ids_list))
            
            # æ›´æ–°å…¨å±€é…ç½®ï¼ˆç”¨äºå½“å‰ä¼šè¯ï¼‰
            global ADS_POWER_CONFIG
            ADS_POWER_CONFIG.update({
                'local_api_url': adspower_configs['adspower_api_url'],
                'user_id': adspower_configs['adspower_user_id'],
                'multi_user_ids': multi_user_ids_list,
                'max_concurrent_tasks': auto_concurrent_tasks,  # ä½¿ç”¨è‡ªåŠ¨è®¡ç®—çš„å€¼
                'task_timeout': int(adspower_configs['task_timeout']),
                'browser_startup_delay': float(adspower_configs['browser_startup_delay']),
                'request_interval': float(adspower_configs['request_interval']),
                'user_switch_interval': int(adspower_configs['user_switch_interval']),
                'user_rotation_enabled': adspower_configs['user_rotation_enabled'],
                'headless': adspower_configs['adspower_headless'],
                'health_check': adspower_configs['adspower_health_check']
            })
            
            # åŒæ—¶æ›´æ–°æ•°æ®åº“ä¸­çš„max_concurrent_taskså€¼
            max_concurrent_config = SystemConfig.query.filter_by(key='max_concurrent_tasks').first()
            if max_concurrent_config:
                max_concurrent_config.value = str(auto_concurrent_tasks)
                max_concurrent_config.updated_at = datetime.utcnow()
            else:
                max_concurrent_config = SystemConfig(
                    key='max_concurrent_tasks',
                    value=str(auto_concurrent_tasks),
                    description='AdsPoweré…ç½®: max_concurrent_tasks'
                )
                db.session.add(max_concurrent_config)
            
            # æ›´æ–°ä»»åŠ¡ç®¡ç†å™¨çš„é…ç½®
            if hasattr(task_manager, 'max_concurrent_tasks'):
                task_manager.max_concurrent_tasks = auto_concurrent_tasks  # ä½¿ç”¨è‡ªåŠ¨è®¡ç®—çš„å€¼
            if hasattr(task_manager, 'user_id_pool'):
                task_manager.user_id_pool = multi_user_ids_list
            if hasattr(task_manager, 'request_interval'):
                task_manager.request_interval = float(adspower_configs['request_interval'])
            if hasattr(task_manager, 'user_switch_interval'):
                task_manager.user_switch_interval = int(adspower_configs['user_switch_interval'])
            if hasattr(task_manager, 'user_rotation_enabled'):
                task_manager.user_rotation_enabled = adspower_configs['user_rotation_enabled']
            
            db.session.commit()
            flash('AdsPoweré…ç½®å·²æ›´æ–°', 'success')
            
        elif config_type == 'general':
            # å¤„ç†åŸºç¡€è®¾ç½®
            general_configs = {
                'system_name': request.form.get('system_name', 'TwitteræŠ“å–ç®¡ç†ç³»ç»Ÿ'),
                'admin_email': request.form.get('admin_email', ''),
                'data_retention_days': request.form.get('data_retention_days', '30'),
                'auto_backup': 'auto_backup' in request.form
            }
            
            for key, value in general_configs.items():
                config = SystemConfig.query.filter_by(key=key).first()
                if config:
                    config.value = str(value)
                    config.updated_at = datetime.utcnow()
                else:
                    config = SystemConfig(
                        key=key,
                        value=str(value),
                        description=f'åŸºç¡€è®¾ç½®: {key}'
                    )
                    db.session.add(config)
            
            db.session.commit()
            flash('åŸºç¡€è®¾ç½®å·²æ›´æ–°', 'success')
            
        elif config_type == 'scraping':
            # å¤„ç†æŠ“å–é…ç½®
            scraping_configs = {
                'default_max_tweets': request.form.get('default_max_tweets', '100'),
                'request_delay': request.form.get('request_delay', '2'),
                'browser_timeout': request.form.get('browser_timeout', '30'),
                'retry_attempts': request.form.get('retry_attempts', '3'),
                'user_agents': request.form.get('user_agents', ''),
                'enable_proxy': 'enable_proxy' in request.form
            }
            
            for key, value in scraping_configs.items():
                config = SystemConfig.query.filter_by(key=key).first()
                if config:
                    config.value = str(value)
                    config.updated_at = datetime.utcnow()
                else:
                    config = SystemConfig(
                        key=key,
                        value=str(value),
                        description=f'æŠ“å–é…ç½®: {key}'
                    )
                    db.session.add(config)
            
            db.session.commit()
            flash('æŠ“å–é…ç½®å·²æ›´æ–°', 'success')
            
        elif config_type == 'feishu':
            # å¤„ç†é£ä¹¦é…ç½®
            feishu_configs = {
                'feishu_app_id': request.form.get('feishu_app_id', ''),
                'feishu_app_secret': request.form.get('feishu_app_secret', ''),
                'feishu_spreadsheet_token': request.form.get('feishu_spreadsheet_token', ''),
                'feishu_table_id': request.form.get('feishu_table_id', ''),
                'feishu_enabled': 'feishu_enabled' in request.form,
                'feishu_auto_sync': 'feishu_auto_sync' in request.form,
                'sync_interval': request.form.get('sync_interval', '24')
            }
            
            for key, value in feishu_configs.items():
                config = SystemConfig.query.filter_by(key=key).first()
                if config:
                    config.value = str(value)
                    config.updated_at = datetime.utcnow()
                else:
                    config = SystemConfig(
                        key=key,
                        value=str(value),
                        description=f'é£ä¹¦é…ç½®: {key}'
                    )
                    db.session.add(config)
            
            # æ›´æ–°å…¨å±€é£ä¹¦é…ç½®
            global FEISHU_CONFIG
            FEISHU_CONFIG.update({
                'app_id': feishu_configs['feishu_app_id'],
                'app_secret': feishu_configs['feishu_app_secret'],
                'spreadsheet_token': feishu_configs['feishu_spreadsheet_token'],
                'table_id': feishu_configs['feishu_table_id'],
                'enabled': feishu_configs['feishu_enabled']
            })
            
            db.session.commit()
            flash('é£ä¹¦é…ç½®å·²æ›´æ–°', 'success')
            
        elif config_type == 'export':
            # å¤„ç†å¯¼å‡ºè®¾ç½®
            export_configs = {
                'export_excel': 'export_excel' in request.form,
                'export_csv': 'export_csv' in request.form,
                'export_json': 'export_json' in request.form,
                'export_fields': ','.join(request.form.getlist('export_fields')),
                'export_filename_template': request.form.get('export_filename_template', 'twitter_data_{date}')
            }
            
            for key, value in export_configs.items():
                config = SystemConfig.query.filter_by(key=key).first()
                if config:
                    config.value = str(value)
                    config.updated_at = datetime.utcnow()
                else:
                    config = SystemConfig(
                        key=key,
                        value=str(value),
                        description=f'å¯¼å‡ºè®¾ç½®: {key}'
                    )
                    db.session.add(config)
            
            db.session.commit()
            flash('å¯¼å‡ºè®¾ç½®å·²æ›´æ–°', 'success')
        
        return redirect(url_for('config'))
        
    except Exception as e:
        flash(f'é…ç½®æ›´æ–°å¤±è´¥: {str(e)}', 'error')
        return redirect(url_for('config'))

@app.route('/influencers')
def influencers():
    """åšä¸»ç®¡ç†é¡µé¢"""
    return render_template('influencers.html')

@app.route('/sync_feishu', methods=['POST'])
def sync_feishu():
    """åŒæ­¥æ•°æ®åˆ°é£ä¹¦ï¼ˆæ”¯æŒå…¨éƒ¨åŒæ­¥æˆ–æŒ‰ä»»åŠ¡IDåŒæ­¥ï¼‰"""
    print("\n" + "="*60)
    print("ğŸš€ [åç«¯] å¼€å§‹å¤„ç†é£ä¹¦åŒæ­¥è¯·æ±‚")
    try:
        # è·å–è¯·æ±‚å‚æ•°
        data = request.form.to_dict()
        task_id = data.get('task_id')
        print(f"ğŸ“‹ [åç«¯] æ¥æ”¶åˆ°è¯·æ±‚å‚æ•°: {data}")
        print(f"ğŸ“‹ [åç«¯] ä»»åŠ¡ID: {task_id}")
        
        # æ£€æŸ¥é£ä¹¦é…ç½®
        print(f"ğŸ”§ [åç«¯] æ£€æŸ¥é£ä¹¦é…ç½®çŠ¶æ€")
        print(f"   - é£ä¹¦å¯ç”¨çŠ¶æ€: {FEISHU_CONFIG.get('enabled')}")
        if not FEISHU_CONFIG.get('enabled'):
            print("âŒ [åç«¯] é£ä¹¦åŒæ­¥æœªå¯ç”¨")
            return jsonify({'success': False, 'message': 'é£ä¹¦åŒæ­¥æœªå¯ç”¨'}), 400
        
        required_fields = ['app_id', 'app_secret', 'spreadsheet_token', 'table_id']
        missing_fields = [field for field in required_fields if not FEISHU_CONFIG.get(field)]
        print(f"ğŸ”§ [åç«¯] æ£€æŸ¥å¿…éœ€é…ç½®å­—æ®µ: {required_fields}")
        print(f"ğŸ”§ [åç«¯] ç¼ºå°‘çš„é…ç½®å­—æ®µ: {missing_fields}")
        if missing_fields:
            print(f"âŒ [åç«¯] é£ä¹¦é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {missing_fields}")
            return jsonify({'success': False, 'message': f'é£ä¹¦é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {", ".join(missing_fields)}'}), 400
        
        print(f"âœ… [åç«¯] é£ä¹¦é…ç½®æ£€æŸ¥é€šè¿‡")
        
        # æ„å»ºæŸ¥è¯¢
        print(f"ğŸ” [åç«¯] æ„å»ºæ•°æ®åº“æŸ¥è¯¢")
        query = TweetData.query
        if task_id:
            query = query.filter(TweetData.task_id == task_id)
            print(f"   - æŒ‰ä»»åŠ¡IDè¿‡æ»¤: {task_id}")
        else:
            print(f"   - æŸ¥è¯¢æ‰€æœ‰ä»»åŠ¡çš„æ•°æ®")
        
        # è·å–æ‰€æœ‰ç›¸å…³æ¨æ–‡æ•°æ®ï¼ˆåŒ…æ‹¬å·²åŒæ­¥å’ŒæœªåŒæ­¥çš„ï¼‰
        print(f"ğŸ“Š [åç«¯] æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢")
        all_tweets = query.all()
        print(f"ğŸ“Š [åç«¯] æŸ¥è¯¢åˆ°æ€»æ¨æ–‡æ•°: {len(all_tweets)}")
        
        # åˆ†åˆ«ç»Ÿè®¡å·²åŒæ­¥å’ŒæœªåŒæ­¥çš„æ•°æ®
        synced_tweets = [t for t in all_tweets if t.synced_to_feishu]
        unsynced_tweets = [t for t in all_tweets if not t.synced_to_feishu]
        print(f"ğŸ“Š [åç«¯] æ•°æ®ç»Ÿè®¡:")
        print(f"   - å·²åŒæ­¥æ¨æ–‡æ•°: {len(synced_tweets)}")
        print(f"   - æœªåŒæ­¥æ¨æ–‡æ•°: {len(unsynced_tweets)}")
        
        # æ£€æŸ¥é‡å¤å†…å®¹ï¼ˆåŸºäºæ¨æ–‡å†…å®¹å’Œé“¾æ¥ï¼‰
        print(f"ğŸ” [åç«¯] å¼€å§‹æ£€æŸ¥é‡å¤å†…å®¹")
        duplicate_check = {}
        potential_duplicates = []
        
        for tweet in unsynced_tweets:
            # åˆ›å»ºå†…å®¹æŒ‡çº¹ï¼ˆåŸºäºå†…å®¹å‰100å­—ç¬¦å’Œé“¾æ¥ï¼‰
            content_fingerprint = (tweet.content[:100] if tweet.content else '') + (tweet.link or '')
            if content_fingerprint in duplicate_check:
                potential_duplicates.append({
                    'current': tweet,
                    'existing': duplicate_check[content_fingerprint]
                })
            else:
                duplicate_check[content_fingerprint] = tweet
        
        print(f"ğŸ” [åç«¯] é‡å¤å†…å®¹æ£€æŸ¥å®Œæˆï¼Œå‘ç° {len(potential_duplicates)} ç»„æ½œåœ¨é‡å¤")
        
        # æ„å»ºè¯¦ç»†çš„åŒæ­¥æŠ¥å‘Š
        sync_report = {
            'total_tweets': len(all_tweets),
            'already_synced': len(synced_tweets),
            'to_sync': len(unsynced_tweets),
            'potential_duplicates': len(potential_duplicates)
        }
        print(f"ğŸ“Š [åç«¯] åŒæ­¥æŠ¥å‘Š: {sync_report}")
        
        if not unsynced_tweets:
            message = f'å†…å®¹å·²ç»åŒæ­¥è¿‡äº†ï¼Œä¸ç”¨å†åŒæ­¥äº†ï¼'
            if task_id:
                message += f'ä»»åŠ¡ {task_id} çš„æ‰€æœ‰æ•°æ®ï¼ˆ{len(all_tweets)} æ¡ï¼‰éƒ½å·²åœ¨é£ä¹¦ä¸­'
            else:
                message += f'æ‰€æœ‰æ•°æ®ï¼ˆ{len(all_tweets)} æ¡ï¼‰éƒ½å·²åœ¨é£ä¹¦ä¸­'
            print(f"â„¹ï¸ [åç«¯] æ— æ–°æ•°æ®éœ€è¦åŒæ­¥: {message}")
            return jsonify({
                'success': True, 
                'message': message,
                'report': sync_report
            })
        
        # å¦‚æœå‘ç°æ½œåœ¨é‡å¤å†…å®¹ï¼Œè®°å½•ä½†ç»§ç»­åŒæ­¥
        if potential_duplicates:
            print(f"âš ï¸ [åç«¯] å‘ç° {len(potential_duplicates)} ç»„æ½œåœ¨é‡å¤å†…å®¹ï¼Œä½†å°†ç»§ç»­åŒæ­¥")
            for dup in potential_duplicates[:3]:  # åªæ‰“å°å‰3ä¸ª
                print(f"   - é‡å¤å†…å®¹: {dup['current'].content[:50]}...")
        
        # åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨
        print(f"ğŸ”§ [åç«¯] åˆå§‹åŒ–äº‘åŒæ­¥ç®¡ç†å™¨")
        sync_config = {
            'feishu': {
                'enabled': True,
                'app_id': FEISHU_CONFIG['app_id'],
                'app_secret': FEISHU_CONFIG['app_secret'],
                'spreadsheet_token': FEISHU_CONFIG['spreadsheet_token'],
                'table_id': FEISHU_CONFIG['table_id'],
                'base_url': 'https://open.feishu.cn/open-apis'
            }
        }
        print(f"ğŸ”§ [åç«¯] åŒæ­¥é…ç½®: {sync_config}")
        sync_manager = CloudSyncManager(sync_config)
        print(f"âœ… [åç«¯] äº‘åŒæ­¥ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # å‡†å¤‡æ•°æ®ï¼ŒæŒ‰ç…§é£ä¹¦å¤šç»´è¡¨æ ¼å­—æ®µæ˜ å°„
        print(f"ğŸ”„ [åç«¯] å¼€å§‹å‡†å¤‡åŒæ­¥æ•°æ®")
        sync_data = []
        for idx, tweet in enumerate(unsynced_tweets):
            print(f"ğŸ“ [åç«¯] å¤„ç†ç¬¬ {idx + 1}/{len(unsynced_tweets)} æ¡æ¨æ–‡")
            # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„ç±»å‹æ ‡ç­¾ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨è‡ªåŠ¨åˆ†ç±»
            content_type = tweet.content_type or classify_content_type(tweet.content)
            print(f"   - æ¨æ–‡ID: {tweet.id}")
            print(f"   - å†…å®¹ç±»å‹: {content_type}")
            
            # å¤„ç†å‘å¸ƒæ—¶é—´
            print(f"   - ğŸ• å¼€å§‹å¤„ç†å‘å¸ƒæ—¶é—´")
            print(f"     - åŸå§‹å‘å¸ƒæ—¶é—´: {tweet.publish_time} (ç±»å‹: {type(tweet.publish_time)})")
            
            publish_time = ''
            if tweet.publish_time:
                try:
                    if isinstance(tweet.publish_time, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºdatetime
                        print(f"     - å‘å¸ƒæ—¶é—´ä¸ºå­—ç¬¦ä¸²ï¼Œå¼€å§‹è§£æ")
                        from dateutil import parser
                        dt = parser.parse(tweet.publish_time)
                        publish_time = int(dt.timestamp())  # ä½¿ç”¨ç§’çº§æ—¶é—´æˆ³ï¼Œä¸ä¹˜ä»¥1000
                        print(f"     - å­—ç¬¦ä¸²è§£ææˆåŠŸ: {publish_time} ({dt})")
                    else:
                        # å¦‚æœå·²ç»æ˜¯datetimeå¯¹è±¡
                        print(f"     - å‘å¸ƒæ—¶é—´ä¸ºdatetimeå¯¹è±¡")
                        publish_time = int(tweet.publish_time.timestamp())  # ä½¿ç”¨ç§’çº§æ—¶é—´æˆ³ï¼Œä¸ä¹˜ä»¥1000
                        print(f"     - datetimeè½¬æ¢æˆåŠŸ: {publish_time} ({tweet.publish_time})")
                    
                    # éªŒè¯æ—¶é—´æˆ³åˆç†æ€§
                    if publish_time < 946684800:  # 2000å¹´1æœˆ1æ—¥çš„æ—¶é—´æˆ³
                        print(f"     - âš ï¸ å‘å¸ƒæ—¶é—´æˆ³å¼‚å¸¸ ({publish_time})ï¼Œå¯èƒ½æ˜¯1970å¹´é—®é¢˜")
                        publish_time = int(datetime.now().timestamp())
                        print(f"     - ä¿®æ­£ä¸ºå½“å‰æ—¶é—´æˆ³: {publish_time}")
                    
                    print(f"   - âœ… æœ€ç»ˆå‘å¸ƒæ—¶é—´: {publish_time} ({datetime.fromtimestamp(publish_time)})")
                except Exception as e:
                    print(f"   - âŒ å‘å¸ƒæ—¶é—´è§£æå¤±è´¥: {e}")
                    publish_time = int(datetime.now().timestamp())
                    print(f"   - ä½¿ç”¨å½“å‰æ—¶é—´æˆ³: {publish_time}")
            else:
                print(f"     - å‘å¸ƒæ—¶é—´ä¸ºç©ºï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                publish_time = int(datetime.now().timestamp())
                print(f"   - é»˜è®¤å‘å¸ƒæ—¶é—´: {publish_time}")
            
            # å¤„ç†åˆ›å»ºæ—¶é—´
            print(f"   - ğŸ• å¼€å§‹å¤„ç†åˆ›å»ºæ—¶é—´")
            print(f"     - åŸå§‹åˆ›å»ºæ—¶é—´: {tweet.scraped_at} (ç±»å‹: {type(tweet.scraped_at)})")
            
            if tweet.scraped_at:
                create_time = int(tweet.scraped_at.timestamp())
                print(f"     - åˆ›å»ºæ—¶é—´è½¬æ¢æˆåŠŸ: {create_time} ({tweet.scraped_at})")
            else:
                create_time = int(datetime.now().timestamp())
                print(f"     - åˆ›å»ºæ—¶é—´ä¸ºç©ºï¼Œä½¿ç”¨å½“å‰æ—¶é—´: {create_time}")
            
            # éªŒè¯åˆ›å»ºæ—¶é—´æˆ³åˆç†æ€§
            if create_time < 946684800:  # 2000å¹´1æœˆ1æ—¥çš„æ—¶é—´æˆ³
                print(f"     - âš ï¸ åˆ›å»ºæ—¶é—´æˆ³å¼‚å¸¸ ({create_time})ï¼Œå¯èƒ½æ˜¯1970å¹´é—®é¢˜")
                create_time = int(datetime.now().timestamp())
                print(f"     - ä¿®æ­£ä¸ºå½“å‰æ—¶é—´æˆ³: {create_time}")
            
            print(f"   - âœ… æœ€ç»ˆåˆ›å»ºæ—¶é—´: {create_time} ({datetime.fromtimestamp(create_time)})")
            
            tweet_data = {
                'æ¨æ–‡åŸæ–‡å†…å®¹': tweet.content,
                'å‘å¸ƒæ—¶é—´': publish_time,
                'ä½œè€…ï¼ˆè´¦å·ï¼‰': tweet.username,
                'æ¨æ–‡é“¾æ¥': tweet.link or '',
                'è¯é¢˜æ ‡ç­¾ï¼ˆHashtagï¼‰': ', '.join(json.loads(tweet.hashtags) if tweet.hashtags else []),
                'ç±»å‹æ ‡ç­¾': content_type,
                'è¯„è®º': 0,  # Twitter APIé™åˆ¶ï¼Œæš‚æ—¶è®¾ä¸º0
                'ç‚¹èµ': tweet.likes,
                'è½¬å‘': tweet.retweets,
                'åˆ›å»ºæ—¶é—´': create_time
            }
            sync_data.append(tweet_data)
            print(f"   - æ•°æ®å­—æ®µæ•°: {len(tweet_data)}")
            print(f"   - æ•°æ®å†…å®¹é¢„è§ˆ: {str(tweet_data)[:200]}...")
        
        print(f"âœ… [åç«¯] æ•°æ®å‡†å¤‡å®Œæˆï¼Œå…± {len(sync_data)} æ¡è®°å½•")
        
        # æ˜¾ç¤ºå‰3æ¡æ•°æ®çš„è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
        print(f"ğŸ“‹ [åç«¯] å‡†å¤‡åŒæ­¥çš„æ•°æ®ç¤ºä¾‹:")
        for i, item in enumerate(sync_data[:3]):
            print(f"   - ç¬¬{i+1}æ¡æ•°æ®:")
            for key, value in item.items():
                if key in ['å‘å¸ƒæ—¶é—´', 'åˆ›å»ºæ—¶é—´']:
                    if isinstance(value, (int, float)) and value > 0:
                        readable_time = datetime.fromtimestamp(value)
                        print(f"     - {key}: {value} ({readable_time})")
                    else:
                        print(f"     - {key}: {value} (æ— æ•ˆæ—¶é—´æˆ³)")
                else:
                    print(f"     - {key}: {str(value)[:50]}..." if len(str(value)) > 50 else f"     - {key}: {value}")
        
        # åŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼
        print(f"ğŸš€ [åç«¯] å¼€å§‹æ‰§è¡Œé£ä¹¦åŒæ­¥")
        print(f"   - è¡¨æ ¼Token: {FEISHU_CONFIG['spreadsheet_token'][:10]}...")
        print(f"   - è¡¨æ ¼ID: {FEISHU_CONFIG['table_id']}")
        success = sync_manager.sync_to_feishu(
            sync_data,
            FEISHU_CONFIG['spreadsheet_token'],
            FEISHU_CONFIG['table_id']
        )
        print(f"ğŸ“Š [åç«¯] é£ä¹¦åŒæ­¥ç»“æœ: {success}")
        
        if success:
            print(f"âœ… [åç«¯] åŒæ­¥æˆåŠŸï¼Œæ›´æ–°æ•°æ®åº“çŠ¶æ€")
            # æ›´æ–°åŒæ­¥çŠ¶æ€
            for tweet in unsynced_tweets:
                tweet.synced_to_feishu = True
            db.session.commit()
            print(f"âœ… [åç«¯] æ•°æ®åº“çŠ¶æ€æ›´æ–°å®Œæˆ")
            
            # æ„å»ºè¯¦ç»†çš„æˆåŠŸæ¶ˆæ¯
            message = f'æˆåŠŸåŒæ­¥ {len(unsynced_tweets)} æ¡æ–°æ•°æ®åˆ°é£ä¹¦'
            if task_id:
                message += f'ï¼ˆä»»åŠ¡ {task_id}ï¼‰'
            
            if synced_tweets:
                message += f'ï¼Œå¦æœ‰ {len(synced_tweets)} æ¡æ•°æ®ä¹‹å‰å·²åŒæ­¥'
            
            if potential_duplicates:
                message += f'\næ³¨æ„ï¼šæ£€æµ‹åˆ° {len(potential_duplicates)} ç»„æ½œåœ¨é‡å¤å†…å®¹ï¼Œå·²ä¸€å¹¶åŒæ­¥'
            
            sync_report['synced_count'] = len(unsynced_tweets)
            print(f"ğŸ‰ [åç«¯] åŒæ­¥å®Œæˆï¼Œè¿”å›æˆåŠŸå“åº”: {message}")
            
            return jsonify({
                'success': True, 
                'message': message,
                'report': sync_report
            })
        else:
            print(f"âŒ [åç«¯] åŒæ­¥å¤±è´¥ï¼Œè¿”å›é”™è¯¯å“åº”")
            return jsonify({'success': False, 'message': 'åŒæ­¥åˆ°é£ä¹¦å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé£ä¹¦é…ç½®'}), 500
            
    except Exception as e:
        print(f"âŒ [åç«¯] é£ä¹¦åŒæ­¥è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸")
        print(f"   - å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        print(f"   - å¼‚å¸¸æ¶ˆæ¯: {str(e)}")
        db.session.rollback()
        print(f"ğŸ”„ [åç«¯] æ•°æ®åº“å›æ»šå®Œæˆ")
        import traceback
        error_details = traceback.format_exc()
        print(f"ğŸ“Š [åç«¯] å¼‚å¸¸è¯¦æƒ…: {error_details}")
        print(f"âŒ [åç«¯] è¿”å›é”™è¯¯å“åº”: åŒæ­¥å¤±è´¥: {str(e)}")
        print("="*60 + "\n")
        return jsonify({'success': False, 'message': f'åŒæ­¥å¤±è´¥: {str(e)}'}), 500

# APIè·¯ç”±
@app.route('/api/tasks', methods=['GET'])
def api_get_tasks():
    """è·å–ä»»åŠ¡åˆ—è¡¨"""
    tasks = ScrapingTask.query.order_by(ScrapingTask.created_at.desc()).all()
    return jsonify([task.to_dict() for task in tasks])

@app.route('/api/tasks', methods=['POST'])
def api_create_task():
    """åˆ›å»ºæ–°ä»»åŠ¡"""
    try:
        data = request.get_json()
        
        # éªŒè¯ä»»åŠ¡åç§°
        task_name = data.get('name', '').strip()
        if not task_name:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡åç§°ä¸èƒ½ä¸ºç©º'}), 400
        
        # éªŒè¯å…³é”®è¯å’Œç›®æ ‡è´¦å·è‡³å°‘å¡«å†™ä¸€ä¸ª
        target_keywords = data.get('target_keywords', [])
        target_accounts = data.get('target_accounts', [])
        
        if not target_keywords and not target_accounts:
            return jsonify({'success': False, 'error': 'å…³é”®è¯å’Œç›®æ ‡è´¦å·è‡³å°‘éœ€è¦å¡«å†™ä¸€ä¸ª'}), 400
        
        task = ScrapingTask(
            name=task_name,
            target_accounts=json.dumps(target_accounts),
            target_keywords=json.dumps(target_keywords),
            max_tweets=data.get('max_tweets', 50),
            min_likes=data.get('min_likes', 0),
            min_retweets=data.get('min_retweets', 0),
            min_comments=data.get('min_comments', 0)
        )
        
        db.session.add(task)
        db.session.commit()
        
        return jsonify({'success': True, 'task_id': task.id})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 400

@app.route('/api/tasks/<int:task_id>/start', methods=['POST'])
def api_start_task(task_id):
    """å¯åŠ¨ä»»åŠ¡"""
    app.logger.info(f"æ”¶åˆ°å¯åŠ¨ä»»åŠ¡è¯·æ±‚: task_id={task_id}")
    
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²åœ¨è¿è¡Œ
    if task_manager.is_task_running(task_id):
        error_msg = 'è¯¥ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­'
        app.logger.warning(f"ä»»åŠ¡å¯åŠ¨å¤±è´¥ - {error_msg}: task_id={task_id}")
        return jsonify({'success': False, 'error': error_msg}), 400
    
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¯åŠ¨æ–°ä»»åŠ¡
    if not task_manager.can_start_task():
        status = task_manager.get_task_status()
        # æä¾›è¯¦ç»†çš„å¤±è´¥åŸå› 
        error_details = {
            'running_count': status['running_count'],
            'max_concurrent': status['max_concurrent'],
            'available_slots': status['available_slots'],
            'available_browsers': status['available_browsers'],
            'current_tasks': status.get('current_tasks', []),
            'user_id_pool': len(task_manager.user_id_pool),
            'available_user_ids': task_manager.available_user_ids
        }
        
        # å½“è¾¾åˆ°å¹¶å‘é™åˆ¶æ—¶ï¼Œè‡ªåŠ¨å°†ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
        if status['running_count'] >= status['max_concurrent']:
            app.logger.info(f"ä»»åŠ¡ {task_id} å°†åŠ å…¥é˜Ÿåˆ—ç­‰å¾…æ‰§è¡Œ")
            try:
                success, message = task_manager.start_task(task_id)  # è¿™ä¼šè‡ªåŠ¨åŠ å…¥é˜Ÿåˆ—
                if success:
                    return jsonify({
                        'success': True, 
                        'message': f'ä»»åŠ¡å·²åŠ å…¥é˜Ÿåˆ—ï¼Œ{message}',
                        'queued': True
                    })
                else:
                    return jsonify({
                        'success': False, 
                        'error': f'åŠ å…¥é˜Ÿåˆ—å¤±è´¥: {message}'
                    }), 400
            except Exception as e:
                app.logger.error(f"åŠ å…¥é˜Ÿåˆ—å¤±è´¥: {str(e)}")
                return jsonify({
                    'success': False, 
                    'error': f'åŠ å…¥é˜Ÿåˆ—å¤±è´¥: {str(e)}'
                }), 500
        elif len(task_manager.user_id_pool) == 0:
            error_msg = f'æ— å¯ç”¨çš„æµè§ˆå™¨ç”¨æˆ·IDï¼Œå¯ç”¨ç”¨æˆ·IDæ± ä¸ºç©ºã€‚é…ç½®çš„ç”¨æˆ·ID: {task_manager.available_user_ids}'
        else:
            error_msg = f'æ— æ³•å¯åŠ¨ä»»åŠ¡ï¼ŒåŸå› æœªçŸ¥ã€‚è¿è¡Œä»»åŠ¡æ•°: {status["running_count"]}/{status["max_concurrent"]}ï¼Œå¯ç”¨æµè§ˆå™¨: {len(task_manager.user_id_pool)}'
        
        app.logger.error(f"ä»»åŠ¡å¯åŠ¨å¤±è´¥ - {error_msg}: task_id={task_id}, details={error_details}")
        
        return jsonify({
            'success': False, 
            'error': error_msg,
            'details': error_details
        }), 400
    
    try:
        app.logger.info(f"å¼€å§‹å¯åŠ¨ä»»åŠ¡: task_id={task_id}")
        success, message = task_manager.start_task(task_id)
        if success:
            app.logger.info(f"ä»»åŠ¡å¯åŠ¨æˆåŠŸ: task_id={task_id}, message={message}")
            return jsonify({'success': True, 'message': message})
        else:
            # è·å–æ›´è¯¦ç»†çš„çŠ¶æ€ä¿¡æ¯
            status = task_manager.get_task_status()
            error_details = {
                'running_count': status['running_count'],
                'max_concurrent': status['max_concurrent'],
                'available_slots': status['available_slots'],
                'available_browsers': status['available_browsers'],
                'user_id_pool': len(task_manager.user_id_pool),
                'available_user_ids': task_manager.available_user_ids,
                'original_message': message
            }
            app.logger.error(f"ä»»åŠ¡å¯åŠ¨å¤±è´¥: task_id={task_id}, message={message}, details={error_details}")
            return jsonify({
                'success': False, 
                'error': f'ä»»åŠ¡å¯åŠ¨å¤±è´¥: {message}',
                'details': error_details
            }), 400
        
    except Exception as e:
        # è·å–è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯
        import traceback
        error_details = {
            'exception_type': type(e).__name__,
            'exception_message': str(e),
            'traceback': traceback.format_exc()
        }
        app.logger.error(f"ä»»åŠ¡å¯åŠ¨å¼‚å¸¸: task_id={task_id}, exception={str(e)}, traceback={traceback.format_exc()}")
        return jsonify({
            'success': False, 
            'error': f'ä»»åŠ¡å¯åŠ¨å¼‚å¸¸: {str(e)}',
            'details': error_details
        }), 500

@app.route('/api/tasks/<int:task_id>/stop', methods=['POST'])
def api_stop_task(task_id):
    """åœæ­¢ä»»åŠ¡"""
    try:
        success, message = task_manager.stop_task(task_id)
        if success:
            return jsonify({'success': True, 'message': message})
        else:
            return jsonify({'success': False, 'error': message}), 400
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/queue/status')
def api_queue_status():
    """è·å–ä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€"""
    try:
        queue_status = task_manager.get_queue_status()
        
        return jsonify({
            'success': True,
            'data': queue_status
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/queue/clear', methods=['POST'])
def api_clear_queue():
    """æ¸…ç©ºä»»åŠ¡é˜Ÿåˆ—"""
    try:
        task_manager.clear_queue()
        return jsonify({
            'success': True,
            'message': 'ä»»åŠ¡é˜Ÿåˆ—å·²æ¸…ç©º'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/task-manager/status')
def api_task_manager_status():
    """è·å–ä»»åŠ¡ç®¡ç†å™¨çŠ¶æ€"""
    try:
        status = task_manager.get_status()
        return jsonify({
            'success': True,
            'data': status
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks/<int:task_id>/restart', methods=['POST'])
def api_restart_task(task_id):
    """é‡æ–°å¯åŠ¨ä»»åŠ¡"""
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²åœ¨è¿è¡Œ
    if task_manager.is_task_running(task_id):
        return jsonify({'success': False, 'error': 'è¯¥ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­ï¼Œè¯·å…ˆåœæ­¢'}), 400
    
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¯åŠ¨æ–°ä»»åŠ¡
    if not task_manager.can_start_task():
        status = task_manager.get_task_status()
        # æä¾›è¯¦ç»†çš„å¤±è´¥åŸå› 
        error_details = {
            'running_count': status['running_count'],
            'max_concurrent': status['max_concurrent'],
            'available_slots': status['available_slots'],
            'available_browsers': status['available_browsers'],
            'current_tasks': status.get('current_tasks', []),
            'user_id_pool': len(task_manager.user_id_pool),
            'available_user_ids': task_manager.available_user_ids
        }
        
        if status['running_count'] >= status['max_concurrent']:
            error_msg = f'å·²è¾¾åˆ°æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°é™åˆ¶({status["max_concurrent"]})ï¼Œå½“å‰è¿è¡Œä»»åŠ¡æ•°: {status["running_count"]}'
        elif len(task_manager.user_id_pool) == 0:
            error_msg = f'æ— å¯ç”¨çš„æµè§ˆå™¨ç”¨æˆ·IDï¼Œå¯ç”¨ç”¨æˆ·IDæ± ä¸ºç©ºã€‚é…ç½®çš„ç”¨æˆ·ID: {task_manager.available_user_ids}'
        else:
            error_msg = f'æ— æ³•é‡æ–°å¯åŠ¨ä»»åŠ¡ï¼ŒåŸå› æœªçŸ¥ã€‚è¿è¡Œä»»åŠ¡æ•°: {status["running_count"]}/{status["max_concurrent"]}ï¼Œå¯ç”¨æµè§ˆå™¨: {len(task_manager.user_id_pool)}'
        
        return jsonify({
            'success': False, 
            'error': error_msg,
            'details': error_details
        }), 400
    
    try:
        # è·å–ä»»åŠ¡
        task = ScrapingTask.query.get_or_404(task_id)
        
        # é‡ç½®ä»»åŠ¡çŠ¶æ€
        task.status = 'pending'
        task.result_count = 0
        task.started_at = None
        task.completed_at = None
        task.error_message = None
        
        # åˆ é™¤è¯¥ä»»åŠ¡ä¹‹å‰æŠ“å–çš„æ•°æ®ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€æ±‚å†³å®šï¼‰
        # TweetData.query.filter_by(task_id=task_id).delete()
        
        db.session.commit()
        
        # å¯åŠ¨ä»»åŠ¡
        success, message = task_manager.start_task(task_id)
        if success:
            return jsonify({'success': True, 'message': 'ä»»åŠ¡å·²é‡æ–°å¯åŠ¨'})
        else:
            # è·å–æ›´è¯¦ç»†çš„çŠ¶æ€ä¿¡æ¯
            status = task_manager.get_task_status()
            error_details = {
                'running_count': status['running_count'],
                'max_concurrent': status['max_concurrent'],
                'available_slots': status['available_slots'],
                'available_browsers': status['available_browsers'],
                'user_id_pool': len(task_manager.user_id_pool),
                'available_user_ids': task_manager.available_user_ids,
                'original_message': message
            }
            return jsonify({
                'success': False, 
                'error': f'é‡æ–°å¯åŠ¨å¤±è´¥: {message}',
                'details': error_details
            }), 400
        
    except Exception as e:
        # è·å–è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯
        import traceback
        error_details = {
            'exception_type': type(e).__name__,
            'exception_message': str(e),
            'traceback': traceback.format_exc()
        }
        return jsonify({
            'success': False, 
            'error': f'é‡æ–°å¯åŠ¨å¼‚å¸¸: {str(e)}',
            'details': error_details
        }), 500

@app.route('/api/tasks/<int:task_id>', methods=['GET'])
def api_get_task(task_id):
    """è·å–å•ä¸ªä»»åŠ¡è¯¦æƒ…"""
    try:
        task = ScrapingTask.query.get_or_404(task_id)
        
        # è·å–ä»»åŠ¡ç›¸å…³çš„æ¨æ–‡æ•°æ®ç»Ÿè®¡
        tweets_count = TweetData.query.filter_by(task_id=task_id).count()
        recent_tweets = TweetData.query.filter_by(task_id=task_id).order_by(TweetData.scraped_at.desc()).limit(5).all()
        
        task_data = task.to_dict()
        task_data['tweets_count'] = tweets_count
        task_data['recent_tweets'] = [tweet.to_dict() for tweet in recent_tweets]
        
        # è¿”å›HTMLæ ¼å¼çš„ä»»åŠ¡è¯¦æƒ…ï¼ˆç”¨äºæ¨¡æ€æ¡†æ˜¾ç¤ºï¼‰
        return render_template('task_detail.html', task=task, tweets_count=tweets_count, recent_tweets=recent_tweets)
        
    except Exception as e:
        return f'<div class="alert alert-danger">åŠ è½½ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {str(e)}</div>', 500

@app.route('/api/tasks/<int:task_id>/tweets', methods=['GET'])
def api_get_task_tweets(task_id):
    """è·å–ä»»åŠ¡çš„æ¨æ–‡æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰"""
    try:
        task = ScrapingTask.query.get_or_404(task_id)
        
        # è·å–ä»»åŠ¡ç›¸å…³çš„æ¨æ–‡æ•°æ®
        tweets = TweetData.query.filter_by(task_id=task_id).order_by(TweetData.scraped_at.desc()).all()
        
        return jsonify({
            'success': True,
            'task': task.to_dict(),
            'tweets_count': len(tweets),
            'tweets': [tweet.to_dict() for tweet in tweets]
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/tasks/<int:task_id>', methods=['DELETE'])
def api_delete_task(task_id):
    """åˆ é™¤ä»»åŠ¡"""
    try:
        task = ScrapingTask.query.get_or_404(task_id)
        
        # åˆ é™¤ç›¸å…³çš„æ¨æ–‡æ•°æ®
        TweetData.query.filter_by(task_id=task_id).delete()
        
        # åˆ é™¤ä»»åŠ¡
        db.session.delete(task)
        db.session.commit()
        
        return jsonify({'success': True, 'message': 'ä»»åŠ¡å·²åˆ é™¤'})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/data/<int:tweet_id>')
def tweet_detail(tweet_id):
    """æ¨æ–‡è¯¦æƒ…é¡µé¢"""
    try:
        tweet = TweetData.query.get_or_404(tweet_id)
        return render_template('tweet_detail.html', tweet=tweet)
    except Exception as e:
        flash(f'åŠ è½½æ¨æ–‡è¯¦æƒ…å¤±è´¥: {str(e)}', 'danger')
        return redirect(url_for('data'))


@app.route('/data/<int:tweet_id>', methods=['DELETE'])
def api_delete_tweet(tweet_id):
    """åˆ é™¤æ¨æ–‡"""
    try:
        tweet = TweetData.query.get_or_404(tweet_id)
        db.session.delete(tweet)
        db.session.commit()
        
        return jsonify({'success': True, 'message': 'æ¨æ–‡å·²åˆ é™¤'})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/chart_data')
def api_chart_data():
    """è·å–å›¾è¡¨æ•°æ®"""
    try:
        from datetime import datetime, timedelta
        from sqlalchemy import func, extract
        import re
        
        # æ¯æ—¥æ¨æ–‡æ•°é‡ç»Ÿè®¡ï¼ˆæœ€è¿‘30å¤©ï¼‰
        thirty_days_ago = datetime.now() - timedelta(days=30)
        daily_tweets = db.session.query(
            func.date(TweetData.scraped_at).label('date'),
            func.count(TweetData.id).label('count')
        ).filter(
            TweetData.scraped_at >= thirty_days_ago
        ).group_by(
            func.date(TweetData.scraped_at)
        ).order_by('date').all()
        
        # æ ¼å¼åŒ–æ¯æ—¥æ¨æ–‡æ•°æ®
        daily_data = {
            'labels': [item.date.strftime('%m-%d') if hasattr(item.date, 'strftime') else str(item.date) for item in daily_tweets],
            'data': [item.count for item in daily_tweets]
        }
        
        # çƒ­é—¨è¯é¢˜æ ‡ç­¾ç»Ÿè®¡ï¼ˆæå–#æ ‡ç­¾ï¼‰
        tweets_with_hashtags = TweetData.query.filter(
            TweetData.content.like('%#%')
        ).limit(1000).all()  # é™åˆ¶æŸ¥è¯¢æ•°é‡ä»¥æé«˜æ€§èƒ½
        
        hashtag_counts = {}
        for tweet in tweets_with_hashtags:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è¯é¢˜æ ‡ç­¾
            hashtags = re.findall(r'#\w+', tweet.content)
            for hashtag in hashtags:
                hashtag_counts[hashtag] = hashtag_counts.get(hashtag, 0) + 1
        
        # å–å‰10ä¸ªçƒ­é—¨æ ‡ç­¾
        top_hashtags = sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        hashtags_data = {
            'labels': [item[0] for item in top_hashtags],
            'data': [item[1] for item in top_hashtags]
        }
        
        # äº’åŠ¨æ•°æ®è¶‹åŠ¿ï¼ˆæœ€è¿‘30å¤©çš„å¹³å‡äº’åŠ¨æ•°ï¼‰
        engagement_data = db.session.query(
            func.date(TweetData.scraped_at).label('date'),
            func.avg(TweetData.likes).label('avg_likes'),
            func.avg(TweetData.retweets).label('avg_retweets'),
            func.avg(TweetData.comments).label('avg_comments')
        ).filter(
            TweetData.scraped_at >= thirty_days_ago
        ).group_by(
            func.date(TweetData.scraped_at)
        ).order_by('date').all()
        
        # æ ¼å¼åŒ–äº’åŠ¨æ•°æ®
        engagement_chart_data = {
            'labels': [item.date.strftime('%m-%d') if hasattr(item.date, 'strftime') else str(item.date) for item in engagement_data],
            'datasets': [
                {
                    'label': 'å¹³å‡ç‚¹èµæ•°',
                    'data': [float(item.avg_likes or 0) for item in engagement_data],
                    'borderColor': 'rgb(255, 99, 132)',
                    'backgroundColor': 'rgba(255, 99, 132, 0.2)'
                },
                {
                    'label': 'å¹³å‡è½¬å‘æ•°',
                    'data': [float(item.avg_retweets or 0) for item in engagement_data],
                    'borderColor': 'rgb(54, 162, 235)',
                    'backgroundColor': 'rgba(54, 162, 235, 0.2)'
                },
                {
                    'label': 'å¹³å‡è¯„è®ºæ•°',
                    'data': [float(item.avg_comments or 0) for item in engagement_data],
                    'borderColor': 'rgb(255, 205, 86)',
                    'backgroundColor': 'rgba(255, 205, 86, 0.2)'
                }
            ]
        }
        
        return jsonify({
            'success': True,
            'data': {
                'daily_tweets': daily_data,
                'hashtags': hashtags_data,
                'engagement': engagement_chart_data
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'è·å–å›¾è¡¨æ•°æ®å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/data/export')
def api_export_data():
    """å¯¼å‡ºæ•°æ®ä¸ºExcelæ–‡ä»¶"""
    try:
        from datetime import datetime
        import io
        import pandas as pd
        from flask import send_file
        import json
        
        # è·å–ç­›é€‰å‚æ•°
        search = request.args.get('search', '')
        task_id = request.args.get('task_id', type=int)
        min_likes = request.args.get('min_likes', type=int)
        min_retweets = request.args.get('min_retweets', type=int)
        
        # æ„å»ºæŸ¥è¯¢ï¼ˆä¸dataé¡µé¢ç›¸åŒçš„ç­›é€‰é€»è¾‘ï¼‰
        query = TweetData.query.join(ScrapingTask, TweetData.task_id == ScrapingTask.id)
        
        # æœç´¢è¿‡æ»¤
        if search:
            query = query.filter(
                db.or_(
                    TweetData.content.contains(search),
                    TweetData.username.contains(search)
                )
            )
        
        # ä»»åŠ¡è¿‡æ»¤
        if task_id:
            query = query.filter(TweetData.task_id == task_id)
        
        # ç‚¹èµæ•°è¿‡æ»¤
        if min_likes is not None:
            query = query.filter(TweetData.likes >= min_likes)
        
        # è½¬å‘æ•°è¿‡æ»¤
        if min_retweets is not None:
            query = query.filter(TweetData.retweets >= min_retweets)
        
        # æŒ‰æŠ“å–æ—¶é—´æ’åº
        tweets = query.order_by(TweetData.scraped_at.desc()).all()
        
        if not tweets:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ•°æ®å¯å¯¼å‡º'}), 400
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ® - åŒ…å«æ‰€æœ‰é‡è¦å­—æ®µ
        export_data = []
        # ç¼“å­˜ä»»åŠ¡åç§°ä»¥æé«˜æ€§èƒ½
        task_name_cache = {}
        
        for tweet in tweets:
            # è·å–ä»»åŠ¡åç§°
            task_name = ''
            if tweet.task_id:
                if tweet.task_id not in task_name_cache:
                    task = ScrapingTask.query.get(tweet.task_id)
                    task_name_cache[tweet.task_id] = task.name if task else ''
                task_name = task_name_cache[tweet.task_id]
            
            # å¤„ç†è¯é¢˜æ ‡ç­¾
            hashtags_str = ''
            if tweet.hashtags:
                try:
                    hashtags_list = json.loads(tweet.hashtags) if isinstance(tweet.hashtags, str) else tweet.hashtags
                    if isinstance(hashtags_list, list):
                        hashtags_str = ', '.join([f'#{tag}' for tag in hashtags_list if tag])
                    else:
                        hashtags_str = str(tweet.hashtags)
                except (json.JSONDecodeError, TypeError):
                    hashtags_str = str(tweet.hashtags) if tweet.hashtags else ''
            
            # å¤„ç†å‘å¸ƒæ—¶é—´
            publish_time_str = ''
            if tweet.publish_time:
                if isinstance(tweet.publish_time, str):
                    publish_time_str = tweet.publish_time
                else:
                    publish_time_str = tweet.publish_time.strftime('%Y-%m-%d %H:%M:%S')
            
            # å¤„ç†å¤šåª’ä½“å†…å®¹
            media_info = ''
            if tweet.media_content:
                try:
                    media_list = json.loads(tweet.media_content) if isinstance(tweet.media_content, str) else tweet.media_content
                    if isinstance(media_list, list) and media_list:
                        media_types = [item.get('type', 'æœªçŸ¥') for item in media_list if isinstance(item, dict)]
                        media_info = ', '.join(media_types)
                except (json.JSONDecodeError, TypeError):
                    media_info = 'æœ‰åª’ä½“å†…å®¹' if tweet.media_content else ''
            
            # æ„å»ºå¯¼å‡ºè¡Œæ•°æ®
            row = {
                'ID': tweet.id,
                'æ¨æ–‡åŸæ–‡å†…å®¹': tweet.content or '',
                'å®Œæ•´å†…å®¹': tweet.full_content or tweet.content or '',
                'ä½œè€…ï¼ˆè´¦å·ï¼‰': tweet.username or '',
                'å‘å¸ƒæ—¶é—´': publish_time_str,
                'æ¨æ–‡é“¾æ¥': tweet.link or '',
                'è¯é¢˜æ ‡ç­¾': hashtags_str,
                'ç±»å‹æ ‡ç­¾': tweet.content_type or '',
                'è¯„è®ºæ•°': tweet.comments or 0,
                'ç‚¹èµæ•°': tweet.likes or 0,
                'è½¬å‘æ•°': tweet.retweets or 0,
                'å¤šåª’ä½“å†…å®¹': media_info,
                'æŠ“å–æ—¶é—´': tweet.scraped_at.strftime('%Y-%m-%d %H:%M:%S') if tweet.scraped_at else '',
                'ä»»åŠ¡åç§°': task_name,
                'ä»»åŠ¡ID': tweet.task_id,
                'æ˜¯å¦å·²åŒæ­¥é£ä¹¦': 'æ˜¯' if tweet.synced_to_feishu else 'å¦',
                'æ˜¯å¦åŒ…å«è¯¦æƒ…å†…å®¹': 'æ˜¯' if tweet.has_detailed_content else 'å¦',
                'è¯¦æƒ…æŠ“å–é”™è¯¯': tweet.detail_error or ''
            }
            
            export_data.append(row)
        
        # åˆ›å»ºExcelæ–‡ä»¶
        df = pd.DataFrame(export_data)
        
        # ç”Ÿæˆæ–‡ä»¶å
        filename_template = SystemConfig.query.filter_by(key='export_filename_template').first()
        
        # è·å–ä»»åŠ¡åç§°
        task_name = 'all_data'
        if tweets and tweets[0].task_id:
            task = ScrapingTask.query.get(tweets[0].task_id)
            if task:
                task_name = task.name
        
        if filename_template and filename_template.value:
            filename = filename_template.value.format(
                date=datetime.now().strftime('%Y%m%d'),
                time=datetime.now().strftime('%H%M%S'),
                task_name=task_name
            )
        else:
            filename = f'twitter_data_{task_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        
        # åˆ›å»ºå†…å­˜ä¸­çš„Excelæ–‡ä»¶
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # å†™å…¥æ•°æ®åˆ°Excel
            df.to_excel(writer, sheet_name='æ¨æ–‡æ•°æ®', index=False)
            
            # è·å–å·¥ä½œè¡¨å¹¶è®¾ç½®åˆ—å®½
            worksheet = writer.sheets['æ¨æ–‡æ•°æ®']
            
            # è®¾ç½®åˆ—å®½ä»¥é€‚åº”å†…å®¹
            column_widths = {
                'A': 8,   # ID
                'B': 50,  # æ¨æ–‡åŸæ–‡å†…å®¹
                'C': 50,  # å®Œæ•´å†…å®¹
                'D': 20,  # ä½œè€…ï¼ˆè´¦å·ï¼‰
                'E': 20,  # å‘å¸ƒæ—¶é—´
                'F': 40,  # æ¨æ–‡é“¾æ¥
                'G': 30,  # è¯é¢˜æ ‡ç­¾
                'H': 15,  # ç±»å‹æ ‡ç­¾
                'I': 10,  # è¯„è®ºæ•°
                'J': 10,  # ç‚¹èµæ•°
                'K': 10,  # è½¬å‘æ•°
                'L': 20,  # å¤šåª’ä½“å†…å®¹
                'M': 20,  # æŠ“å–æ—¶é—´
                'N': 20,  # ä»»åŠ¡åç§°
                'O': 10,  # ä»»åŠ¡ID
                'P': 15,  # æ˜¯å¦å·²åŒæ­¥é£ä¹¦
                'Q': 15,  # æ˜¯å¦åŒ…å«è¯¦æƒ…å†…å®¹
                'R': 30   # è¯¦æƒ…æŠ“å–é”™è¯¯
            }
            
            for col, width in column_widths.items():
                worksheet.column_dimensions[col].width = width
            
            # è®¾ç½®è¡¨å¤´æ ·å¼
            from openpyxl.styles import Font, PatternFill, Alignment
            header_font = Font(bold=True, color='FFFFFF')
            header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
            header_alignment = Alignment(horizontal='center', vertical='center')
            
            for cell in worksheet[1]:
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = header_alignment
        
        output.seek(0)
        
        return send_file(
            output,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            as_attachment=True,
            download_name=f'{filename}.xlsx'
        )
        
    except Exception as e:
        print(f"Excelå¯¼å‡ºé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': f'å¯¼å‡ºå¤±è´¥: {str(e)}'}), 500

@app.route('/api/data/export/<int:task_id>')
def api_export_task_data(task_id):
    """å¯¼å‡ºç‰¹å®šä»»åŠ¡æ•°æ®"""
    try:
        tweets = TweetData.query.filter_by(task_id=task_id).all()
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        data = [tweet.to_dict() for tweet in tweets]
        
        return jsonify({
            'success': True,
            'data': data,
            'count': len(data)
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/data/sync_feishu/<int:task_id>', methods=['POST'])
def api_sync_feishu(task_id):
    """åŒæ­¥æ•°æ®åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼"""
    print(f"\nğŸ”„ [FEISHU_SYNC] å¼€å§‹åŒæ­¥ä»»åŠ¡ {task_id} åˆ°é£ä¹¦")
    print(f"â° [FEISHU_SYNC] åŒæ­¥æ—¶é—´: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # æ£€æŸ¥é£ä¹¦é…ç½®
        print(f"ğŸ“‹ [FEISHU_SYNC] æ£€æŸ¥é£ä¹¦é…ç½®...")
        if not FEISHU_CONFIG.get('enabled'):
            print(f"âŒ [FEISHU_SYNC] é£ä¹¦åŒæ­¥æœªå¯ç”¨")
            return jsonify({'success': False, 'error': 'é£ä¹¦åŒæ­¥æœªå¯ç”¨'}), 400
        
        print(f"âœ… [FEISHU_SYNC] é£ä¹¦åŒæ­¥å·²å¯ç”¨")
        
        # æ£€æŸ¥é£ä¹¦é…ç½®å®Œæ•´æ€§
        print(f"ğŸ” [FEISHU_SYNC] æ£€æŸ¥é…ç½®å®Œæ•´æ€§...")
        required_fields = ['app_id', 'app_secret', 'spreadsheet_token', 'table_id']
        missing_fields = [field for field in required_fields if not FEISHU_CONFIG.get(field)]
        if missing_fields:
            print(f"âŒ [FEISHU_SYNC] é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {missing_fields}")
            return jsonify({
                'success': False, 
                'error': f'é£ä¹¦é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {", ".join(missing_fields)}'
            }), 400
        
        print(f"âœ… [FEISHU_SYNC] é…ç½®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
        print(f"ğŸ“Š [FEISHU_SYNC] é…ç½®ä¿¡æ¯: app_id={FEISHU_CONFIG.get('app_id')[:8]}..., spreadsheet_token={FEISHU_CONFIG.get('spreadsheet_token')[:8]}..., table_id={FEISHU_CONFIG.get('table_id')}")
        
        # è·å–ä»»åŠ¡æ•°æ® - åªè·å–æœªåŒæ­¥çš„æ•°æ®
        print(f"ğŸ“Š [FEISHU_SYNC] æŸ¥è¯¢ä»»åŠ¡ {task_id} çš„æœªåŒæ­¥æ•°æ®...")
        # ä½¿ç”¨0è€Œä¸æ˜¯Falseï¼Œå› ä¸ºSQLiteä¸­BOOLEANå­˜å‚¨ä¸ºæ•´æ•°
        tweets = TweetData.query.filter_by(task_id=task_id, synced_to_feishu=0).all()
        print(f"ğŸ“Š [FEISHU_SYNC] æ‰¾åˆ° {len(tweets)} æ¡æœªåŒæ­¥æ•°æ®")
        
        if not tweets:
            # æ£€æŸ¥æ˜¯å¦æœ‰å·²åŒæ­¥çš„æ•°æ®
            print(f"ğŸ” [FEISHU_SYNC] æ²¡æœ‰æœªåŒæ­¥æ•°æ®ï¼Œæ£€æŸ¥å·²åŒæ­¥æ•°æ®...")
            # ä½¿ç”¨1è€Œä¸æ˜¯True
            synced_count = TweetData.query.filter_by(task_id=task_id, synced_to_feishu=1).count()
            print(f"ğŸ“Š [FEISHU_SYNC] å·²åŒæ­¥æ•°æ®æ•°é‡: {synced_count}")
            if synced_count > 0:
                print(f"âœ… [FEISHU_SYNC] æ‰€æœ‰æ•°æ®éƒ½å·²åŒæ­¥")
                return jsonify({'success': True, 'message': f'ä»»åŠ¡ {task_id} çš„æ‰€æœ‰æ•°æ®ï¼ˆ{synced_count} æ¡ï¼‰éƒ½å·²åŒæ­¥åˆ°é£ä¹¦'})
            else:
                print(f"âŒ [FEISHU_SYNC] æ²¡æœ‰ä»»ä½•æ•°æ®éœ€è¦åŒæ­¥")
                return jsonify({'success': False, 'error': 'æ²¡æœ‰æ•°æ®éœ€è¦åŒæ­¥'}), 400
        
        # åˆå§‹åŒ–äº‘åŒæ­¥ç®¡ç†å™¨
        print(f"ğŸ”§ [FEISHU_SYNC] åˆå§‹åŒ–äº‘åŒæ­¥ç®¡ç†å™¨...")
        sync_config = {
            'feishu': {
                'enabled': True,
                'app_id': FEISHU_CONFIG['app_id'],
                'app_secret': FEISHU_CONFIG['app_secret'],
                'spreadsheet_token': FEISHU_CONFIG['spreadsheet_token'],
                'table_id': FEISHU_CONFIG['table_id'],
                'base_url': 'https://open.feishu.cn/open-apis'
            }
        }
        print(f"ğŸ“‹ [FEISHU_SYNC] åŒæ­¥é…ç½®: {sync_config}")
        sync_manager = CloudSyncManager(sync_config)
        print(f"âœ… [FEISHU_SYNC] äº‘åŒæ­¥ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # å‡†å¤‡æ•°æ®ï¼ŒæŒ‰ç…§é£ä¹¦å¤šç»´è¡¨æ ¼å­—æ®µæ˜ å°„
        print(f"ğŸ“ [FEISHU_SYNC] å¼€å§‹å‡†å¤‡æ•°æ®ï¼Œå…± {len(tweets)} æ¡æ¨æ–‡...")
        data = []
        for i, tweet in enumerate(tweets):
            print(f"ğŸ“ [FEISHU_SYNC] å¤„ç†ç¬¬ {i+1}/{len(tweets)} æ¡æ¨æ–‡ (ID: {tweet.id})")
            
            # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„ç±»å‹æ ‡ç­¾ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨è‡ªåŠ¨åˆ†ç±»
            content_type = tweet.content_type or classify_content_type(tweet.content)
            print(f"ğŸ·ï¸ [FEISHU_SYNC] æ¨æ–‡ {tweet.id} ç±»å‹æ ‡ç­¾: {content_type}")
            
            # å¤„ç†å‘å¸ƒæ—¶é—´ - ä¿®å¤æ—¶é—´æˆ³è½¬æ¢é—®é¢˜ï¼Œä½¿ç”¨ç§’çº§æ—¶é—´æˆ³ä¸cloud_sync.pyä¿æŒä¸€è‡´
            print(f"â° [FEISHU_SYNC] å¤„ç†æ¨æ–‡ {tweet.id} çš„å‘å¸ƒæ—¶é—´...")
            print(f"â° [FEISHU_SYNC] åŸå§‹å‘å¸ƒæ—¶é—´: {tweet.publish_time} (ç±»å‹: {type(tweet.publish_time)})")
            print(f"â° [FEISHU_SYNC] æŠ“å–æ—¶é—´: {tweet.scraped_at} (ç±»å‹: {type(tweet.scraped_at)})")
            
            publish_time = 0
            if tweet.publish_time:
                try:
                    if isinstance(tweet.publish_time, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºdatetime
                        from dateutil import parser
                        dt = parser.parse(tweet.publish_time)
                        # è½¬æ¢ä¸ºç§’çº§æ—¶é—´æˆ³ï¼ˆä¸cloud_sync.pyä¿æŒä¸€è‡´ï¼‰
                        publish_time = int(dt.timestamp())
                        print(f"â° [FEISHU_SYNC] å­—ç¬¦ä¸²æ—¶é—´è§£ææˆåŠŸ: {dt} -> {publish_time}")
                    else:
                        # å¦‚æœå·²ç»æ˜¯datetimeå¯¹è±¡
                        publish_time = int(tweet.publish_time.timestamp())
                        print(f"â° [FEISHU_SYNC] datetimeå¯¹è±¡è½¬æ¢æˆåŠŸ: {tweet.publish_time} -> {publish_time}")
                except Exception as e:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨æŠ“å–æ—¶é—´ä½œä¸ºå¤‡é€‰
                    print(f"âŒ [FEISHU_SYNC] å‘å¸ƒæ—¶é—´è§£æå¤±è´¥: {e}, ä½¿ç”¨æŠ“å–æ—¶é—´ä½œä¸ºå¤‡é€‰")
                    publish_time = int(tweet.scraped_at.timestamp())
                    print(f"â° [FEISHU_SYNC] å¤‡é€‰æ—¶é—´æˆ³: {publish_time}")
            else:
                # å¦‚æœæ²¡æœ‰å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨æŠ“å–æ—¶é—´
                print(f"âš ï¸ [FEISHU_SYNC] æ²¡æœ‰å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨æŠ“å–æ—¶é—´")
                publish_time = int(tweet.scraped_at.timestamp())
                print(f"â° [FEISHU_SYNC] æŠ“å–æ—¶é—´æˆ³: {publish_time}")
            
            # éªŒè¯æ—¶é—´æˆ³åˆç†æ€§ï¼Œé¿å…1970å¹´é—®é¢˜
            if publish_time < 946684800:  # 2000å¹´1æœˆ1æ—¥çš„æ—¶é—´æˆ³
                print(f"âš ï¸ [FEISHU_SYNC] å‘å¸ƒæ—¶é—´æˆ³å¼‚å¸¸ ({publish_time})ï¼Œå¯èƒ½æ˜¯1970å¹´é—®é¢˜ï¼Œä¿®æ­£ä¸ºå½“å‰æ—¶é—´")
                publish_time = int(datetime.now().timestamp())
                print(f"â° [FEISHU_SYNC] ä¿®æ­£åæ—¶é—´æˆ³: {publish_time}")
            
            # æ ¹æ®é£ä¹¦è¡¨æ ¼çš„å®é™…å­—æ®µåç§°è¿›è¡Œç²¾ç¡®æ˜ å°„
            # ä»ç”¨æˆ·æä¾›çš„æˆªå›¾å¯ä»¥çœ‹åˆ°å­—æ®µåŒ…æ‹¬ï¼šæ¨æ–‡åŸæ–‡å†…å®¹ã€å‘å¸ƒæ—¶é—´ã€ä½œè€…ï¼ˆè´¦å·ï¼‰ã€æ¨æ–‡é“¾æ¥ã€è¯é¢˜æ ‡ç­¾ï¼ˆHashtagï¼‰ã€ç±»å‹æ ‡ç­¾ã€è¯„è®ºã€ç‚¹èµã€è½¬å‘
            hashtags_str = ', '.join(json.loads(tweet.hashtags) if tweet.hashtags else [])
            
            # è½¬æ¢ä¸ºæ¯«ç§’çº§æ—¶é—´æˆ³ï¼ˆé£ä¹¦APIè¦æ±‚ï¼‰
            if publish_time < 10000000000:  # ç§’çº§æ—¶é—´æˆ³
                publish_time_ms = publish_time * 1000
            else:  # å·²ç»æ˜¯æ¯«ç§’çº§
                publish_time_ms = publish_time
            
            print(f"â° [FEISHU_SYNC] å‘å¸ƒæ—¶é—´æˆ³è½¬æ¢: {publish_time} -> {publish_time_ms} (æ¯«ç§’çº§)")
            
            tweet_data = {
                'æ¨æ–‡åŸæ–‡å†…å®¹': tweet.content or '',
                # æ³¨æ„ï¼šç§»é™¤å‘å¸ƒæ—¶é—´å­—æ®µï¼Œä¸åŒæ­¥æ—¶é—´æˆ³
                'ä½œè€…ï¼ˆè´¦å·ï¼‰': tweet.username or '',
                'æ¨æ–‡é“¾æ¥': tweet.link or '',
                'è¯é¢˜æ ‡ç­¾ï¼ˆHashtagï¼‰': hashtags_str,
                'ç±»å‹æ ‡ç­¾': content_type or '',
                'è¯„è®º': tweet.comments or 0,
                'ç‚¹èµ': tweet.likes or 0,
                'è½¬å‘': tweet.retweets or 0
                # æ³¨æ„ï¼šç§»é™¤åˆ›å»ºæ—¶é—´å­—æ®µï¼Œè®©é£ä¹¦è‡ªåŠ¨ç”Ÿæˆ
            }
            
            print(f"ğŸ“Š [FEISHU_SYNC] æ¨æ–‡ {tweet.id} æ•°æ®æ˜ å°„å®Œæˆ:")
            print(f"   - å†…å®¹: {(tweet.content or '')[:50]}...")
            print(f"   - ä½œè€…: {tweet.username or ''}")
            print(f"   - é“¾æ¥: {tweet.link or ''}")
            print(f"   - æ ‡ç­¾: {hashtags_str}")
            print(f"   - ç±»å‹: {content_type or ''}")
            
            data.append(tweet_data)
        
        # åŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼
        print(f"ğŸš€ [FEISHU_SYNC] å¼€å§‹åŒæ­¥ {len(data)} æ¡æ•°æ®åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼...")
        print(f"ğŸ“‹ [FEISHU_SYNC] ç›®æ ‡è¡¨æ ¼: {FEISHU_CONFIG['spreadsheet_token']}")
        print(f"ğŸ“‹ [FEISHU_SYNC] ç›®æ ‡è¡¨ID: {FEISHU_CONFIG['table_id']}")
        
        success = sync_manager.sync_to_feishu(
            data,
            FEISHU_CONFIG['spreadsheet_token'],
            FEISHU_CONFIG['table_id']
        )
        
        print(f"ğŸ“Š [FEISHU_SYNC] åŒæ­¥ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        if success:
            print(f"âœ… [FEISHU_SYNC] åŒæ­¥æˆåŠŸï¼Œå¼€å§‹æ›´æ–°æ•°æ®åº“çŠ¶æ€...")
            # æ›´æ–°åŒæ­¥çŠ¶æ€å’Œå†…å®¹ç±»å‹
            for i, tweet in enumerate(tweets):
                print(f"ğŸ“ [FEISHU_SYNC] æ›´æ–°æ¨æ–‡ {tweet.id} åŒæ­¥çŠ¶æ€")
                # ä½¿ç”¨1è€Œä¸æ˜¯Trueï¼Œå› ä¸ºSQLiteä¸­BOOLEANå­˜å‚¨ä¸ºæ•´æ•°
                tweet.synced_to_feishu = 1
                tweet.content_type = classify_content_type(tweet.content)
            
            print(f"ğŸ’¾ [FEISHU_SYNC] æäº¤æ•°æ®åº“æ›´æ”¹...")
            db.session.commit()
            print(f"âœ… [FEISHU_SYNC] æ•°æ®åº“æ›´æ–°å®Œæˆ")
            
            # æ‰§è¡Œæ•°æ®éªŒè¯
            print(f"ğŸ” [FEISHU_SYNC] å¼€å§‹æ•°æ®éªŒè¯...")
            try:
                from feishu_data_validator import FeishuDataValidator
                validator = FeishuDataValidator()
                validation_result = validator.validate_sync_data(task_id=task_id)
                
                if validation_result.get('success'):
                    comparison = validation_result['comparison_result']
                    summary = comparison['summary']
                    print(f"âœ… [FEISHU_SYNC] æ•°æ®éªŒè¯å®Œæˆ")
                    print(f"ğŸ“Š [FEISHU_SYNC] éªŒè¯ç»“æœ: åŒæ­¥å‡†ç¡®ç‡ {summary['sync_accuracy']:.2f}%")
                    print(f"ğŸ“Š [FEISHU_SYNC] åŒ¹é…è®°å½•: {summary['matched_count']}/{summary['total_local']}")
                    
                    # åœ¨è¿”å›æ¶ˆæ¯ä¸­åŒ…å«éªŒè¯ç»“æœ
                    validation_msg = f"ï¼ŒéªŒè¯ç»“æœ: å‡†ç¡®ç‡ {summary['sync_accuracy']:.2f}% ({summary['matched_count']}/{summary['total_local']} æ¡åŒ¹é…)"
                    if summary['sync_accuracy'] < 95:
                        validation_msg += f"ï¼Œå‘ç° {summary['field_mismatch_count']} æ¡å­—æ®µä¸åŒ¹é…"
                else:
                    print(f"âš ï¸ [FEISHU_SYNC] æ•°æ®éªŒè¯å¤±è´¥: {validation_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    validation_msg = "ï¼Œæ•°æ®éªŒè¯å¤±è´¥"
                    
            except Exception as e:
                print(f"âŒ [FEISHU_SYNC] æ•°æ®éªŒè¯å¼‚å¸¸: {e}")
                validation_msg = "ï¼Œæ•°æ®éªŒè¯å¼‚å¸¸"
            
            print(f"ğŸ‰ [FEISHU_SYNC] ä»»åŠ¡ {task_id} åŒæ­¥å®Œæˆï¼Œå…± {len(data)} æ¡æ•°æ®")
            return jsonify({'success': True, 'message': f'æˆåŠŸåŒæ­¥ {len(data)} æ¡æ•°æ®åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼{validation_msg}'})
        else:
            print(f"âŒ [FEISHU_SYNC] åŒæ­¥å¤±è´¥")
            return jsonify({'success': False, 'error': 'é£ä¹¦åŒæ­¥å¤±è´¥'}), 500
            
    except Exception as e:
        print(f"âŒ [FEISHU_SYNC] åŒæ­¥è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        import traceback
        print(f"ğŸ“‹ [FEISHU_SYNC] å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/status')
def api_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    try:
        # è·å–ä»»åŠ¡ç»Ÿè®¡
        total_tasks = ScrapingTask.query.count()
        running_tasks = ScrapingTask.query.filter_by(status='running').count()
        completed_tasks = ScrapingTask.query.filter_by(status='completed').count()
        failed_tasks = ScrapingTask.query.filter_by(status='failed').count()
        queued_tasks = ScrapingTask.query.filter_by(status='queued').count()
        
        # è·å–æ¨æ–‡ç»Ÿè®¡
        total_tweets = TweetData.query.count()
        today_tweets = TweetData.query.filter(
            db.func.date(TweetData.scraped_at) == db.func.date(db.func.now())
        ).count()
        
        # è·å–å¹¶è¡Œä»»åŠ¡çŠ¶æ€
        task_status = task_manager.get_task_status()
        
        # è·å–å½“å‰è¿è¡Œçš„ä»»åŠ¡è¯¦æƒ…
        current_tasks = []
        for task_id in task_status['running_tasks']:
            task = ScrapingTask.query.get(task_id)
            if task:
                current_tasks.append({
                    'id': task.id,
                    'name': task.name,
                    'status': task.status,
                    'started_at': task.started_at.isoformat() if task.started_at else None
                })
        
        return jsonify({
            'success': True,
            'data': {
                'tasks': {
                    'total': total_tasks,
                    'running': running_tasks,
                    'completed': completed_tasks,
                    'failed': failed_tasks,
                    'queued': queued_tasks
                },
                'tweets': {
                    'total': total_tweets,
                    'today': today_tweets
                },
                'parallel_status': {
                    'running_count': task_status['running_count'],
                    'max_concurrent': task_status['max_concurrent'],
                    'available_slots': task_status['available_slots'],
                    'available_browsers': task_status['available_browsers'],
                    'current_tasks': current_tasks
                },
                'system_running': task_status['running_count'] > 0
            }
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/config/feishu', methods=['GET'])
def api_get_feishu_config():
    """è·å–é£ä¹¦é…ç½®"""
    return jsonify({
        'app_id': FEISHU_CONFIG['app_id'],
        'app_secret': FEISHU_CONFIG['app_secret'] if FEISHU_CONFIG['app_secret'] else 'æœªé…ç½®',
        'spreadsheet_token': FEISHU_CONFIG['spreadsheet_token'],
        'table_id': FEISHU_CONFIG['table_id'],
        'enabled': FEISHU_CONFIG['enabled'],
        'auto_sync': FEISHU_CONFIG.get('auto_sync', False)
    })

@app.route('/api/config/feishu', methods=['POST'])
def api_update_feishu_config():
    """æ›´æ–°é£ä¹¦é…ç½®"""
    try:
        data = request.get_json()
        
        # å‡†å¤‡é£ä¹¦é…ç½®æ•°æ®
        feishu_configs = {
            'feishu_app_id': data.get('app_id', FEISHU_CONFIG['app_id']),
            'feishu_app_secret': data.get('app_secret', FEISHU_CONFIG['app_secret']),
            'feishu_spreadsheet_token': data.get('spreadsheet_token', FEISHU_CONFIG['spreadsheet_token']),
            'feishu_table_id': data.get('table_id', FEISHU_CONFIG['table_id']),
            'feishu_enabled': str(data.get('enabled', FEISHU_CONFIG['enabled'])),
            'feishu_auto_sync': str(data.get('auto_sync', FEISHU_CONFIG.get('auto_sync', False))),
            'sync_interval': str(data.get('sync_interval', 300))
        }
        
        # æ›´æ–°æˆ–åˆ›å»ºé…ç½®è®°å½•åˆ°æ•°æ®åº“
        for key, value in feishu_configs.items():
            config = SystemConfig.query.filter_by(key=key).first()
            if config:
                config.value = str(value)
                config.updated_at = datetime.utcnow()
            else:
                config = SystemConfig(
                    key=key,
                    value=str(value),
                    description=f'é£ä¹¦é…ç½®: {key}'
                )
                db.session.add(config)
        
        db.session.commit()
        
        # æ›´æ–°å…¨å±€é…ç½®ï¼ˆç”¨äºå½“å‰ä¼šè¯ï¼‰
        FEISHU_CONFIG.update({
            'app_id': feishu_configs['feishu_app_id'],
            'app_secret': feishu_configs['feishu_app_secret'],
            'spreadsheet_token': feishu_configs['feishu_spreadsheet_token'],
            'table_id': feishu_configs['feishu_table_id'],
            'enabled': feishu_configs['feishu_enabled'].lower() == 'true',
            'auto_sync': feishu_configs['feishu_auto_sync'].lower() == 'true'
        })
        
        return jsonify({'success': True, 'message': 'é£ä¹¦é…ç½®æ›´æ–°æˆåŠŸ'})
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/data/validate_feishu/<int:task_id>', methods=['POST'])
def api_validate_feishu_data(task_id):
    """éªŒè¯é£ä¹¦æ•°æ®åŒæ­¥å‡†ç¡®æ€§"""
    try:
        print(f"\nğŸ” [FEISHU_VALIDATE] å¼€å§‹éªŒè¯ä»»åŠ¡ {task_id} çš„é£ä¹¦æ•°æ®")
        print(f"â° [FEISHU_VALIDATE] éªŒè¯æ—¶é—´: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ£€æŸ¥é£ä¹¦é…ç½®
        if not FEISHU_CONFIG.get('enabled'):
            return jsonify({'success': False, 'error': 'é£ä¹¦åŒæ­¥æœªå¯ç”¨'}), 400
        
        required_fields = ['app_id', 'app_secret', 'spreadsheet_token', 'table_id']
        missing_fields = [field for field in required_fields if not FEISHU_CONFIG.get(field)]
        if missing_fields:
            return jsonify({
                'success': False, 
                'error': f'é£ä¹¦é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {", ".join(missing_fields)}'
            }), 400
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        task = ScrapingTask.query.get(task_id)
        if not task:
            return jsonify({'success': False, 'error': f'ä»»åŠ¡ {task_id} ä¸å­˜åœ¨'}), 404
        
        # æ‰§è¡Œæ•°æ®éªŒè¯
        from feishu_data_validator import FeishuDataValidator
        validator = FeishuDataValidator()
        validation_result = validator.validate_sync_data(task_id=task_id)
        
        if validation_result.get('success'):
            comparison = validation_result['comparison_result']
            summary = comparison['summary']
            
            print(f"âœ… [FEISHU_VALIDATE] éªŒè¯å®Œæˆ")
            print(f"ğŸ“Š [FEISHU_VALIDATE] åŒæ­¥å‡†ç¡®ç‡: {summary['sync_accuracy']:.2f}%")
            
            # æ„å»ºè¯¦ç»†çš„éªŒè¯æŠ¥å‘Š
            validation_report = {
                'task_id': task_id,
                'task_name': task.name,
                'validation_time': validation_result.get('validation_time'),
                'summary': summary,
                'details': {
                    'matched_records_count': len(comparison['matched_records']),
                    'missing_in_feishu_count': len(comparison['missing_in_feishu']),
                    'extra_in_feishu_count': len(comparison['extra_in_feishu']),
                    'field_mismatches_count': len(comparison['field_mismatches'])
                },
                'quality_assessment': {
                    'level': 'excellent' if summary['sync_accuracy'] >= 95 else 'good' if summary['sync_accuracy'] >= 85 else 'needs_improvement',
                    'description': f"åŒæ­¥å‡†ç¡®ç‡ {summary['sync_accuracy']:.2f}%"
                }
            }
            
            # å¦‚æœæœ‰ä¸åŒ¹é…çš„æ•°æ®ï¼Œæä¾›æ ·ä¾‹
            if comparison['missing_in_feishu']:
                validation_report['missing_samples'] = comparison['missing_in_feishu'][:3]
            
            if comparison['field_mismatches']:
                validation_report['mismatch_samples'] = comparison['field_mismatches'][:3]
            
            return jsonify({
                'success': True, 
                'message': f'æ•°æ®éªŒè¯å®Œæˆï¼Œå‡†ç¡®ç‡ {summary["sync_accuracy"]:.2f}%',
                'validation_report': validation_report
            })
        else:
            # ç¡®ä¿é”™è¯¯ä¿¡æ¯å®Œæ•´
            error_msg = validation_result.get('error')
            if not error_msg:
                error_msg = 'æ•°æ®éªŒè¯å¤±è´¥ï¼Œä½†æœªæä¾›å…·ä½“é”™è¯¯ä¿¡æ¯'
                print(f"âš ï¸ [FEISHU_VALIDATE] è­¦å‘Š: éªŒè¯ç»“æœä¸­ç¼ºå°‘é”™è¯¯ä¿¡æ¯")
                print(f"ğŸ“‹ [FEISHU_VALIDATE] å®Œæ•´éªŒè¯ç»“æœ: {validation_result}")
            
            print(f"âŒ [FEISHU_VALIDATE] éªŒè¯å¤±è´¥: {error_msg}")
            return jsonify({'success': False, 'error': error_msg}), 500
            
    except Exception as e:
        print(f"âŒ [FEISHU_VALIDATE] éªŒè¯å¼‚å¸¸: {e}")
        import traceback
        print(f"ğŸ“‹ [FEISHU_VALIDATE] å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/config/feishu/test', methods=['POST'])
def api_test_feishu_connection():
    """æµ‹è¯•é£ä¹¦è¿æ¥"""
    import io
    import sys
    from contextlib import redirect_stdout, redirect_stderr
    
    # æ•è·æ—¥å¿—è¾“å‡º
    log_capture = io.StringIO()
    
    try:
        # ä»è¯·æ±‚ä½“è·å–é…ç½®
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False, 
                'error': 'è¯·æä¾›é£ä¹¦é…ç½®ä¿¡æ¯', 
                'status_code': 400,
                'logs': []
            }), 400
        
        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        required_fields = ['app_id', 'app_secret', 'spreadsheet_token', 'table_id']
        missing_fields = [field for field in required_fields if not data.get(field)]
        if missing_fields:
            return jsonify({
                'success': False, 
                'error': f'é£ä¹¦é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {", ".join(missing_fields)}',
                'status_code': 400,
                'logs': []
            }), 400
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        test_config = {
            'feishu': {
                'enabled': True,
                'app_id': data['app_id'],
                'app_secret': data['app_secret'],
                'spreadsheet_token': data['spreadsheet_token'],
                'table_id': data['table_id']
            }
        }
        
        # æ•è·æ§åˆ¶å°è¾“å‡º
        with redirect_stdout(log_capture), redirect_stderr(log_capture):
            print(f"[é£ä¹¦æµ‹è¯•] å¼€å§‹æµ‹è¯•è¿æ¥...")
            print(f"[é£ä¹¦æµ‹è¯•] App ID: {data['app_id']}")
            print(f"[é£ä¹¦æµ‹è¯•] æ–‡æ¡£Token: {data['spreadsheet_token']}")
            print(f"[é£ä¹¦æµ‹è¯•] è¡¨æ ¼ID: {data['table_id']}")
            
            # åˆå§‹åŒ–äº‘åŒæ­¥ç®¡ç†å™¨
            sync_manager = CloudSyncManager(test_config)
            
            # è®¾ç½®é£ä¹¦é…ç½®
            print(f"[é£ä¹¦æµ‹è¯•] æ­£åœ¨è®¾ç½®é£ä¹¦é…ç½®...")
            if not sync_manager.setup_feishu(data['app_id'], data['app_secret']):
                print(f"[é£ä¹¦æµ‹è¯•] é£ä¹¦é…ç½®è®¾ç½®å¤±è´¥")
                logs = log_capture.getvalue().split('\n')
                return jsonify({
                    'success': False, 
                    'error': 'é£ä¹¦é…ç½®è®¾ç½®å¤±è´¥', 
                    'status_code': 500,
                    'logs': logs
                }), 500
            
            print(f"[é£ä¹¦æµ‹è¯•] é£ä¹¦é…ç½®è®¾ç½®æˆåŠŸ")
            
            # æµ‹è¯•è¿æ¥ï¼ˆå‘é€ä¸€æ¡æµ‹è¯•æ•°æ®ï¼‰
            current_time = datetime.utcnow()
            test_data = [{
                'æ¨æ–‡åŸæ–‡å†…å®¹': 'æµ‹è¯•è¿æ¥ - ' + current_time.strftime('%Y-%m-%d %H:%M:%S'),
                'æ¨æ–‡åŸ æ–‡å†…å®¹': 'æµ‹è¯•è¿æ¥ - ' + current_time.strftime('%Y-%m-%d %H:%M:%S'),
                'å‘å¸ƒæ—¶é—´': current_time.strftime('%Y-%m-%d %H:%M:%S'),  # ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼
                'ä½œè€…ï¼ˆè´¦å·ï¼‰': 'test_user',
                'æ¨æ–‡é“¾æ¥': 'https://twitter.com/test',
                'è¯é¢˜æ ‡ç­¾ï¼ˆHashtagï¼‰': '#æµ‹è¯•',
                'ç±»å‹æ ‡ç­¾': 'æµ‹è¯•',
                'è¯„è®º': 0,
                'è½¬å‘': 0,
                'ç‚¹èµ': 0,
                'åˆ›å»ºæ—¶é—´': current_time.strftime('%Y-%m-%d %H:%M:%S')  # ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼
            }]
            
            print(f"[é£ä¹¦æµ‹è¯•] æ­£åœ¨å‘é€æµ‹è¯•æ•°æ®...")
            
            try:
                success = sync_manager.sync_to_feishu(
                    test_data,
                    data['spreadsheet_token'],
                    data['table_id']
                )
                
                # è·å–æ•è·çš„æ—¥å¿—
                logs = log_capture.getvalue().split('\n')
                logs = [log.strip() for log in logs if log.strip()]  # è¿‡æ»¤ç©ºè¡Œ
                
                if success:
                    print(f"[é£ä¹¦æµ‹è¯•] è¿æ¥æµ‹è¯•æˆåŠŸï¼")
                    logs = log_capture.getvalue().split('\n')
                    logs = [log.strip() for log in logs if log.strip()]
                    return jsonify({
                        'success': True, 
                        'message': 'é£ä¹¦è¿æ¥æµ‹è¯•æˆåŠŸ', 
                        'status_code': 200,
                        'logs': logs
                    }), 200
                else:
                    print(f"[é£ä¹¦æµ‹è¯•] åŒæ­¥æ“ä½œè¿”å›å¤±è´¥")
                    logs = log_capture.getvalue().split('\n')
                    logs = [log.strip() for log in logs if log.strip()]
                    return jsonify({
                        'success': False, 
                        'error': 'é£ä¹¦è¿æ¥æµ‹è¯•å¤±è´¥ï¼šåŒæ­¥æ“ä½œè¿”å›å¤±è´¥', 
                        'status_code': 500,
                        'logs': logs
                    }), 500
            except Exception as sync_error:
                print(f"[é£ä¹¦æµ‹è¯•] åŒæ­¥å¼‚å¸¸: {str(sync_error)}")
                logs = log_capture.getvalue().split('\n')
                logs = [log.strip() for log in logs if log.strip()]
                return jsonify({
                    'success': False, 
                    'error': f'é£ä¹¦è¿æ¥æµ‹è¯•å¤±è´¥ï¼š{str(sync_error)}', 
                    'status_code': 500,
                    'logs': logs
                }), 500
            
    except Exception as e:
        logs = log_capture.getvalue().split('\n')
        logs = [log.strip() for log in logs if log.strip()]
        return jsonify({
            'success': False, 
            'error': f'é£ä¹¦è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}', 
            'status_code': 500,
            'logs': logs
        }), 500

@app.route('/api/tweet/update_content_type', methods=['POST'])
def api_update_tweet_content_type():
    """æ›´æ–°æ¨æ–‡çš„ç±»å‹æ ‡ç­¾"""
    try:
        data = request.get_json()
        tweet_id = data.get('tweet_id')
        content_type = data.get('content_type', '').strip()
        
        if not tweet_id:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘æ¨æ–‡ID'
            })
        
        # æŸ¥æ‰¾æ¨æ–‡
        tweet = TweetData.query.get(tweet_id)
        if not tweet:
            return jsonify({
                'success': False,
                'message': 'æ¨æ–‡ä¸å­˜åœ¨'
            })
        
        # æ›´æ–°ç±»å‹æ ‡ç­¾
        tweet.content_type = content_type
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': 'ç±»å‹æ ‡ç­¾æ›´æ–°æˆåŠŸ',
            'tweet_id': tweet_id,
            'content_type': content_type
        })
        
    except Exception as e:
        logger.error(f"æ›´æ–°æ¨æ–‡ç±»å‹æ ‡ç­¾å¤±è´¥: {str(e)}")
        db.session.rollback()
        return jsonify({
            'success': False,
            'message': f'æ›´æ–°å¤±è´¥: {str(e)}'
        })

@app.route('/api/check_adspower_installation', methods=['POST'])
def api_check_adspower_installation():
    """æ£€æµ‹AdsPowerå®‰è£…çŠ¶æ€"""
    try:
        app.logger.debug('Starting api_test_open_adspower')
        data = request.form.to_dict()
        
        # ä»æ•°æ®åº“è·å–é…ç½®ä¿¡æ¯
        configs = SystemConfig.query.all()
        config_dict = {cfg.key: cfg.value for cfg in configs}
        
        # è·å–APIé…ç½®ä¿¡æ¯
        api_host = data.get('api_host') or config_dict.get('adspower_api_host', 'localhost')
        api_port = data.get('api_port') or config_dict.get('adspower_api_port', '50325')
        api_status = config_dict.get('adspower_api_status', '')
        api_key = config_dict.get('adspower_api_key', '')
        
        # å¦‚æœAPIçŠ¶æ€ä¸ºå…³é—­ï¼Œç›´æ¥è¿”å›å¤±è´¥
        if api_status == 'å…³é—­':
            return jsonify({
                'success': False,
                'message': 'AdsPower APIæ¥å£çŠ¶æ€å·²è®¾ç½®ä¸ºå…³é—­'
            })
        
        # æ„å»ºAPI URL
        api_url = f'http://{api_host}:{api_port}'
        test_url = f"{api_url}/api/v1/user/list"
        
        # å‡†å¤‡è¯·æ±‚å¤´
        headers = {}
        if api_key:
            headers['Authorization'] = f'Bearer {api_key}'
        
        try:
            response = requests.get(test_url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                if result.get('code') == 0:
                    # AdsPoweræ­£åœ¨è¿è¡Œ
                    users = result.get('data', {}).get('list', [])
                    response = jsonify({
                        'success': True,
                        'message': 'AdsPowerå·²å®‰è£…å¹¶æ­£åœ¨è¿è¡Œ',
                        'user_count': len(users),
                        'api_url': api_url
                    })
                    response.headers['Content-Type'] = 'application/json'
                    return response
                else:
                    return jsonify({
                        'success': False,
                        'message': f'AdsPower APIè¿”å›é”™è¯¯: {result.get("msg", "æœªçŸ¥é”™è¯¯")}'
                    })
            elif response.status_code == 401:
                return jsonify({
                    'success': False,
                    'message': 'API KeyéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®'
                })
            else:
                return jsonify({
                    'success': False,
                    'message': f'HTTPé”™è¯¯: {response.status_code}'
                })
                
        except requests.exceptions.ConnectionError as e:
            return jsonify({
                'success': False,
                'message': f'è¿æ¥å¤±è´¥: æ— æ³•è¿æ¥åˆ° {api_url}ï¼Œè¯·ç¡®ä¿AdsPowerå·²å¯åŠ¨'
            })
        except requests.exceptions.Timeout as e:
            return jsonify({
                'success': False,
                'message': 'è¿æ¥è¶…æ—¶: AdsPowerå“åº”è¶…æ—¶'
            })
        except Exception as e:
            return jsonify({
                'success': False,
                'message': f'è¯·æ±‚é”™è¯¯: {str(e)}'
            })
            
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'æ£€æµ‹å¤±è´¥: {str(e)}'
        })

@app.route('/api/test_adspower_connection', methods=['POST'])
def api_test_adspower_connection():
    """æµ‹è¯•AdsPowerè¿æ¥"""
    try:
        data = request.get_json() or {}
        
        # ä»æ•°æ®åº“è·å–é…ç½®ä¿¡æ¯
        configs = SystemConfig.query.all()
        config_dict = {cfg.key: cfg.value for cfg in configs}
        
        # è·å–APIé…ç½®ä¿¡æ¯
        api_host = data.get('api_host') or config_dict.get('adspower_api_host', 'localhost')
        api_port = data.get('api_port') or config_dict.get('adspower_api_port', '50325')
        api_status = config_dict.get('adspower_api_status', '')
        api_key = config_dict.get('adspower_api_key', '')
        user_id = data.get('user_id') or config_dict.get('adspower_user_id', '')
        
        if not user_id:
            return jsonify({'success': False, 'message': 'è¯·æä¾›ç”¨æˆ·ID'})
        
        # å¦‚æœAPIçŠ¶æ€ä¸ºå…³é—­ï¼Œç›´æ¥è¿”å›å¤±è´¥
        if api_status == 'å…³é—­':
            return jsonify({
                'success': False,
                'message': 'AdsPower APIæ¥å£çŠ¶æ€å·²è®¾ç½®ä¸ºå…³é—­'
            })
        
        # æµ‹è¯•AdsPower APIè¿æ¥
        
        # æ„å»ºAPI URL
        api_url = f'http://{api_host}:{api_port}'
        test_url = f"{api_url}/api/v1/user/list"
        
        # å‡†å¤‡è¯·æ±‚å¤´
        headers = {}
        if api_key:
            headers['Authorization'] = f'Bearer {api_key}'
        
        try:
            response = requests.get(test_url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                if result.get('code') == 0:
                    # æ£€æŸ¥æŒ‡å®šç”¨æˆ·æ˜¯å¦å­˜åœ¨
                    users = result.get('data', {}).get('list', [])
                    user_exists = any(user.get('user_id') == user_id for user in users)
                    
                    if user_exists:
                        return jsonify({
                            'success': True, 
                            'message': f'AdsPowerè¿æ¥æˆåŠŸï¼Œç”¨æˆ·ID {user_id} å­˜åœ¨',
                            'api_url': api_url
                        })
                    else:
                        return jsonify({
                            'success': False, 
                            'message': f'AdsPowerè¿æ¥æˆåŠŸï¼Œä½†ç”¨æˆ·ID {user_id} ä¸å­˜åœ¨ã€‚å¯ç”¨ç”¨æˆ·: {", ".join([u.get("user_id", "æœªçŸ¥") for u in users])}'
                        })
                else:
                    return jsonify({
                        'success': False, 
                        'message': f'AdsPower APIè¿”å›é”™è¯¯: {result.get("msg", "æœªçŸ¥é”™è¯¯")}'
                    })
            elif response.status_code == 401:
                return jsonify({
                    'success': False,
                    'message': 'API KeyéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®'
                })
            else:
                return jsonify({
                    'success': False,
                    'message': f'HTTPé”™è¯¯: {response.status_code}'
                })
                    
        except requests.exceptions.Timeout as e:
            return jsonify({
                'success': False,
                'message': f'è¿æ¥è¶…æ—¶: {str(e)}'
            })
        except requests.exceptions.ConnectionError as e:
            return jsonify({
                'success': False,
                'message': f'è¿æ¥å¤±è´¥: {str(e)}'
            })
        except Exception as e:
            return jsonify({
                'success': False,
                'message': f'è¯·æ±‚é”™è¯¯: {str(e)}'
            })
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}'})

@app.route('/api/test_open_adspower', methods=['POST'])
def api_test_open_adspower():
    """æµ‹è¯•æ‰“å¼€ AdsPower æµè§ˆå™¨çª—å£"""
    try:
        data = request.form.to_dict()
        
        # ä»æ•°æ®åº“è·å–é…ç½®ä¿¡æ¯
        configs = SystemConfig.query.all()
        config_dict = {cfg.key: cfg.value for cfg in configs}
        
        # è·å–APIé…ç½®ä¿¡æ¯
        api_host = data.get('api_host') or config_dict.get('adspower_api_host', 'localhost')
        api_port = data.get('api_port') or config_dict.get('adspower_api_port', '50325')
        api_status = config_dict.get('adspower_api_status', '')
        api_key = config_dict.get('adspower_api_key', '')
        user_id = data.get('user_id') or config_dict.get('adspower_user_id', '')
        
        if not user_id:
            return jsonify({'success': False, 'message': 'è¯·æä¾›ç”¨æˆ·ID'})
        
        if api_status == 'å…³é—­':
            return jsonify({
                'success': False,
                'message': 'AdsPower APIæ¥å£çŠ¶æ€å·²è®¾ç½®ä¸ºå…³é—­'
            })
        
        # åˆ›å»º AdsPowerLauncher å®ä¾‹
        launcher_config = {
            'local_api_url': f'http://{api_host}:{api_port}',
            'user_id': user_id,
            'api_status': api_status,
            'api_key': api_key
        }
        launcher = AdsPowerLauncher(launcher_config)
        
        # è°ƒç”¨ start_browser æ–¹æ³•
        browser_info = launcher.start_browser(user_id=user_id)
        
        if browser_info:
            return jsonify({
                'success': True,
                'message': 'AdsPower æµè§ˆå™¨çª—å£æ‰“å¼€æˆåŠŸ',
                'browser_info': browser_info
            })
        else:
            return jsonify({
                'success': False,
                'message': 'æ‰“å¼€æµè§ˆå™¨å¤±è´¥'
            })
    except Exception as e:
        app.logger.error(f'Error in api_test_open_adspower: {str(e)}', exc_info=True)
        return jsonify({
            'success': False,
            'message': f'æ‰“å¼€ AdsPower å¤±è´¥: {str(e)}'
        })

# é¡µé¢ç»“æ„åˆ†æç›¸å…³API
@app.route('/page-analyzer')
def page_analyzer():
    """é¡µé¢åˆ†æå™¨"""
    return render_template('page_analyzer.html')

@app.route('/enhanced-scraping')
def enhanced_scraping():
    """å¢å¼ºæ¨æ–‡æŠ“å–é¡µé¢"""
    return render_template('enhanced_scraping.html')

@app.route('/api/analyze-page-structure', methods=['POST'])
def api_analyze_page_structure():
    """åˆ†æé¡µé¢ç»“æ„API"""
    try:
        data = request.get_json()
        url = data.get('url')
        page_type = data.get('page_type', 'auto')
        
        if not url:
            return jsonify({'success': False, 'error': 'è¯·æä¾›ç›®æ ‡URL'}), 400
        
        # å¯åŠ¨æµè§ˆå™¨
        browser_manager = AdsPowerLauncher(ADS_POWER_CONFIG)
        user_id = ADS_POWER_CONFIG['user_id']  # ä»é…ç½®è·å–
        
        browser_info = browser_manager.start_browser(user_id)
        if not browser_info:
            return jsonify({'success': False, 'error': 'æµè§ˆå™¨å¯åŠ¨å¤±è´¥'}), 500
        
        debug_port = browser_info.get('ws', {}).get('puppeteer')
        
        # åˆ›å»ºé¡µé¢ç»“æ„åˆ†æå™¨
        analyzer = PageStructureAnalyzer(debug_port)
        
        # åˆ†æé¡µé¢ç»“æ„
        def run_analysis():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(analyzer.analyze_page_structure(url, page_type))
            finally:
                loop.close()
        
        analysis_result = run_analysis()
        
        if analysis_result:
            return jsonify({
                'success': True,
                'data': analysis_result
            })
        else:
            return jsonify({'success': False, 'error': 'é¡µé¢ç»“æ„åˆ†æå¤±è´¥'}), 500
            
    except Exception as e:
        return jsonify({'success': False, 'error': f'åˆ†æå¤±è´¥: {str(e)}'}), 500

# å…¨å±€æ™ºèƒ½é‡‡é›†ä»»åŠ¡ç®¡ç†
intelligent_scraping_tasks = {}

# å¢å¼ºæ¨æ–‡æŠ“å–ä»»åŠ¡ç®¡ç†
enhanced_scraping_tasks = {}

@app.route('/api/start-intelligent-scraping', methods=['POST'])
def api_start_intelligent_scraping():
    """å¯åŠ¨æ™ºèƒ½é‡‡é›†API"""
    try:
        data = request.get_json()
        analysis = data.get('analysis')
        config = data.get('config', {})
        
        if not analysis:
            return jsonify({'success': False, 'error': 'è¯·å…ˆåˆ†æé¡µé¢ç»“æ„'}), 400
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"intelligent_scraping_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        # å¯åŠ¨æµè§ˆå™¨
        browser_manager = AdsPowerLauncher(ADS_POWER_CONFIG)
        user_id = ADS_POWER_CONFIG['user_id']  # ä»é…ç½®è·å–
        
        browser_info = browser_manager.start_browser(user_id)
        if not browser_info:
            return jsonify({'success': False, 'error': 'æµè§ˆå™¨å¯åŠ¨å¤±è´¥'}), 500
        
        debug_port = browser_info.get('ws', {}).get('puppeteer')
        
        # åˆ›å»ºæ™ºèƒ½é‡‡é›†å™¨
        scraper = IntelligentScraper(debug_port)
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        intelligent_scraping_tasks[task_id] = {
            'status': 'running',
            'collected_count': 0,
            'target_count': config.get('max_items', 50),
            'latest_data': [],
            'error': None,
            'scraper': scraper
        }
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œæ™ºèƒ½é‡‡é›†
        def run_intelligent_scraping():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # æ‰§è¡Œæ™ºèƒ½é‡‡é›†
                collected_data = loop.run_until_complete(
                    scraper.intelligent_scrape(
                        analysis['page_info']['url'],
                        analysis,
                        config
                    )
                )
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                intelligent_scraping_tasks[task_id].update({
                    'status': 'completed',
                    'collected_count': len(collected_data),
                    'latest_data': collected_data[-10:] if len(collected_data) > 10 else collected_data
                })
                
            except Exception as e:
                intelligent_scraping_tasks[task_id].update({
                    'status': 'failed',
                    'error': str(e)
                })
            finally:
                loop.close()
        
        # å¯åŠ¨é‡‡é›†çº¿ç¨‹
        scraping_thread = threading.Thread(target=run_intelligent_scraping)
        scraping_thread.start()
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'æ™ºèƒ½é‡‡é›†å·²å¯åŠ¨'
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': f'å¯åŠ¨æ™ºèƒ½é‡‡é›†å¤±è´¥: {str(e)}'}), 500

@app.route('/api/scraping-progress/<task_id>', methods=['GET'])
def api_get_scraping_progress(task_id):
    """è·å–æ™ºèƒ½é‡‡é›†è¿›åº¦API"""
    try:
        if task_id not in intelligent_scraping_tasks:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
        
        task_info = intelligent_scraping_tasks[task_id]
        
        # å¦‚æœæœ‰æ–°æ•°æ®ï¼Œæ¨¡æ‹Ÿå®æ—¶æ›´æ–°
        if task_info['status'] == 'running' and task_info['scraper']:
            # è¿™é‡Œå¯ä»¥ä»é‡‡é›†å™¨è·å–å®æ—¶è¿›åº¦
            # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            pass
        
        return jsonify({
            'success': True,
            'data': {
                'status': task_info['status'],
                'collected_count': task_info['collected_count'],
                'target_count': task_info['target_count'],
                'latest_data': task_info['latest_data'],
                'error': task_info.get('error')
            }
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': f'è·å–è¿›åº¦å¤±è´¥: {str(e)}'}), 500

@app.route('/api/start-optimized-scraping', methods=['POST'])
def api_start_optimized_scraping():
    """å¯åŠ¨ä¼˜åŒ–æŠ“å–API - æ”¯æŒå¤šçª—å£å¹¶å‘å’Œå®æ—¶æ•°æ®ä¿å­˜"""
    try:
        data = request.get_json()
        target_accounts = data.get('target_accounts', [])
        target_keywords = data.get('target_keywords', [])
        max_tweets = data.get('max_tweets', 20)
        max_windows = data.get('max_windows', 2)
        
        if not target_accounts and not target_keywords:
            return jsonify({
                'success': False, 
                'error': 'è¯·è‡³å°‘æä¾›ä¸€ä¸ªç›®æ ‡è´¦å·æˆ–å…³é”®è¯'
            }), 400
        
        # ä½¿ç”¨ä¼˜åŒ–æŠ“å–å™¨å¯åŠ¨ä»»åŠ¡
        task_id = f"optimized_{int(datetime.now().timestamp())}"
        
        def run_optimized_scraping():
            try:
                with app.app_context():
                    # åˆ›å»ºä»»åŠ¡è®°å½•
                    task = ScrapingTask(
                        name=f"ä¼˜åŒ–æŠ“å–ä»»åŠ¡_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        target_accounts=json.dumps(target_accounts),
                        target_keywords=json.dumps(target_keywords),
                        max_tweets=max_tweets,
                        status='running',
                        started_at=datetime.utcnow()
                    )
                    db.session.add(task)
                    db.session.commit()
                    
                    # å¯åŠ¨ä¼˜åŒ–æŠ“å–
                    results = optimized_scraper.scrape_multiple_accounts(
                        accounts=target_accounts,
                        keywords=target_keywords,
                        max_tweets_per_account=max_tweets,
                        max_windows=max_windows,
                        task_id=task.id
                    )
                    
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    task.status = 'completed'
                    task.completed_at = datetime.utcnow()
                    task.result_count = len(results)
                    db.session.commit()
                    
                    print(f"âœ… ä¼˜åŒ–æŠ“å–ä»»åŠ¡å®Œæˆï¼Œå…±æŠ“å– {len(results)} æ¡æ¨æ–‡")
                    
            except Exception as e:
                print(f"âŒ ä¼˜åŒ–æŠ“å–ä»»åŠ¡å¤±è´¥: {e}")
                with app.app_context():
                    task = ScrapingTask.query.filter_by(name__like=f"%{task_id}%").first()
                    if task:
                        task.status = 'failed'
                        task.error_message = str(e)
                        task.completed_at = datetime.utcnow()
                        db.session.commit()
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œ
        thread = threading.Thread(target=run_optimized_scraping)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'message': 'ä¼˜åŒ–æŠ“å–ä»»åŠ¡å·²å¯åŠ¨',
            'task_id': task_id,
            'accounts': target_accounts,
            'keywords': target_keywords,
            'max_tweets': max_tweets,
            'max_windows': max_windows
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'å¯åŠ¨ä¼˜åŒ–æŠ“å–å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/start-enhanced-scraping', methods=['POST'])
def api_start_enhanced_scraping():
    """å¯åŠ¨å¢å¼ºæ¨æ–‡æŠ“å–API"""
    try:
        data = request.get_json()
        target_accounts = data.get('target_accounts', [])
        target_keywords = data.get('target_keywords', [])
        max_tweets = data.get('max_tweets', 20)
        enable_details = data.get('enable_details', True)
        task_name = data.get('task_name', f'å¢å¼ºæŠ“å–_{datetime.utcnow().strftime("%Y%m%d_%H%M%S")}')
        
        if not target_accounts and not target_keywords:
            return jsonify({'success': False, 'error': 'è¯·è‡³å°‘æŒ‡å®šä¸€ä¸ªç›®æ ‡è´¦å·æˆ–å…³é”®è¯'}), 400
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"enhanced_scraping_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        # ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡åç§°æˆ–é»˜è®¤åç§°
        if not task_name.strip():
            task_name = f'å¢å¼ºæŠ“å–_{datetime.utcnow().strftime("%Y%m%d_%H%M%S")}'
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        enhanced_scraping_tasks[task_id] = {
            'status': 'running',
            'collected_count': 0,
            'target_count': max_tweets,
            'details_scraped': 0,
            'latest_data': [],
            'error': None,
            'config': {
                'target_accounts': target_accounts,
                'target_keywords': target_keywords,
                'max_tweets': max_tweets,
                'enable_details': enable_details
            }
        }
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¢å¼ºæŠ“å–
        def run_enhanced_scraping():
            import time  # ç¡®ä¿timeæ¨¡å—åœ¨å‡½æ•°å†…éƒ¨å¯ç”¨
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # å¯åŠ¨æµè§ˆå™¨
                browser_manager = AdsPowerLauncher(ADS_POWER_CONFIG)
                user_id = ADS_POWER_CONFIG['user_id']  # ä»é…ç½®è·å–
                
                browser_info = browser_manager.start_browser(user_id)
                if not browser_info:
                    raise Exception('æµè§ˆå™¨å¯åŠ¨å¤±è´¥')
                
                debug_port = browser_info.get('ws', {}).get('puppeteer')
                
                # åˆ›å»ºå¢å¼ºTwitterè§£æå™¨
                from enhanced_twitter_parser import EnhancedTwitterParser
                # from optimized_scraping_engine import OptimizedScrapingEngine
                
                # åˆ›å»ºæŠ“å–å¼•æ“
                # scraping_engine = OptimizedScrapingEngine(max_workers=4)
                # scraping_engine.start_engine()  # å¯åŠ¨æŠ“å–å¼•æ“
                
                # åˆ›å»ºå¢å¼ºè§£æå™¨
                window_id = f"window_{user_id}_{int(time.time())}"
                parser = EnhancedTwitterParser(user_id, window_id)
                
                # åˆå§‹åŒ–è§£æå™¨ï¼ˆè¿æ¥åˆ°æµè§ˆå™¨ï¼‰
                loop.run_until_complete(parser.initialize_with_debug_port(debug_port))
                
                collected_tweets = []
                details_scraped = 0
                
                # æŠ“å–ç”¨æˆ·æ¨æ–‡
                for account in target_accounts:
                    if len(collected_tweets) >= max_tweets:
                        break
                    
                    try:
                        # æ£€æµ‹è´¦å·ç±»å‹
                        account_type = detect_account_type(account)
                        
                        # å¯¼èˆªåˆ°ç”¨æˆ·é¡µé¢å¹¶è·å–ç”¨æˆ·ä¿¡æ¯
                        loop.run_until_complete(parser.navigate_to_profile(account))
                        
                        # ä½¿ç”¨å¢å¼ºæŠ“å–æ–¹æ³•ï¼ˆæ¯æ¬¡æ»šåŠ¨åç«‹å³æŠ“å–å’Œä¿å­˜ï¼‰
                        user_tweets = loop.run_until_complete(
                            parser.enhanced_scrape_user_tweets(
                                username=account,
                                max_tweets=min(max_tweets - len(collected_tweets), 10),
                                enable_enhanced=enable_details
                            )
                        )
                        
                        # å¯¹ç”¨æˆ·æ¨æ–‡è¿›è¡Œæ™ºèƒ½è¯¦æƒ…æŠ“å–
                        if enable_details:
                            enhanced_user_tweets = []
                            for tweet in user_tweets:
                                if (details_scraped < max_tweets // 2 and 
                                    tweet.get('link') and 
                                    parser.should_scrape_details(tweet, account_type)):
                                    
                                    try:
                                        details = loop.run_until_complete(
                                            parser.scrape_tweet_details(tweet['link'])
                                        )
                                        tweet.update(details)
                                        if tweet.get('has_detailed_content'):
                                            details_scraped += 1
                                    except Exception as e:
                                        tweet['detail_error'] = str(e)
                                
                                tweet['source'] = f'ç”¨æˆ·:{account}'
                                tweet['account_type'] = account_type
                                enhanced_user_tweets.append(tweet)
                            
                            collected_tweets.extend(enhanced_user_tweets)
                        else:
                            for tweet in user_tweets:
                                tweet['source'] = f'ç”¨æˆ·:{account}'
                                tweet['account_type'] = account_type
                            collected_tweets.extend(user_tweets)
                        
                        # æ›´æ–°è¿›åº¦
                        enhanced_scraping_tasks[task_id].update({
                            'collected_count': len(collected_tweets),
                            'details_scraped': details_scraped,
                            'latest_data': collected_tweets[-5:] if len(collected_tweets) > 5 else collected_tweets
                        })
                        
                    except Exception as e:
                        continue
                
                # æŠ“å–å…³é”®è¯æ¨æ–‡
                for keyword in target_keywords:
                    if len(collected_tweets) >= max_tweets:
                        break
                    
                    try:
                        # ä½¿ç”¨å¢å¼ºå…³é”®è¯æŠ“å–æ–¹æ³•ï¼ˆæ¯æ¬¡æ»šåŠ¨åç«‹å³æŠ“å–å’Œä¿å­˜ï¼‰
                        keyword_tweets = loop.run_until_complete(
                            parser.enhanced_scrape_keyword_tweets(
                                keyword=keyword,
                                max_tweets=min(max_tweets - len(collected_tweets), 10),
                                enable_enhanced=enable_details
                            )
                        )
                        
                        # å¯¹å…³é”®è¯æ¨æ–‡è¿›è¡Œè¯¦æƒ…æŠ“å–
                        if enable_details:
                            enhanced_keyword_tweets = []
                            for tweet in keyword_tweets:
                                if (details_scraped < max_tweets // 3 and 
                                    tweet.get('link') and 
                                    parser.should_scrape_details(tweet, 'general')):
                                    
                                    try:
                                        details = loop.run_until_complete(
                                            parser.scrape_tweet_details(tweet['link'])
                                        )
                                        tweet.update(details)
                                        if tweet.get('has_detailed_content'):
                                            details_scraped += 1
                                    except Exception as e:
                                        tweet['detail_error'] = str(e)
                                
                                tweet['source'] = f'å…³é”®è¯:{keyword}'
                                enhanced_keyword_tweets.append(tweet)
                            
                            collected_tweets.extend(enhanced_keyword_tweets)
                        else:
                            for tweet in keyword_tweets:
                                tweet['source'] = f'å…³é”®è¯:{keyword}'
                            collected_tweets.extend(keyword_tweets)
                        
                        # æ›´æ–°è¿›åº¦
                        enhanced_scraping_tasks[task_id].update({
                            'collected_count': len(collected_tweets),
                            'details_scraped': details_scraped,
                            'latest_data': collected_tweets[-5:] if len(collected_tweets) > 5 else collected_tweets
                        })
                        
                    except Exception as e:
                        continue
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                if collected_tweets:
                    # åˆ›å»ºæ–°ä»»åŠ¡è®°å½•
                    task = ScrapingTask(
                        name=task_name,
                        target_accounts=json.dumps(target_accounts),
                        target_keywords=json.dumps(target_keywords),
                        max_tweets=max_tweets,
                        status='completed',
                        result_count=len(collected_tweets)
                    )
                    db.session.add(task)
                    db.session.commit()
                    
                    # ä¿å­˜æ¨æ–‡æ•°æ®
                    saved_count = _save_tweets_to_db(collected_tweets, task.id)
                    
                    enhanced_scraping_tasks[task_id].update({
                        'status': 'completed',
                        'task_db_id': task.id,
                        'saved_count': saved_count
                    })
                else:
                    enhanced_scraping_tasks[task_id]['status'] = 'completed'
                
                # å…³é—­è§£æå™¨
                loop.run_until_complete(parser.close())
                
            except Exception as e:
                enhanced_scraping_tasks[task_id].update({
                    'status': 'failed',
                    'error': str(e)
                })
            finally:
                # åœæ­¢æŠ“å–å¼•æ“
                # try:
                #     scraping_engine.stop_engine()
                # except:
                #     pass
                loop.close()
        
        # å¯åŠ¨æŠ“å–çº¿ç¨‹
        scraping_thread = threading.Thread(target=run_enhanced_scraping)
        scraping_thread.start()
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'å¢å¼ºæ¨æ–‡æŠ“å–å·²å¯åŠ¨'
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': f'å¯åŠ¨å¢å¼ºæŠ“å–å¤±è´¥: {str(e)}'}), 500

@app.route('/api/enhanced-scraping-progress/<task_id>', methods=['GET'])
def api_get_enhanced_scraping_progress(task_id):
    """è·å–å¢å¼ºæ¨æ–‡æŠ“å–è¿›åº¦API"""
    try:
        if task_id not in enhanced_scraping_tasks:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
        
        task_info = enhanced_scraping_tasks[task_id]
        
        return jsonify({
            'success': True,
            'data': {
                'status': task_info['status'],
                'collected_count': task_info['collected_count'],
                'target_count': task_info['target_count'],
                'details_scraped': task_info.get('details_scraped', 0),
                'latest_data': task_info['latest_data'],
                'error': task_info.get('error'),
                'config': task_info.get('config', {}),
                'task_db_id': task_info.get('task_db_id'),
                'saved_count': task_info.get('saved_count')
            }
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': f'è·å–è¿›åº¦å¤±è´¥: {str(e)}'}), 500

# åšä¸»ç®¡ç†ç›¸å…³API
@app.route('/api/influencers', methods=['GET'])
def api_get_influencers():
    """è·å–åšä¸»åˆ—è¡¨API"""
    try:
        page = request.args.get('page', 1, type=int)
        per_page = request.args.get('per_page', 20, type=int)
        category = request.args.get('category')
        search = request.args.get('search')
        
        query = TwitterInfluencer.query
        
        # åˆ†ç±»ç­›é€‰
        if category:
            query = query.filter(TwitterInfluencer.category == category)
        
        # æœç´¢ç­›é€‰
        if search:
            query = query.filter(
                db.or_(
                    TwitterInfluencer.name.contains(search),
                    TwitterInfluencer.username.contains(search),
                    TwitterInfluencer.description.contains(search)
                )
            )
        
        # åˆ†é¡µ
        pagination = query.order_by(TwitterInfluencer.created_at.desc()).paginate(
            page=page, per_page=per_page, error_out=False
        )
        
        return jsonify({
            'success': True,
            'data': {
                'influencers': [influencer.to_dict() for influencer in pagination.items],
                'total': pagination.total,
                'pages': pagination.pages,
                'current_page': page,
                'per_page': per_page
            }
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': f'è·å–åšä¸»åˆ—è¡¨å¤±è´¥: {str(e)}'}), 500

@app.route('/api/influencers', methods=['POST'])
def api_add_influencer():
    """æ·»åŠ åšä¸»API"""
    try:
        data = request.get_json()
        
        # éªŒè¯å¿…å¡«å­—æ®µ
        required_fields = ['name', 'profile_url']
        for field in required_fields:
            if not data.get(field):
                return jsonify({'success': False, 'error': f'ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}'}), 400
        
        profile_url = data['profile_url'].strip()
        
        # ä»URLæå–ç”¨æˆ·å
        username = ''
        if 'x.com/' in profile_url or 'twitter.com/' in profile_url:
            try:
                username = profile_url.split('/')[-1].split('?')[0]
                if username.startswith('@'):
                    username = username[1:]
            except:
                pass
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = TwitterInfluencer.query.filter(
            db.or_(
                TwitterInfluencer.profile_url == profile_url,
                TwitterInfluencer.username == username
            )
        ).first()
        
        if existing:
            return jsonify({'success': False, 'error': 'è¯¥åšä¸»å·²å­˜åœ¨'}), 400
        
        # åˆ›å»ºæ–°åšä¸»
        influencer = TwitterInfluencer(
            name=data['name'].strip(),
            username=username,
            profile_url=profile_url,
            description=data.get('description', '').strip(),
            category=data.get('category', 'å…¶ä»–'),
            followers_count=data.get('followers_count', 0)
        )
        
        db.session.add(influencer)
        db.session.commit()
        
        return jsonify({
            'success': True,
            'data': influencer.to_dict(),
            'message': 'åšä¸»æ·»åŠ æˆåŠŸ'
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'success': False, 'error': f'æ·»åŠ åšä¸»å¤±è´¥: {str(e)}'}), 500

@app.route('/api/influencers/<int:influencer_id>', methods=['PUT'])
def api_update_influencer(influencer_id):
    """æ›´æ–°åšä¸»ä¿¡æ¯API"""
    try:
        influencer = TwitterInfluencer.query.get(influencer_id)
        if not influencer:
            return jsonify({'success': False, 'error': 'åšä¸»ä¸å­˜åœ¨'}), 404
        
        data = request.get_json()
        
        # æ›´æ–°å­—æ®µ
        if 'name' in data:
            influencer.name = data['name'].strip()
        if 'description' in data:
            influencer.description = data['description'].strip()
        if 'category' in data:
            influencer.category = data['category']
        if 'followers_count' in data:
            influencer.followers_count = data['followers_count']
        if 'is_active' in data:
            influencer.is_active = data['is_active']
        
        influencer.updated_at = datetime.utcnow()
        db.session.commit()
        
        return jsonify({
            'success': True,
            'data': influencer.to_dict(),
            'message': 'åšä¸»ä¿¡æ¯æ›´æ–°æˆåŠŸ'
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'success': False, 'error': f'æ›´æ–°åšä¸»å¤±è´¥: {str(e)}'}), 500

@app.route('/api/influencers/<int:influencer_id>', methods=['GET'])
def api_get_influencer(influencer_id):
    """è·å–å•ä¸ªåšä¸»ä¿¡æ¯API"""
    try:
        influencer = TwitterInfluencer.query.get(influencer_id)
        if not influencer:
            return jsonify({'success': False, 'error': 'åšä¸»ä¸å­˜åœ¨'}), 404
        
        return jsonify({
            'success': True,
            'data': influencer.to_dict()
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': f'è·å–åšä¸»å¤±è´¥: {str(e)}'}), 500

@app.route('/api/influencers/batch-scrape', methods=['POST'])
def api_batch_scrape_influencers():
    """æ‰¹é‡æŠ“å–åšä¸»æ¨æ–‡API"""
    try:
        data = request.get_json()
        influencer_ids = data.get('influencer_ids', [])
        
        if not influencer_ids:
            return jsonify({'success': False, 'error': 'è¯·é€‰æ‹©è¦æŠ“å–çš„åšä¸»'}), 400
        
        # éªŒè¯åšä¸»æ˜¯å¦å­˜åœ¨
        influencers = TwitterInfluencer.query.filter(TwitterInfluencer.id.in_(influencer_ids)).all()
        if len(influencers) != len(influencer_ids):
            return jsonify({'success': False, 'error': 'éƒ¨åˆ†åšä¸»ä¸å­˜åœ¨'}), 400
        
        # åˆ›å»ºæ‰¹é‡æŠ“å–ä»»åŠ¡
        task_name = f"æ‰¹é‡æŠ“å–åšä¸»æ¨æ–‡ - {len(influencers)}ä¸ªåšä¸»"
        target_accounts = [inf.username for inf in influencers if inf.username]
        
        if not target_accounts:
            return jsonify({'success': False, 'error': 'é€‰ä¸­çš„åšä¸»æ²¡æœ‰æœ‰æ•ˆçš„ç”¨æˆ·å'}), 400
        
        # åˆ›å»ºæŠ“å–ä»»åŠ¡
        task = ScrapingTask(
            name=task_name,
            target_accounts=json.dumps(target_accounts),
            target_keywords=json.dumps([]),
            max_tweets=data.get('max_tweets', 50),
            min_likes=data.get('min_likes', 0),
            min_retweets=data.get('min_retweets', 0),
            min_comments=data.get('min_comments', 0),
            status='pending'
        )
        
        db.session.add(task)
        db.session.commit()
        
        # å¯åŠ¨å¼‚æ­¥æŠ“å–ä»»åŠ¡
        def run_batch_scraping():
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(task_executor.execute_task(task.id))
                
                # æ›´æ–°åšä¸»çš„æœ€åæŠ“å–æ—¶é—´
                for influencer in influencers:
                    influencer.last_scraped = datetime.utcnow()
                db.session.commit()
                
            except Exception as e:
                pass
            finally:
                loop.close()
        
        # å¯åŠ¨æŠ“å–çº¿ç¨‹
        import threading
        scraping_thread = threading.Thread(target=run_batch_scraping)
        scraping_thread.start()
        
        return jsonify({
            'success': True,
            'task_id': task.id,
            'message': f'å·²å¯åŠ¨æ‰¹é‡æŠ“å–ä»»åŠ¡ï¼Œå°†æŠ“å– {len(influencers)} ä¸ªåšä¸»çš„æ¨æ–‡'
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'success': False, 'error': f'å¯åŠ¨æ‰¹é‡æŠ“å–å¤±è´¥: {str(e)}'}), 500

@app.route('/api/influencers/<int:influencer_id>', methods=['DELETE'])
def api_delete_influencer(influencer_id):
    """åˆ é™¤åšä¸»API"""
    try:
        influencer = TwitterInfluencer.query.get(influencer_id)
        if not influencer:
            return jsonify({'success': False, 'error': 'åšä¸»ä¸å­˜åœ¨'}), 404
        
        db.session.delete(influencer)
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': 'åšä¸»åˆ é™¤æˆåŠŸ'
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'success': False, 'error': f'åˆ é™¤åšä¸»å¤±è´¥: {str(e)}'}), 500

@app.route('/api/influencers/<int:influencer_id>/toggle-status', methods=['PATCH'])
def api_toggle_influencer_status(influencer_id):
    """åˆ‡æ¢åšä¸»çŠ¶æ€API"""
    try:
        influencer = TwitterInfluencer.query.get(influencer_id)
        if not influencer:
            return jsonify({'success': False, 'error': 'åšä¸»ä¸å­˜åœ¨'}), 404
        
        # åˆ‡æ¢çŠ¶æ€
        influencer.is_active = not influencer.is_active
        influencer.updated_at = datetime.utcnow()
        db.session.commit()
        
        status_text = 'å¯ç”¨' if influencer.is_active else 'ç¦ç”¨'
        
        return jsonify({
            'success': True,
            'data': influencer.to_dict(),
            'message': f'åšä¸»å·²{status_text}'
        })
        
    except Exception as e:
        db.session.rollback()
        return jsonify({'success': False, 'error': f'åˆ‡æ¢çŠ¶æ€å¤±è´¥: {str(e)}'}), 500

@app.route('/api/influencers/stats', methods=['GET'])
def api_get_influencer_stats():
    """è·å–åšä¸»ç»Ÿè®¡æ•°æ®API"""
    try:
        from datetime import datetime, timedelta
        import json
        
        # ä»ä»»åŠ¡ä¸­æå–æ‰€æœ‰ä½¿ç”¨è¿‡çš„åšä¸»
        all_tasks = ScrapingTask.query.all()
        task_influencers = set()
        
        for task in all_tasks:
            if task.target_accounts:
                try:
                    accounts = json.loads(task.target_accounts)
                    for account in accounts:
                        # æ¸…ç†ç”¨æˆ·åï¼Œå»é™¤@ç¬¦å·
                        clean_username = account.lstrip('@') if account.startswith('@') else account
                        task_influencers.add(clean_username.lower())
                except:
                    continue
        
        # æ€»åšä¸»æ•°ï¼ˆä»»åŠ¡ä¸­ä½¿ç”¨çš„åšä¸»æ•°é‡ï¼‰
        total_influencers = len(task_influencers)
        
        # TwitterInfluencerè¡¨ä¸­çš„åšä¸»æ•°
        managed_influencers = TwitterInfluencer.query.count()
        
        # å¯ç”¨åšä¸»æ•°ï¼ˆTwitterInfluencerè¡¨ä¸­å¯ç”¨çš„ï¼‰
        active_influencers = TwitterInfluencer.query.filter(TwitterInfluencer.is_active == True).count()
        
        # åˆ†ç±»æ•°é‡ï¼ˆæœ‰åšä¸»çš„åˆ†ç±»ï¼‰
        categories_with_influencers = db.session.query(TwitterInfluencer.category).filter(
            TwitterInfluencer.category.isnot(None)
        ).distinct().count()
        
        # ä»Šæ—¥æŠ“å–æ•°é‡ï¼ˆæœ€è¿‘24å°æ—¶å†…æœ‰ä»»åŠ¡çš„åšä¸»æ•°ï¼‰
        today_start = datetime.now() - timedelta(days=1)
        recent_tasks = ScrapingTask.query.filter(
            ScrapingTask.created_at >= today_start
        ).all()
        
        scraped_today = set()
        for task in recent_tasks:
            if task.target_accounts:
                try:
                    accounts = json.loads(task.target_accounts)
                    for account in accounts:
                        clean_username = account.lstrip('@') if account.startswith('@') else account
                        scraped_today.add(clean_username.lower())
                except:
                    continue
        
        return jsonify({
            'success': True,
            'data': {
                'total_influencers': total_influencers,  # ä»»åŠ¡ä¸­ä½¿ç”¨çš„åšä¸»æ€»æ•°
                'active_influencers': active_influencers,  # ç®¡ç†è¡¨ä¸­å¯ç”¨çš„åšä¸»æ•°
                'managed_influencers': managed_influencers,  # ç®¡ç†è¡¨ä¸­çš„åšä¸»æ€»æ•°
                'total_categories': categories_with_influencers,
                'scraped_today': len(scraped_today)  # ä»Šæ—¥ä»»åŠ¡æ¶‰åŠçš„åšä¸»æ•°
            }
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': f'è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}'}), 500

@app.route('/api/influencers/categories', methods=['GET'])
def api_get_influencer_categories():
    """è·å–åšä¸»åˆ†ç±»åˆ—è¡¨API"""
    try:
        categories = ['æé’±', 'æŠ•æ”¾', 'å‰¯ä¸šå¹²è´§', 'æƒ…ç»ªç±»', 'å…¶ä»–']
        
        # ç»Ÿè®¡æ¯ä¸ªåˆ†ç±»çš„åšä¸»æ•°é‡
        category_stats = []
        for category in categories:
            count = TwitterInfluencer.query.filter(TwitterInfluencer.category == category).count()
            category_stats.append({
                'name': category,
                'count': count
            })
        
        return jsonify({
            'success': True,
            'data': category_stats
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': f'è·å–åˆ†ç±»å¤±è´¥: {str(e)}'}), 500

# åˆå§‹åŒ–æ•°æ®åº“
def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    with app.app_context():
        db.create_all()
        
        # é‡ç½®æ‰€æœ‰runningçŠ¶æ€çš„ä»»åŠ¡ä¸ºpendingçŠ¶æ€
        # è¿™æ˜¯ä¸ºäº†è§£å†³ç³»ç»Ÿé‡å¯åä»»åŠ¡çŠ¶æ€ä¸ä¸€è‡´çš„é—®é¢˜
        try:
            running_tasks = ScrapingTask.query.filter_by(status='running').all()
            if running_tasks:
                for task in running_tasks:
                    task.status = 'pending'
                db.session.commit()
        except Exception as e:
            print(f"âš ï¸ é‡ç½®ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        
        # ä»æ•°æ®åº“åŠ è½½é…ç½®
        try:
            load_config_from_database()
        except Exception as e:
            print(f"âš ï¸ é…ç½®åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        # åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨ï¼ˆåœ¨é…ç½®åŠ è½½åï¼‰
        try:
            init_task_manager()
        except Exception as e:
            print(f"âš ï¸ ä»»åŠ¡ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

@app.route('/debug-adspower')
def debug_adspower():
    """AdsPowerè°ƒè¯•é¡µé¢"""
    with open('debug_adspower.html', 'r', encoding='utf-8') as f:
        return f.read()

# ç³»ç»Ÿç®¡ç†APIç«¯ç‚¹
@app.route('/api/backup-database', methods=['POST'])
def api_backup_database():
    """å¤‡ä»½æ•°æ®åº“API"""
    try:
        import shutil
        from datetime import datetime
        
        # è·å–æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        db_path = app.config['SQLALCHEMY_DATABASE_URI'].replace('sqlite:///', '')
        if not os.path.exists(db_path):
            return jsonify({'success': False, 'error': 'æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_dir = './backups'
        os.makedirs(backup_dir, exist_ok=True)
        
        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_filename = f'twitter_scraper_backup_{timestamp}.db'
        backup_path = os.path.join(backup_dir, backup_filename)
        
        # å¤åˆ¶æ•°æ®åº“æ–‡ä»¶
        shutil.copy2(db_path, backup_path)
        
        return jsonify({
            'success': True,
            'message': f'æ•°æ®åº“å¤‡ä»½æˆåŠŸï¼Œæ–‡ä»¶ä¿å­˜ä¸º: {backup_filename}',
            'backup_path': backup_path
        })
        
    except Exception as e:
        app.logger.error(f"æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'å¤‡ä»½å¤±è´¥: {str(e)}'}), 500

@app.route('/api/clean-expired-data', methods=['POST'])
def api_clean_expired_data():
    """æ¸…ç†è¿‡æœŸæ•°æ®API"""
    try:
        from datetime import datetime, timedelta
        
        # æ¸…ç†30å¤©å‰çš„æ¨æ–‡æ•°æ®
        cutoff_date = datetime.now() - timedelta(days=30)
        expired_tweets = TweetData.query.filter(TweetData.scraped_at < cutoff_date).all()
        count = len(expired_tweets)
        
        for tweet in expired_tweets:
            db.session.delete(tweet)
        
        # æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡ï¼ˆä¿ç•™æœ€è¿‘7å¤©ï¼‰
        task_cutoff_date = datetime.now() - timedelta(days=7)
        expired_tasks = ScrapingTask.query.filter(
            ScrapingTask.status.in_(['completed', 'failed']),
            ScrapingTask.created_at < task_cutoff_date
        ).all()
        
        task_count = len(expired_tasks)
        for task in expired_tasks:
            db.session.delete(task)
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'count': count + task_count,
            'message': f'æ¸…ç†å®Œæˆï¼šåˆ é™¤äº† {count} æ¡æ¨æ–‡æ•°æ®å’Œ {task_count} ä¸ªè¿‡æœŸä»»åŠ¡'
        })
        
    except Exception as e:
        db.session.rollback()
        app.logger.error(f"æ¸…ç†è¿‡æœŸæ•°æ®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'æ¸…ç†å¤±è´¥: {str(e)}'}), 500

@app.route('/api/export-logs', methods=['GET'])
def api_export_logs():
    """å¯¼å‡ºæ—¥å¿—API"""
    try:
        import zipfile
        from flask import send_file
        import tempfile
        
        # åˆ›å»ºä¸´æ—¶zipæ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.zip')
        
        with zipfile.ZipFile(temp_file.name, 'w') as zipf:
            # æ·»åŠ åº”ç”¨æ—¥å¿—
            log_files = ['twitter_scraper.log', 'app.log']
            for log_file in log_files:
                if os.path.exists(log_file):
                    zipf.write(log_file, log_file)
            
            # æ·»åŠ ç³»ç»Ÿä¿¡æ¯
            import sys
            import psutil
            system_info = f"""ç³»ç»Ÿä¿¡æ¯å¯¼å‡º
æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Pythonç‰ˆæœ¬: {sys.version}
å†…å­˜ä½¿ç”¨: {psutil.virtual_memory().percent}%
ç£ç›˜ä½¿ç”¨: {psutil.disk_usage('/').percent}%
"""
            zipf.writestr('system_info.txt', system_info)
        
        return send_file(
            temp_file.name,
            as_attachment=True,
            download_name=f'twitter_scraper_logs_{datetime.now().strftime("%Y%m%d_%H%M%S")}.zip',
            mimetype='application/zip'
        )
        
    except Exception as e:
        app.logger.error(f"å¯¼å‡ºæ—¥å¿—å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'å¯¼å‡ºå¤±è´¥: {str(e)}'}), 500

@app.route('/api/restart-system', methods=['POST'])
def api_restart_system():
    """é‡å¯ç³»ç»ŸAPI"""
    try:
        # åœæ­¢æ‰€æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡
        running_tasks = ScrapingTask.query.filter_by(status='running').all()
        for task in running_tasks:
            task.status = 'pending'
        db.session.commit()
        
        # å»¶è¿Ÿé‡å¯ï¼Œç»™å‰ç«¯æ—¶é—´æ˜¾ç¤ºæ¶ˆæ¯
        def delayed_restart():
            import time
            time.sleep(3)
            os._exit(0)  # å¼ºåˆ¶é€€å‡ºï¼Œç”±è¿›ç¨‹ç®¡ç†å™¨é‡å¯
        
        import threading
        restart_thread = threading.Thread(target=delayed_restart)
        restart_thread.start()
        
        return jsonify({
            'success': True,
            'message': 'ç³»ç»Ÿå°†åœ¨3ç§’åé‡å¯...'
        })
        
    except Exception as e:
        app.logger.error(f"é‡å¯ç³»ç»Ÿå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'é‡å¯å¤±è´¥: {str(e)}'}), 500

# ç¡®ä¿åœ¨æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–
try:
    init_db()
except Exception as e:
    print(f"âš ï¸ åˆå§‹åŒ–å¤±è´¥: {e}")

if __name__ == '__main__':
    # è®°å½•åº”ç”¨å¯åŠ¨æ—¶é—´
    import time
    app.start_time = time.time()
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_db()
    
    # åˆå§‹åŒ–ä»»åŠ¡æ‰§è¡Œå™¨
    task_executor = ScrapingTaskExecutor()
    
    # å¯åŠ¨Webåº”ç”¨
    app.run(debug=True, host='0.0.0.0', port=8090)