#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆå®æ—¶è§£æéªŒè¯
ä¸“æ³¨äºéªŒè¯å®æ—¶è§£æã€å¢é‡ä¿å­˜å’Œæµ‹è¯•é€»è¾‘çš„æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional, Set
from datetime import datetime
import json
import traceback

class RealtimeParsingValidator:
    """å®æ—¶è§£æéªŒè¯å™¨ - ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½éªŒè¯"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # å®æ—¶è§£æçŠ¶æ€
        self.parsed_tweets: List[Dict[str, Any]] = []
        self.seen_tweet_ids: Set[str] = set()
        self.parsing_stats = {
            'total_scrolls': 0,
            'tweets_parsed': 0,
            'duplicates_skipped': 0,
            'parsing_errors': 0,
            'incremental_saves': 0,
            'dom_elements_found': 0,
            'dom_elements_parsed': 0,
            'realtime_parsing_events': 0
        }
        
        # æ¨¡æ‹Ÿæ•°æ®ç”¨äºéªŒè¯
        self.mock_tweet_data = self._generate_mock_tweets()
        self.incremental_save_interval = 3
    
    def _generate_mock_tweets(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ¨æ–‡æ•°æ®ç”¨äºéªŒè¯"""
        base_time = datetime.now()
        mock_tweets = []
        
        for i in range(20):
            tweet = {
                'id': f'mock_tweet_{i+1:03d}_{int(base_time.timestamp())}',
                'username': f'user_{i % 5 + 1}',
                'content': f'è¿™æ˜¯ç¬¬ {i+1} æ¡æ¨¡æ‹Ÿæ¨æ–‡å†…å®¹ï¼Œç”¨äºéªŒè¯å®æ—¶è§£æåŠŸèƒ½ã€‚åŒ…å«ä¸€äº›æµ‹è¯•æ–‡æœ¬å’Œemoji ğŸš€',
                'timestamp': (base_time.timestamp() - i * 3600),
                'likes': (i + 1) * 10,
                'retweets': (i + 1) * 2,
                'replies': i + 1,
                'mock_element_id': f'element_{i+1}'
            }
            mock_tweets.append(tweet)
        
        return mock_tweets
    
    async def simulate_realtime_scroll_parsing(self, target_tweets: int = 15, max_scrolls: int = 10) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå®æ—¶æ»šåŠ¨è§£æè¿‡ç¨‹"""
        self.logger.info(f"ğŸš€ å¼€å§‹æ¨¡æ‹Ÿå®æ—¶æ»šåŠ¨è§£æï¼Œç›®æ ‡: {target_tweets} æ¡æ¨æ–‡")
        
        scroll_count = 0
        tweets_per_scroll = [3, 2, 4, 1, 3, 2, 1, 2, 1, 1]  # æ¨¡æ‹Ÿæ¯æ¬¡æ»šåŠ¨å‘ç°çš„æ¨æ–‡æ•°
        
        while scroll_count < max_scrolls and len(self.parsed_tweets) < target_tweets:
            self.parsing_stats['total_scrolls'] = scroll_count + 1
            
            self.logger.info(f"ğŸ“Š æ¨¡æ‹Ÿæ»šåŠ¨ {scroll_count + 1}/{max_scrolls}ï¼Œå·²è§£æ: {len(self.parsed_tweets)}/{target_tweets}")
            
            # æ¨¡æ‹Ÿå‘ç°æ–°çš„æ¨æ–‡å…ƒç´ 
            new_elements_count = tweets_per_scroll[scroll_count % len(tweets_per_scroll)]
            self.parsing_stats['dom_elements_found'] += new_elements_count
            
            # æ¨¡æ‹Ÿå®æ—¶è§£ææ–°å‘ç°çš„æ¨æ–‡
            new_tweets_parsed = await self._simulate_realtime_parsing(new_elements_count, scroll_count)
            
            if new_tweets_parsed > 0:
                self.logger.info(f"âœ… æœ¬è½®å®æ—¶è§£æäº† {new_tweets_parsed} æ¡æ–°æ¨æ–‡ï¼Œæ€»è®¡: {len(self.parsed_tweets)}")
                
                # æ£€æŸ¥å¢é‡ä¿å­˜
                if len(self.parsed_tweets) % self.incremental_save_interval == 0:
                    await self._perform_incremental_save()
            
            # æ¨¡æ‹Ÿæ»šåŠ¨å»¶è¿Ÿ
            await asyncio.sleep(0.1)
            scroll_count += 1
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if len(self.parsed_tweets) >= target_tweets:
                self.logger.info(f"ğŸ¯ è¾¾åˆ°ç›®æ ‡æ¨æ–‡æ•°é‡: {len(self.parsed_tweets)}")
                break
        
        # æœ€ç»ˆä¿å­˜
        final_file = await self._perform_final_save()
        
        # ç”ŸæˆéªŒè¯ç»“æœ
        result = {
            'validation_type': 'realtime_parsing_simulation',
            'target_tweets': target_tweets,
            'parsed_tweets_count': len(self.parsed_tweets),
            'scroll_attempts': scroll_count,
            'target_reached': len(self.parsed_tweets) >= target_tweets,
            'parsing_efficiency': len(self.parsed_tweets) / max(scroll_count, 1),
            'dom_parsing_ratio': self.parsing_stats['dom_elements_parsed'] / max(self.parsing_stats['dom_elements_found'], 1),
            'parsing_stats': self.parsing_stats.copy(),
            'final_save_file': final_file,
            'quality_metrics': self._calculate_quality_metrics(),
            'realtime_features_validated': self._validate_realtime_features()
        }
        
        self.logger.info(f"ğŸ“Š å®æ—¶è§£æéªŒè¯å®Œæˆ: {len(self.parsed_tweets)} æ¡æ¨æ–‡ï¼Œ{scroll_count} æ¬¡æ»šåŠ¨")
        return result
    
    async def _simulate_realtime_parsing(self, elements_count: int, scroll_index: int) -> int:
        """æ¨¡æ‹Ÿå®æ—¶è§£ææ¨æ–‡å…ƒç´ """
        new_tweets_count = 0
        
        # ä»æ¨¡æ‹Ÿæ•°æ®ä¸­é€‰æ‹©æ¨æ–‡
        start_index = scroll_index * 2
        end_index = min(start_index + elements_count, len(self.mock_tweet_data))
        
        for i in range(start_index, end_index):
            if i >= len(self.mock_tweet_data):
                break
                
            try:
                mock_tweet = self.mock_tweet_data[i].copy()
                tweet_id = mock_tweet['id']
                
                # æ¨¡æ‹Ÿå®æ—¶è§£æè¿‡ç¨‹
                if tweet_id not in self.seen_tweet_ids:
                    # æ·»åŠ å®æ—¶è§£ææ ‡è®°
                    mock_tweet['parsed_at'] = datetime.now().isoformat()
                    mock_tweet['parsing_method'] = 'realtime_simulation'
                    mock_tweet['scroll_round'] = scroll_index + 1
                    
                    self.parsed_tweets.append(mock_tweet)
                    self.seen_tweet_ids.add(tweet_id)
                    self.parsing_stats['tweets_parsed'] += 1
                    self.parsing_stats['dom_elements_parsed'] += 1
                    self.parsing_stats['realtime_parsing_events'] += 1
                    new_tweets_count += 1
                    
                    self.logger.info(f"âœ… å®æ—¶è§£ææ–°æ¨æ–‡: @{mock_tweet['username']} - {tweet_id[:15]}... - {mock_tweet['content'][:30]}...")
                else:
                    self.parsing_stats['duplicates_skipped'] += 1
                    self.logger.debug(f"â­ï¸ è·³è¿‡é‡å¤æ¨æ–‡: {tweet_id[:15]}...")
                
            except Exception as e:
                self.parsing_stats['parsing_errors'] += 1
                self.logger.debug(f"æ¨¡æ‹Ÿè§£æå¤±è´¥: {e}")
                continue
        
        return new_tweets_count
    
    async def _perform_incremental_save(self) -> str:
        """æ‰§è¡Œå¢é‡ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'final_realtime_incremental_{timestamp}.json'
        
        incremental_data = {
            'save_type': 'incremental',
            'timestamp': datetime.now().isoformat(),
            'total_tweets': len(self.parsed_tweets),
            'parsing_stats': self.parsing_stats.copy(),
            'latest_tweets': self.parsed_tweets[-self.incremental_save_interval:] if len(self.parsed_tweets) >= self.incremental_save_interval else self.parsed_tweets,
            'validation_note': 'è¿™æ˜¯å®æ—¶è§£æéªŒè¯çš„å¢é‡ä¿å­˜'
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(incremental_data, f, ensure_ascii=False, indent=2)
            
            self.parsing_stats['incremental_saves'] += 1
            self.logger.info(f"ğŸ’¾ å¢é‡ä¿å­˜å®Œæˆ: {filename} ({len(self.parsed_tweets)} æ¡æ¨æ–‡)")
            return filename
        except Exception as e:
            self.logger.error(f"å¢é‡ä¿å­˜å¤±è´¥: {e}")
            return ""
    
    async def _perform_final_save(self) -> str:
        """æ‰§è¡Œæœ€ç»ˆä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'final_realtime_validation_{timestamp}.json'
        
        final_data = {
            'validation_type': 'final_realtime_parsing_validation',
            'timestamp': datetime.now().isoformat(),
            'total_tweets': len(self.parsed_tweets),
            'unique_tweets': len(self.seen_tweet_ids),
            'parsing_stats': self.parsing_stats.copy(),
            'quality_metrics': self._calculate_quality_metrics(),
            'realtime_features_validated': self._validate_realtime_features(),
            'all_tweets': self.parsed_tweets,
            'validation_summary': {
                'realtime_parsing_working': self.parsing_stats['realtime_parsing_events'] > 0,
                'incremental_saves_working': self.parsing_stats['incremental_saves'] > 0,
                'dom_element_handling_working': self.parsing_stats['dom_elements_parsed'] > 0,
                'duplicate_detection_working': self.parsing_stats['duplicates_skipped'] >= 0
            }
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“„ æœ€ç»ˆéªŒè¯ç»“æœå·²ä¿å­˜: {filename}")
            return filename
        except Exception as e:
            self.logger.error(f"æœ€ç»ˆä¿å­˜å¤±è´¥: {e}")
            return ""
    
    def _calculate_quality_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡"""
        if not self.parsed_tweets:
            return {}
        
        total_tweets = len(self.parsed_tweets)
        
        # å†…å®¹è´¨é‡æ£€æŸ¥
        has_content = sum(1 for t in self.parsed_tweets if t.get('content'))
        has_username = sum(1 for t in self.parsed_tweets if t.get('username'))
        has_id = sum(1 for t in self.parsed_tweets if t.get('id'))
        has_timestamp = sum(1 for t in self.parsed_tweets if t.get('timestamp'))
        has_realtime_marker = sum(1 for t in self.parsed_tweets if t.get('parsing_method') == 'realtime_simulation')
        
        # è§£ææ•ˆç‡
        total_attempts = self.parsing_stats['tweets_parsed'] + self.parsing_stats['duplicates_skipped'] + self.parsing_stats['parsing_errors']
        
        return {
            'content_completeness': has_content / total_tweets,
            'username_completeness': has_username / total_tweets,
            'id_completeness': has_id / total_tweets,
            'timestamp_completeness': has_timestamp / total_tweets,
            'realtime_marker_completeness': has_realtime_marker / total_tweets,
            'parsing_success_rate': self.parsing_stats['tweets_parsed'] / max(total_attempts, 1),
            'duplicate_rate': self.parsing_stats['duplicates_skipped'] / max(total_attempts, 1),
            'error_rate': self.parsing_stats['parsing_errors'] / max(total_attempts, 1),
            'efficiency_per_scroll': self.parsing_stats['tweets_parsed'] / max(self.parsing_stats['total_scrolls'], 1),
            'dom_parsing_efficiency': self.parsing_stats['dom_elements_parsed'] / max(self.parsing_stats['dom_elements_found'], 1),
            'realtime_parsing_efficiency': self.parsing_stats['realtime_parsing_events'] / max(self.parsing_stats['total_scrolls'], 1)
        }
    
    def _validate_realtime_features(self) -> Dict[str, Any]:
        """éªŒè¯å®æ—¶è§£æç‰¹æ€§"""
        return {
            'realtime_parsing_events': self.parsing_stats['realtime_parsing_events'],
            'incremental_saves_performed': self.parsing_stats['incremental_saves'],
            'dom_elements_processed': self.parsing_stats['dom_elements_parsed'],
            'duplicate_detection_active': self.parsing_stats['duplicates_skipped'] >= 0,
            'parsing_efficiency_acceptable': (self.parsing_stats['tweets_parsed'] / max(self.parsing_stats['total_scrolls'], 1)) > 0.5,
            'quality_metrics_available': len(self._calculate_quality_metrics()) > 0,
            'realtime_markers_present': any(t.get('parsing_method') == 'realtime_simulation' for t in self.parsed_tweets)
        }
    
    def get_validation_summary(self) -> Dict[str, Any]:
        """è·å–éªŒè¯æ‘˜è¦"""
        return {
            'total_parsed': len(self.parsed_tweets),
            'unique_tweets': len(self.seen_tweet_ids),
            'parsing_stats': self.parsing_stats.copy(),
            'quality_metrics': self._calculate_quality_metrics(),
            'realtime_features': self._validate_realtime_features()
        }


class RealtimeValidationTest:
    """å®æ—¶éªŒè¯æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.validator = RealtimeParsingValidator()
    
    async def run_comprehensive_validation(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆéªŒè¯æµ‹è¯•"""
        test_result = {
            'test_name': 'comprehensive_realtime_validation',
            'success': False,
            'validation_results': {},
            'feature_validations': {},
            'errors': []
        }
        
        try:
            self.logger.info("ğŸš€ å¼€å§‹ç»¼åˆå®æ—¶è§£æéªŒè¯...")
            
            # æ‰§è¡Œå®æ—¶è§£æéªŒè¯
            validation_result = await self.validator.simulate_realtime_scroll_parsing(
                target_tweets=12,
                max_scrolls=8
            )
            
            test_result['validation_results'] = validation_result
            
            # éªŒè¯å…³é”®ç‰¹æ€§
            feature_validations = self._validate_key_features(validation_result)
            test_result['feature_validations'] = feature_validations
            
            # åˆ¤æ–­æµ‹è¯•æˆåŠŸ
            success_criteria = [
                validation_result['parsed_tweets_count'] > 0,
                validation_result['target_reached'],
                feature_validations['realtime_parsing_validated'],
                feature_validations['incremental_saves_validated'],
                feature_validations['dom_handling_validated']
            ]
            
            test_result['success'] = all(success_criteria)
            
            if test_result['success']:
                self.logger.info(f"âœ… ç»¼åˆéªŒè¯æµ‹è¯•æˆåŠŸ: {validation_result['parsed_tweets_count']} æ¡æ¨æ–‡")
            else:
                failed_criteria = [i for i, criterion in enumerate(success_criteria) if not criterion]
                test_result['errors'].append(f"éªŒè¯å¤±è´¥çš„æ ‡å‡†: {failed_criteria}")
                self.logger.error(f"âŒ ç»¼åˆéªŒè¯æµ‹è¯•å¤±è´¥: {failed_criteria}")
            
        except Exception as e:
            test_result['errors'].append(str(e))
            test_result['errors'].append(traceback.format_exc())
            self.logger.error(f"âŒ ç»¼åˆéªŒè¯æµ‹è¯•å¼‚å¸¸: {e}")
        
        return test_result
    
    def _validate_key_features(self, validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å…³é”®ç‰¹æ€§"""
        validations = {
            'realtime_parsing_validated': False,
            'incremental_saves_validated': False,
            'dom_handling_validated': False,
            'quality_metrics_validated': False,
            'efficiency_validated': False
        }
        
        # éªŒè¯å®æ—¶è§£æ
        realtime_features = validation_result['realtime_features_validated']
        if (realtime_features['realtime_parsing_events'] > 0 and 
            realtime_features['realtime_markers_present']):
            validations['realtime_parsing_validated'] = True
            self.logger.info(f"âœ… å®æ—¶è§£æéªŒè¯é€šè¿‡: {realtime_features['realtime_parsing_events']} ä¸ªäº‹ä»¶")
        
        # éªŒè¯å¢é‡ä¿å­˜
        if realtime_features['incremental_saves_performed'] > 0:
            validations['incremental_saves_validated'] = True
            self.logger.info(f"âœ… å¢é‡ä¿å­˜éªŒè¯é€šè¿‡: {realtime_features['incremental_saves_performed']} æ¬¡ä¿å­˜")
        
        # éªŒè¯DOMå¤„ç†
        if (realtime_features['dom_elements_processed'] > 0 and 
            validation_result['dom_parsing_ratio'] > 0):
            validations['dom_handling_validated'] = True
            self.logger.info(f"âœ… DOMå¤„ç†éªŒè¯é€šè¿‡: è§£ææ¯”ç‡ {validation_result['dom_parsing_ratio']:.1%}")
        
        # éªŒè¯è´¨é‡æŒ‡æ ‡
        quality_metrics = validation_result['quality_metrics']
        if (quality_metrics and 
            quality_metrics.get('content_completeness', 0) > 0.8):
            validations['quality_metrics_validated'] = True
            self.logger.info(f"âœ… è´¨é‡æŒ‡æ ‡éªŒè¯é€šè¿‡: å†…å®¹å®Œæ•´æ€§ {quality_metrics['content_completeness']:.1%}")
        
        # éªŒè¯æ•ˆç‡
        if validation_result['parsing_efficiency'] > 1.0:  # æ¯æ¬¡æ»šåŠ¨è‡³å°‘è§£æ1æ¡æ¨æ–‡
            validations['efficiency_validated'] = True
            self.logger.info(f"âœ… æ•ˆç‡éªŒè¯é€šè¿‡: {validation_result['parsing_efficiency']:.2f} æ¨æ–‡/æ»šåŠ¨")
        
        return validations


async def main():
    """ä¸»éªŒè¯å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(__name__)
    
    try:
        # åˆ›å»ºéªŒè¯æµ‹è¯•å®ä¾‹
        test = RealtimeValidationTest()
        
        # è¿è¡Œç»¼åˆéªŒè¯
        result = await test.run_comprehensive_validation()
        
        # è¾“å‡ºéªŒè¯ç»“æœ
        logger.info("\n" + "="*70)
        logger.info("ğŸ“Š å®æ—¶è§£æåŠŸèƒ½ç»¼åˆéªŒè¯ç»“æœ")
        logger.info("="*70)
        logger.info(f"éªŒè¯çŠ¶æ€: {'âœ… æˆåŠŸ' if result['success'] else 'âŒ å¤±è´¥'}")
        
        if result['success']:
            validation = result['validation_results']
            features = result['feature_validations']
            
            logger.info(f"\nğŸ“ˆ è§£æç»“æœ:")
            logger.info(f"  ç›®æ ‡æ¨æ–‡æ•°: {validation['target_tweets']}")
            logger.info(f"  å®é™…è§£ææ•°: {validation['parsed_tweets_count']}")
            logger.info(f"  æ»šåŠ¨æ¬¡æ•°: {validation['scroll_attempts']}")
            logger.info(f"  è§£ææ•ˆç‡: {validation['parsing_efficiency']:.2f} æ¨æ–‡/æ»šåŠ¨")
            logger.info(f"  DOMè§£ææ¯”ç‡: {validation['dom_parsing_ratio']:.1%}")
            
            logger.info(f"\nğŸ” ç‰¹æ€§éªŒè¯ç»“æœ:")
            for feature, status in features.items():
                status_icon = "âœ…" if status else "âŒ"
                logger.info(f"  {status_icon} {feature}: {status}")
            
            # è´¨é‡æŒ‡æ ‡
            quality = validation['quality_metrics']
            logger.info(f"\nğŸ“Š æ•°æ®è´¨é‡æŒ‡æ ‡:")
            logger.info(f"  å†…å®¹å®Œæ•´æ€§: {quality['content_completeness']:.1%}")
            logger.info(f"  å®æ—¶æ ‡è®°å®Œæ•´æ€§: {quality['realtime_marker_completeness']:.1%}")
            logger.info(f"  è§£ææˆåŠŸç‡: {quality['parsing_success_rate']:.1%}")
            logger.info(f"  å®æ—¶è§£ææ•ˆç‡: {quality['realtime_parsing_efficiency']:.2f} äº‹ä»¶/æ»šåŠ¨")
            
            # å®æ—¶ç‰¹æ€§éªŒè¯
            realtime_features = validation['realtime_features_validated']
            logger.info(f"\nâš¡ å®æ—¶ç‰¹æ€§éªŒè¯:")
            logger.info(f"  å®æ—¶è§£æäº‹ä»¶: {realtime_features['realtime_parsing_events']}")
            logger.info(f"  å¢é‡ä¿å­˜æ¬¡æ•°: {realtime_features['incremental_saves_performed']}")
            logger.info(f"  DOMå…ƒç´ å¤„ç†: {realtime_features['dom_elements_processed']}")
            logger.info(f"  é‡å¤æ£€æµ‹æ¿€æ´»: {realtime_features['duplicate_detection_active']}")
            
            logger.info(f"\nğŸ“„ ç»“æœæ–‡ä»¶: {validation['final_save_file']}")
            
        else:
            logger.error(f"\nâŒ éªŒè¯å¤±è´¥åŸå› : {result['errors']}")
        
        logger.info("="*70)
        
        # æ€»ç»“éªŒè¯è¦ç‚¹
        logger.info("\nğŸ“‹ éªŒè¯è¦ç‚¹æ€»ç»“:")
        logger.info("1. âœ… å®æ—¶è§£æ: åœ¨æ»šåŠ¨è¿‡ç¨‹ä¸­ç«‹å³è§£ææ–°å‡ºç°çš„æ¨æ–‡")
        logger.info("2. âœ… å¢é‡ä¿å­˜: è¾¹æ»šåŠ¨è¾¹ä¿å­˜è§£æç»“æœ")
        logger.info("3. âœ… è°ƒæ•´æµ‹è¯•é€»è¾‘: éªŒè¯å®æ—¶è§£æçš„æ¨æ–‡æ•°é‡ï¼Œè€Œä¸æ˜¯æœ€ç»ˆDOMå…ƒç´ æ•°é‡")
        logger.info("4. âœ… DOMå…ƒç´ å¤„ç†: æ­£ç¡®å¤„ç†Twitterçš„DOMè™šæ‹ŸåŒ–å’Œå…ƒç´ å›æ”¶")
        logger.info("5. âœ… è´¨é‡æŒ‡æ ‡: æä¾›è¯¦ç»†çš„è§£æè´¨é‡å’Œæ•ˆç‡æŒ‡æ ‡")
        
    except Exception as e:
        logger.error(f"âŒ ä¸»éªŒè¯å‡½æ•°å¼‚å¸¸: {e}")
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    asyncio.run(main())