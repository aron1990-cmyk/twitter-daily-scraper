#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sqlite3
import json
from enhanced_tweet_scraper import EnhancedTwitterScraper, load_feishu_config_from_file

def test_feishu_no_publish_time():
    """æµ‹è¯•ç§»é™¤å‘å¸ƒæ—¶é—´å­—æ®µåçš„é£ä¹¦åŒæ­¥åŠŸèƒ½"""
    print("ğŸ”§ æµ‹è¯•ç§»é™¤å‘å¸ƒæ—¶é—´å­—æ®µåçš„é£ä¹¦åŒæ­¥åŠŸèƒ½...")
    
    # åŠ è½½é£ä¹¦é…ç½®
    feishu_config = load_feishu_config_from_file()
    if not feishu_config:
        print("âŒ æ— æ³•åŠ è½½é£ä¹¦é…ç½®")
        return
    
    print(f"âœ… é£ä¹¦é…ç½®åŠ è½½æˆåŠŸï¼Œå¯ç”¨çŠ¶æ€: {feishu_config.get('enabled')}")
    
    # æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
    conn = sqlite3.connect('twitter_scraper.db')
    cursor = conn.cursor()
    
    # ç»Ÿè®¡æ¨æ–‡æ•°æ®
    cursor.execute("""
        SELECT 
            COUNT(*) as total_tweets,
            COUNT(CASE WHEN publish_time IS NOT NULL AND publish_time != '' THEN 1 END) as has_publish_time,
            COUNT(CASE WHEN scraped_at IS NOT NULL THEN 1 END) as has_scraped_at,
            COUNT(CASE WHEN synced_to_feishu = 1 THEN 1 END) as synced_count
        FROM tweet_data
    """)
    
    stats = cursor.fetchone()
    print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
    print(f"   æ€»æ¨æ–‡æ•°: {stats[0]}")
    print(f"   æœ‰å‘å¸ƒæ—¶é—´çš„æ¨æ–‡: {stats[1]}")
    print(f"   æœ‰æŠ“å–æ—¶é—´çš„æ¨æ–‡: {stats[2]}")
    print(f"   å·²åŒæ­¥åˆ°é£ä¹¦çš„æ¨æ–‡: {stats[3]}")
    
    # è·å–ä¸€äº›ç¤ºä¾‹æ¨æ–‡è¿›è¡Œæµ‹è¯•
    cursor.execute("""
        SELECT id, username, content, publish_time, scraped_at, synced_to_feishu
        FROM tweet_data 
        LIMIT 5
    """)
    
    sample_tweets = cursor.fetchall()
    conn.close()
    
    if not sample_tweets:
        print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰æ¨æ–‡æ•°æ®")
        return
    
    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    tweets_data = []
    for tweet in sample_tweets:
        tweet_dict = {
            'id': tweet[0],
            'username': tweet[1],
            'content': tweet[2],
            'publish_time': tweet[3],
            'scraped_at': tweet[4],
            'synced_to_feishu': tweet[5],
            'hashtags': [],
            'content_type': 'æ™®é€šæ¨æ–‡',
            'comments': 0,
            'retweets': 0,
            'likes': 0,
            'link': f'https://twitter.com/{tweet[1]}/status/{tweet[0]}'
        }
        tweets_data.append(tweet_dict)
    
    # æµ‹è¯•æ ¼å¼åŒ–å‡½æ•°
    scraper = EnhancedTwitterScraper()
    formatted_tweets = scraper.format_tweets_for_feishu(tweets_data)
    
    print(f"\nğŸ§ª æµ‹è¯•æ ¼å¼åŒ–ç»“æœ:")
    print(f"   åŸå§‹æ¨æ–‡æ•°: {len(tweets_data)}")
    print(f"   æ ¼å¼åŒ–åæ¨æ–‡æ•°: {len(formatted_tweets)}")
    
    # æ£€æŸ¥æ ¼å¼åŒ–åçš„å­—æ®µ
    if formatted_tweets:
        first_tweet = formatted_tweets[0]
        print(f"\nğŸ“‹ æ ¼å¼åŒ–åçš„å­—æ®µ:")
        for key in first_tweet.keys():
            print(f"   - {key}")
        
        # éªŒè¯æ˜¯å¦ç§»é™¤äº†å‘å¸ƒæ—¶é—´å­—æ®µ
        if 'å‘å¸ƒæ—¶é—´' in first_tweet:
            print("\nâŒ é”™è¯¯ï¼šå‘å¸ƒæ—¶é—´å­—æ®µä»ç„¶å­˜åœ¨ï¼")
            print(f"   å‘å¸ƒæ—¶é—´å€¼: {first_tweet['å‘å¸ƒæ—¶é—´']}")
        else:
            print("\nâœ… æˆåŠŸï¼šå‘å¸ƒæ—¶é—´å­—æ®µå·²è¢«ç§»é™¤")
        
        # æ˜¾ç¤ºç¤ºä¾‹æ•°æ®
        print(f"\nğŸ“ ç¤ºä¾‹æ ¼å¼åŒ–æ•°æ®:")
        print(json.dumps(first_tweet, ensure_ascii=False, indent=2))
    
    # æ£€æŸ¥é…ç½®çŠ¶æ€
    if feishu_config.get('enabled') and all([
        feishu_config.get('app_id'),
        feishu_config.get('app_secret'),
        feishu_config.get('spreadsheet_token'),
        feishu_config.get('table_id')
    ]):
        print("\nğŸš€ é£ä¹¦é…ç½®å®Œæ•´ï¼Œå¯ä»¥è¿›è¡Œå®é™…åŒæ­¥æµ‹è¯•")
    else:
        print("\nâš ï¸  æ£€æµ‹åˆ°å ä½ç¬¦é…ç½®ï¼Œæ— æ³•è¿›è¡Œå®é™…åŒæ­¥")
        print("   è¯·åœ¨ feishu_config.json ä¸­é…ç½®æœ‰æ•ˆçš„é£ä¹¦å‚æ•°")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_feishu_no_publish_time()