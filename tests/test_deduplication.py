import pytest
import json
import hashlib
from pathlib import Path
import sys
import os
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from performance_optimizer import AdvancedDeduplicator

class TestAdvancedDeduplicator:
    """é«˜çº§å»é‡æ¨¡å—æµ‹è¯•ç±»"""
    
    @pytest.fixture
    def sample_tweets(self):
        """åŠ è½½ç¤ºä¾‹æ¨æ–‡æ•°æ®"""
        fixtures_path = Path(__file__).parent / "fixtures" / "sample_tweets.json"
        with open(fixtures_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    @pytest.fixture
    def deduplicator(self):
        """åˆ›å»ºå»é‡å™¨å®ä¾‹"""
        return AdvancedDeduplicator()
    
    @pytest.fixture
    def duplicate_tweets(self):
        """åˆ›å»ºåŒ…å«é‡å¤æ¨æ–‡çš„æµ‹è¯•æ•°æ®"""
        return [
            {
                "id": "1",
                "username": "user1",
                "content": "This is a test tweet about AI technology",
                "timestamp": "2024-01-15 10:00:00",
                "url": "https://twitter.com/user1/status/1",
                "likes": 100,
                "retweets": 20,
                "replies": 5
            },
            {
                "id": "2",
                "username": "user2",
                "content": "This is a test tweet about AI technology",  # å®Œå…¨ç›¸åŒå†…å®¹
                "timestamp": "2024-01-15 11:00:00",
                "url": "https://twitter.com/user2/status/2",
                "likes": 50,
                "retweets": 10,
                "replies": 2
            },
            {
                "id": "3",
                "username": "user3",
                "content": "This is a test tweet about artificial intelligence technology",  # ç›¸ä¼¼å†…å®¹
                "timestamp": "2024-01-15 12:00:00",
                "url": "https://twitter.com/user3/status/3",
                "likes": 75,
                "retweets": 15,
                "replies": 3
            },
            {
                "id": "1",  # é‡å¤ID
                "username": "user1",
                "content": "This is a test tweet about AI technology",
                "timestamp": "2024-01-15 10:00:00",
                "url": "https://twitter.com/user1/status/1",  # é‡å¤URL
                "likes": 100,
                "retweets": 20,
                "replies": 5
            },
            {
                "id": "4",
                "username": "user4",
                "content": "Completely different tweet about sports",
                "timestamp": "2024-01-15 13:00:00",
                "url": "https://twitter.com/user4/status/4",
                "likes": 30,
                "retweets": 5,
                "replies": 1
            }
        ]
    
    def test_deduplicator_initialization(self, deduplicator):
        """æµ‹è¯•å»é‡å™¨åˆå§‹åŒ–
        
        éªŒè¯AdvancedDeduplicatorèƒ½å¤Ÿæ­£ç¡®åˆå§‹åŒ–
        """
        assert deduplicator is not None
        assert hasattr(deduplicator, 'remove_duplicates')
        assert hasattr(deduplicator, 'calculate_similarity')
        assert hasattr(deduplicator, 'generate_content_hash')
    
    def test_url_based_deduplication(self, deduplicator, duplicate_tweets):
        """æµ‹è¯•åŸºäºURLçš„å»é‡
        
        éªŒè¯èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«å’Œç§»é™¤å…·æœ‰ç›¸åŒURLçš„é‡å¤æ¨æ–‡
        """
        # æ‰§è¡Œå»é‡
        unique_tweets, stats = deduplicator.remove_duplicates(
            duplicate_tweets, 
            method='url'
        )
        
        # éªŒè¯å»é‡ç»“æœ
        urls = [tweet['url'] for tweet in unique_tweets]
        assert len(urls) == len(set(urls))  # æ‰€æœ‰URLéƒ½æ˜¯å”¯ä¸€çš„
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        assert 'duplicates_removed' in stats
        assert stats['duplicates_removed'] > 0
        assert len(unique_tweets) < len(duplicate_tweets)
    
    def test_content_based_deduplication(self, deduplicator, duplicate_tweets):
        """æµ‹è¯•åŸºäºå†…å®¹çš„å»é‡
        
        éªŒè¯èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«å’Œç§»é™¤å…·æœ‰ç›¸åŒå†…å®¹çš„é‡å¤æ¨æ–‡
        """
        # æ‰§è¡Œå»é‡
        unique_tweets, stats = deduplicator.remove_duplicates(
            duplicate_tweets, 
            method='content'
        )
        
        # éªŒè¯å»é‡ç»“æœ
        contents = [tweet['content'] for tweet in unique_tweets]
        assert len(contents) == len(set(contents))  # æ‰€æœ‰å†…å®¹éƒ½æ˜¯å”¯ä¸€çš„
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        assert stats['duplicates_removed'] > 0
    
    def test_hash_based_deduplication(self, deduplicator, duplicate_tweets):
        """æµ‹è¯•åŸºäºå“ˆå¸Œçš„å»é‡
        
        éªŒè¯èƒ½å¤Ÿä½¿ç”¨å†…å®¹å“ˆå¸Œæ­£ç¡®è¯†åˆ«é‡å¤æ¨æ–‡
        """
        # æ‰§è¡Œå»é‡
        unique_tweets, stats = deduplicator.remove_duplicates(
            duplicate_tweets, 
            method='hash'
        )
        
        # éªŒè¯å»é‡ç»“æœ
        hashes = [deduplicator.generate_content_hash(tweet['content']) for tweet in unique_tweets]
        assert len(hashes) == len(set(hashes))  # æ‰€æœ‰å“ˆå¸Œéƒ½æ˜¯å”¯ä¸€çš„
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        assert stats['duplicates_removed'] > 0
    
    def test_timestamp_based_deduplication(self, deduplicator):
        """æµ‹è¯•åŸºäºæ—¶é—´æˆ³çš„å»é‡
        
        éªŒè¯èƒ½å¤Ÿè¯†åˆ«åœ¨ç›¸åŒæ—¶é—´å‘å¸ƒçš„é‡å¤æ¨æ–‡
        """
        timestamp_duplicates = [
            {
                "id": "1",
                "content": "Tweet 1",
                "timestamp": "2024-01-15 10:00:00",
                "url": "https://twitter.com/user1/status/1"
            },
            {
                "id": "2",
                "content": "Tweet 2",
                "timestamp": "2024-01-15 10:00:00",  # ç›¸åŒæ—¶é—´æˆ³
                "url": "https://twitter.com/user2/status/2"
            },
            {
                "id": "3",
                "content": "Tweet 3",
                "timestamp": "2024-01-15 11:00:00",
                "url": "https://twitter.com/user3/status/3"
            }
        ]
        
        # æ‰§è¡Œå»é‡
        unique_tweets, stats = deduplicator.remove_duplicates(
            timestamp_duplicates, 
            method='timestamp'
        )
        
        # éªŒè¯å»é‡ç»“æœ
        timestamps = [tweet['timestamp'] for tweet in unique_tweets]
        assert len(timestamps) == len(set(timestamps))  # æ‰€æœ‰æ—¶é—´æˆ³éƒ½æ˜¯å”¯ä¸€çš„
    
    @pytest.mark.parametrize("similarity_threshold", [0.8, 0.85, 0.9, 0.95])
    def test_similarity_threshold_adjustment(self, deduplicator, similarity_threshold):
        """æµ‹è¯•ç›¸ä¼¼åº¦é˜ˆå€¼è°ƒæ•´
        
        éªŒè¯ä¸åŒç›¸ä¼¼åº¦é˜ˆå€¼å¯¹å»é‡ç»“æœçš„å½±å“
        """
        similar_tweets = [
            {
                "id": "1",
                "content": "AI technology is advancing rapidly",
                "url": "https://twitter.com/user1/status/1"
            },
            {
                "id": "2",
                "content": "Artificial intelligence technology is advancing quickly",
                "url": "https://twitter.com/user2/status/2"
            },
            {
                "id": "3",
                "content": "Machine learning is a subset of AI",
                "url": "https://twitter.com/user3/status/3"
            }
        ]
        
        # æ‰§è¡Œå»é‡
        unique_tweets, stats = deduplicator.remove_duplicates(
            similar_tweets, 
            method='similarity',
            similarity_threshold=similarity_threshold
        )
        
        # éªŒè¯é˜ˆå€¼å½±å“
        assert len(unique_tweets) <= len(similar_tweets)
        assert 'similarity_threshold' in stats
        assert stats['similarity_threshold'] == similarity_threshold
    
    def test_calculate_similarity_function(self, deduplicator):
        """æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°
        
        éªŒè¯ç›¸ä¼¼åº¦è®¡ç®—çš„å‡†ç¡®æ€§
        """
        text1 = "This is a test tweet about AI"
        text2 = "This is a test tweet about AI"  # å®Œå…¨ç›¸åŒ
        text3 = "This is a test tweet about artificial intelligence"  # ç›¸ä¼¼
        text4 = "Completely different content about sports"  # å®Œå…¨ä¸åŒ
        
        # æµ‹è¯•å®Œå…¨ç›¸åŒçš„æ–‡æœ¬
        similarity1 = deduplicator.calculate_similarity(text1, text2)
        assert similarity1 == 1.0
        
        # æµ‹è¯•ç›¸ä¼¼æ–‡æœ¬
        similarity2 = deduplicator.calculate_similarity(text1, text3)
        assert 0.5 < similarity2 < 1.0
        
        # æµ‹è¯•å®Œå…¨ä¸åŒçš„æ–‡æœ¬
        similarity3 = deduplicator.calculate_similarity(text1, text4)
        assert similarity3 < 0.5
    
    def test_content_hash_generation(self, deduplicator):
        """æµ‹è¯•å†…å®¹å“ˆå¸Œç”Ÿæˆ
        
        éªŒè¯å†…å®¹å“ˆå¸Œç”Ÿæˆçš„ä¸€è‡´æ€§å’Œå”¯ä¸€æ€§
        """
        content1 = "This is a test tweet"
        content2 = "This is a test tweet"  # ç›¸åŒå†…å®¹
        content3 = "This is a different tweet"  # ä¸åŒå†…å®¹
        
        hash1 = deduplicator.generate_content_hash(content1)
        hash2 = deduplicator.generate_content_hash(content2)
        hash3 = deduplicator.generate_content_hash(content3)
        
        # éªŒè¯ç›¸åŒå†…å®¹ç”Ÿæˆç›¸åŒå“ˆå¸Œ
        assert hash1 == hash2
        
        # éªŒè¯ä¸åŒå†…å®¹ç”Ÿæˆä¸åŒå“ˆå¸Œ
        assert hash1 != hash3
        
        # éªŒè¯å“ˆå¸Œæ ¼å¼
        assert isinstance(hash1, str)
        assert len(hash1) > 0
    
    def test_deduplication_statistics(self, deduplicator, duplicate_tweets):
        """æµ‹è¯•å»é‡ç»Ÿè®¡ä¿¡æ¯
        
        éªŒè¯å»é‡è¿‡ç¨‹ä¸­ç»Ÿè®¡ä¿¡æ¯çš„å‡†ç¡®æ€§
        """
        original_count = len(duplicate_tweets)
        
        # æ‰§è¡Œå»é‡
        unique_tweets, stats = deduplicator.remove_duplicates(duplicate_tweets)
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        assert 'original_count' in stats
        assert 'unique_count' in stats
        assert 'duplicates_removed' in stats
        assert 'deduplication_rate' in stats
        
        assert stats['original_count'] == original_count
        assert stats['unique_count'] == len(unique_tweets)
        assert stats['duplicates_removed'] == original_count - len(unique_tweets)
        
        # éªŒè¯å»é‡ç‡è®¡ç®—
        expected_rate = stats['duplicates_removed'] / original_count
        assert abs(stats['deduplication_rate'] - expected_rate) < 0.01
    
    def test_deduplication_rate_calculation(self, deduplicator):
        """æµ‹è¯•å»é‡ç‡è®¡ç®—å‡†ç¡®æ€§
        
        éªŒè¯å»é‡ç‡åœ¨æŠ¥å‘Šä¸­çš„å‡†ç¡®ä½“ç°
        """
        test_tweets = [
            {"id": "1", "content": "Tweet 1", "url": "url1"},
            {"id": "2", "content": "Tweet 2", "url": "url2"},
            {"id": "1", "content": "Tweet 1", "url": "url1"},  # é‡å¤
            {"id": "3", "content": "Tweet 3", "url": "url3"},
            {"id": "2", "content": "Tweet 2", "url": "url2"},  # é‡å¤
        ]
        
        unique_tweets, stats = deduplicator.remove_duplicates(test_tweets)
        
        # éªŒè¯å»é‡ç‡
        expected_rate = 2 / 5  # 5ä¸ªæ¨æ–‡ä¸­ç§»é™¤2ä¸ªé‡å¤
        assert abs(stats['deduplication_rate'] - expected_rate) < 0.01
        assert len(unique_tweets) == 3
    
    def test_empty_input_handling(self, deduplicator):
        """æµ‹è¯•ç©ºè¾“å…¥å¤„ç†
        
        éªŒè¯å½“è¾“å…¥ä¸ºç©ºæ—¶çš„å¤„ç†é€»è¾‘
        """
        empty_tweets = []
        
        unique_tweets, stats = deduplicator.remove_duplicates(empty_tweets)
        
        assert len(unique_tweets) == 0
        assert stats['original_count'] == 0
        assert stats['unique_count'] == 0
        assert stats['duplicates_removed'] == 0
        assert stats['deduplication_rate'] == 0.0
    
    def test_single_tweet_handling(self, deduplicator):
        """æµ‹è¯•å•æ¡æ¨æ–‡å¤„ç†
        
        éªŒè¯åªæœ‰ä¸€æ¡æ¨æ–‡æ—¶çš„å¤„ç†é€»è¾‘
        """
        single_tweet = [{
            "id": "1",
            "content": "Single tweet",
            "url": "https://twitter.com/user/status/1"
        }]
        
        unique_tweets, stats = deduplicator.remove_duplicates(single_tweet)
        
        assert len(unique_tweets) == 1
        assert stats['original_count'] == 1
        assert stats['unique_count'] == 1
        assert stats['duplicates_removed'] == 0
        assert stats['deduplication_rate'] == 0.0
    
    @pytest.mark.parametrize("method", ['url', 'content', 'hash', 'similarity'])
    def test_different_deduplication_methods(self, deduplicator, duplicate_tweets, method):
        """æµ‹è¯•ä¸åŒå»é‡æ–¹æ³•
        
        éªŒè¯å„ç§å»é‡æ–¹æ³•éƒ½èƒ½æ­£å¸¸å·¥ä½œ
        """
        unique_tweets, stats = deduplicator.remove_duplicates(
            duplicate_tweets, 
            method=method
        )
        
        # éªŒè¯åŸºæœ¬ç»“æœ
        assert isinstance(unique_tweets, list)
        assert isinstance(stats, dict)
        assert len(unique_tweets) <= len(duplicate_tweets)
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        assert 'method' in stats
        assert stats['method'] == method
    
    def test_preserve_highest_engagement(self, deduplicator):
        """æµ‹è¯•ä¿ç•™æœ€é«˜äº’åŠ¨é‡çš„æ¨æ–‡
        
        éªŒè¯åœ¨å»é‡æ—¶ä¿ç•™äº’åŠ¨é‡æœ€é«˜çš„æ¨æ–‡ç‰ˆæœ¬
        """
        engagement_duplicates = [
            {
                "id": "1",
                "content": "Same content",
                "url": "url1",
                "likes": 100,
                "retweets": 20,
                "replies": 5
            },
            {
                "id": "2",
                "content": "Same content",
                "url": "url2",
                "likes": 500,  # æ›´é«˜çš„äº’åŠ¨é‡
                "retweets": 100,
                "replies": 25
            }
        ]
        
        unique_tweets, stats = deduplicator.remove_duplicates(
            engagement_duplicates, 
            method='content',
            preserve_highest_engagement=True
        )
        
        # éªŒè¯ä¿ç•™äº†äº’åŠ¨é‡æ›´é«˜çš„æ¨æ–‡
        assert len(unique_tweets) == 1
        assert unique_tweets[0]['likes'] == 500
        assert unique_tweets[0]['id'] == "2"
    
    def test_batch_deduplication_performance(self, deduplicator):
        """æµ‹è¯•æ‰¹é‡å»é‡æ€§èƒ½
        
        éªŒè¯å¤§é‡æ•°æ®çš„å»é‡æ€§èƒ½
        """
        # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
        large_dataset = []
        for i in range(1000):
            tweet = {
                "id": str(i % 100),  # åˆ›å»ºé‡å¤ID
                "content": f"Tweet content {i % 50}",  # åˆ›å»ºé‡å¤å†…å®¹
                "url": f"https://twitter.com/user/status/{i % 100}",
                "likes": i,
                "retweets": i // 2,
                "replies": i // 5
            }
            large_dataset.append(tweet)
        
        # æ‰§è¡Œå»é‡
        unique_tweets, stats = deduplicator.remove_duplicates(large_dataset)
        
        # éªŒè¯æ€§èƒ½å’Œç»“æœ
        assert len(unique_tweets) < len(large_dataset)
        assert stats['duplicates_removed'] > 0
        assert 'processing_time' in stats  # å‡è®¾å»é‡å™¨è®°å½•å¤„ç†æ—¶é—´
    
    def test_malformed_data_handling(self, deduplicator):
        """æµ‹è¯•ç•¸å½¢æ•°æ®å¤„ç†
        
        éªŒè¯å¯¹ç¼ºå°‘å¿…éœ€å­—æ®µçš„æ¨æ–‡çš„å¤„ç†
        """
        malformed_tweets = [
            {
                "id": "1",
                "content": "Normal tweet",
                "url": "https://twitter.com/user/status/1"
            },
            {
                "id": "2",
                # ç¼ºå°‘contentå­—æ®µ
                "url": "https://twitter.com/user/status/2"
            },
            {
                # ç¼ºå°‘idå­—æ®µ
                "content": "Tweet without ID",
                "url": "https://twitter.com/user/status/3"
            }
        ]
        
        # æ‰§è¡Œå»é‡ï¼Œåº”è¯¥èƒ½å¤Ÿå¤„ç†ç•¸å½¢æ•°æ®
        unique_tweets, stats = deduplicator.remove_duplicates(malformed_tweets)
        
        # éªŒè¯ç»“æœ
        assert isinstance(unique_tweets, list)
        assert isinstance(stats, dict)
        # å…·ä½“è¡Œä¸ºå–å†³äºå®ç°ï¼Œå¯èƒ½è¿‡æ»¤æ‰ç•¸å½¢æ•°æ®æˆ–ä¿ç•™å®ƒä»¬
    
    def test_unicode_content_deduplication(self, deduplicator):
        """æµ‹è¯•Unicodeå†…å®¹å»é‡
        
        éªŒè¯åŒ…å«Unicodeå­—ç¬¦çš„æ¨æ–‡å»é‡
        """
        unicode_tweets = [
            {
                "id": "1",
                "content": "æ¨æ–‡å†…å®¹åŒ…å«ä¸­æ–‡å­—ç¬¦ ğŸ˜€ğŸš€",
                "url": "https://twitter.com/user1/status/1"
            },
            {
                "id": "2",
                "content": "æ¨æ–‡å†…å®¹åŒ…å«ä¸­æ–‡å­—ç¬¦ ğŸ˜€ğŸš€",  # ç›¸åŒUnicodeå†…å®¹
                "url": "https://twitter.com/user2/status/2"
            },
            {
                "id": "3",
                "content": "Different content with Ã©mojis ğŸ‰",
                "url": "https://twitter.com/user3/status/3"
            }
        ]
        
        unique_tweets, stats = deduplicator.remove_duplicates(
            unicode_tweets, 
            method='content'
        )
        
        # éªŒè¯Unicodeå†…å®¹å»é‡
        assert len(unique_tweets) == 2  # åº”è¯¥ç§»é™¤ä¸€ä¸ªé‡å¤çš„ä¸­æ–‡æ¨æ–‡
        contents = [tweet['content'] for tweet in unique_tweets]
        assert len(set(contents)) == 2

if __name__ == "__main__":
    pytest.main(["-v", __file__])