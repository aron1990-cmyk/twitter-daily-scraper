#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é£ä¹¦æ•°æ®éªŒè¯å™¨
ç”¨äºåœ¨æ•°æ®åŒæ­¥åï¼Œè·å–é£ä¹¦ä¸­çš„æ•°æ®å¹¶ä¸æœ¬åœ°æ•°æ®è¿›è¡Œæ¯”å¯¹éªŒè¯
ç¡®ä¿æ•°æ®åŒæ­¥çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
"""

import os
import sys
import json
import requests
from datetime import datetime
from typing import List, Dict, Any, Tuple
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from web_app import app, db, TweetData, FEISHU_CONFIG
from cloud_sync import CloudSyncManager

class FeishuDataValidator:
    """
    é£ä¹¦æ•°æ®éªŒè¯å™¨
    è´Ÿè´£è·å–é£ä¹¦æ•°æ®å¹¶ä¸æœ¬åœ°æ•°æ®è¿›è¡Œæ¯”å¯¹
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.feishu_config = FEISHU_CONFIG
        # æ„å»ºæ­£ç¡®çš„é…ç½®æ ¼å¼ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
        feishu_config_with_base_url = dict(self.feishu_config)
        if 'base_url' not in feishu_config_with_base_url:
            feishu_config_with_base_url['base_url'] = 'https://open.feishu.cn/open-apis'
        
        config = {
            'feishu': feishu_config_with_base_url
        }
        self.sync_manager = CloudSyncManager(config)
        
    def get_feishu_access_token(self) -> str:
        """
        è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ
        
        Returns:
            è®¿é—®ä»¤ç‰Œå­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›None
        """
        return self.sync_manager.get_feishu_access_token()
    
    def get_feishu_table_fields(self, access_token: str) -> Dict[str, Any]:
        """
        è·å–é£ä¹¦è¡¨æ ¼å­—æ®µä¿¡æ¯
        
        Args:
            access_token: é£ä¹¦è®¿é—®ä»¤ç‰Œ
            
        Returns:
            å­—æ®µä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«å­—æ®µååˆ°IDçš„æ˜ å°„å’Œå­—æ®µç±»å‹
        """
        url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{self.feishu_config['spreadsheet_token']}/tables/{self.feishu_config['table_id']}/fields"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json; charset=utf-8"
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=30)
            result = response.json()
            
            if result.get('code') == 0:
                fields_data = result.get('data', {}).get('items', [])
                
                field_name_to_id = {}
                field_types = {}
                field_id_to_name = {}
                
                for field in fields_data:
                    field_name = field.get('field_name', '')
                    field_id = field.get('field_id', '')
                    field_type = field.get('type', 1)
                    
                    field_name_to_id[field_name] = field_id
                    field_types[field_name] = field_type
                    field_id_to_name[field_id] = field_name
                
                return {
                    'field_name_to_id': field_name_to_id,
                    'field_types': field_types,
                    'field_id_to_name': field_id_to_name,
                    'fields_data': fields_data
                }
            else:
                self.logger.error(f"è·å–é£ä¹¦è¡¨æ ¼å­—æ®µå¤±è´¥: {result}")
                return {}
        except Exception as e:
            self.logger.error(f"è·å–é£ä¹¦è¡¨æ ¼å­—æ®µå¼‚å¸¸: {e}")
            return {}
    
    def get_feishu_table_records(self, access_token: str, page_size: int = 100) -> List[Dict[str, Any]]:
        """
        è·å–é£ä¹¦è¡¨æ ¼è®°å½•
        
        Args:
            access_token: é£ä¹¦è®¿é—®ä»¤ç‰Œ
            page_size: æ¯é¡µè®°å½•æ•°
            
        Returns:
            è®°å½•åˆ—è¡¨
        """
        url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{self.feishu_config['spreadsheet_token']}/tables/{self.feishu_config['table_id']}/records"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json; charset=utf-8"
        }
        
        all_records = []
        page_token = None
        
        try:
            while True:
                params = {"page_size": page_size}
                if page_token:
                    params["page_token"] = page_token
                
                response = requests.get(url, headers=headers, params=params, timeout=30)
                result = response.json()
                
                if result.get('code') == 0:
                    data = result.get('data', {})
                    records = data.get('items', [])
                    all_records.extend(records)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                    page_token = data.get('page_token')
                    if not page_token:
                        break
                else:
                    self.logger.error(f"è·å–é£ä¹¦è¡¨æ ¼è®°å½•å¤±è´¥: {result}")
                    break
                    
        except Exception as e:
            self.logger.error(f"è·å–é£ä¹¦è¡¨æ ¼è®°å½•å¼‚å¸¸: {e}")
            
        return all_records
    
    def parse_feishu_records(self, records: List[Dict[str, Any]], field_mapping: Dict[str, str]) -> List[Dict[str, Any]]:
        """
        è§£æé£ä¹¦è®°å½•æ•°æ®
        
        Args:
            records: é£ä¹¦åŸå§‹è®°å½•åˆ—è¡¨
            field_mapping: å­—æ®µIDåˆ°å­—æ®µåçš„æ˜ å°„ï¼ˆå¯èƒ½ä¸éœ€è¦ï¼Œå› ä¸ºå­—æ®µå¯èƒ½å·²ç»æ˜¯åç§°ï¼‰
            
        Returns:
            è§£æåçš„è®°å½•åˆ—è¡¨
        """
        parsed_records = []
        
        for record in records:
            record_id = record.get('record_id', '')
            created_time = record.get('created_time', 0)
            fields_data = record.get('fields', {})
            
            parsed_record = {
                'record_id': record_id,
                'created_time': created_time,
                'fields': {}
            }
            
            # è§£æå­—æ®µæ•°æ®
            for field_key, field_value in fields_data.items():
                # æ£€æŸ¥å­—æ®µé”®æ˜¯å¦å·²ç»æ˜¯å­—æ®µåï¼ˆä¸­æ–‡ï¼‰è¿˜æ˜¯å­—æ®µID
                if field_key in field_mapping.values():  # å¦‚æœæ˜¯å­—æ®µå
                    field_name = field_key
                elif field_key in field_mapping:  # å¦‚æœæ˜¯å­—æ®µID
                    field_name = field_mapping[field_key]
                else:
                    field_name = field_key  # ç›´æ¥ä½¿ç”¨åŸå§‹é”®å
                
                parsed_record['fields'][field_name] = field_value
            
            parsed_records.append(parsed_record)
        
        return parsed_records
    
    def get_local_sync_data(self, task_id: int = None) -> List[Dict[str, Any]]:
        """
        è·å–æœ¬åœ°åŒæ­¥çš„æ•°æ®
        
        Args:
            task_id: ä»»åŠ¡IDï¼Œå¦‚æœä¸ºNoneåˆ™è·å–æ‰€æœ‰æ•°æ®
            
        Returns:
            æœ¬åœ°æ•°æ®åˆ—è¡¨
        """
        try:
            with app.app_context():
                if task_id:
                    tweets = TweetData.query.filter_by(task_id=task_id).all()
                else:
                    tweets = TweetData.query.all()
                
                local_data = []
                for tweet in tweets:
                    # æ„å»ºä¸é£ä¹¦åŒæ­¥æ ¼å¼ä¸€è‡´çš„æ•°æ®
                    tweet_data = {
                        'id': tweet.id,
                        'task_id': tweet.task_id,
                        'æ¨æ–‡åŸæ–‡å†…å®¹': tweet.content or '',
                        'ä½œè€…ï¼ˆè´¦å·ï¼‰': tweet.username or '',
                        'æ¨æ–‡é“¾æ¥': tweet.link or '',
                        'è¯é¢˜æ ‡ç­¾ï¼ˆHashtagï¼‰': tweet.hashtags or '',
                        'ç±»å‹æ ‡ç­¾': tweet.content_type or '',
                        'è¯„è®º': tweet.comments or 0,
                        'ç‚¹èµ': tweet.likes or 0,
                        'è½¬å‘': tweet.retweets or 0,
                        'å‘å¸ƒæ—¶é—´': int(tweet.scraped_at.timestamp() * 1000) if tweet.scraped_at else 0
                    }
                    local_data.append(tweet_data)
                
                return local_data
        except Exception as e:
            self.logger.error(f"è·å–æœ¬åœ°æ•°æ®å¼‚å¸¸: {e}")
            return []
    
    def compare_data(self, local_data: List[Dict[str, Any]], feishu_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ¯”å¯¹æœ¬åœ°æ•°æ®å’Œé£ä¹¦æ•°æ®
        
        Args:
            local_data: æœ¬åœ°æ•°æ®åˆ—è¡¨
            feishu_data: é£ä¹¦æ•°æ®åˆ—è¡¨
            
        Returns:
            æ¯”å¯¹ç»“æœå­—å…¸
        """
        comparison_result = {
            'local_count': len(local_data),
            'feishu_count': len(feishu_data),
            'matched_records': [],
            'missing_in_feishu': [],
            'extra_in_feishu': [],
            'field_mismatches': [],
            'summary': {}
        }
        
        # åˆ›å»ºæœ¬åœ°æ•°æ®çš„å†…å®¹ç´¢å¼•ï¼ˆç”¨æ¨æ–‡åŸæ–‡å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
        local_content_map = {}
        for local_record in local_data:
            content = local_record.get('æ¨æ–‡åŸæ–‡å†…å®¹', '').strip()
            if content:
                local_content_map[content] = local_record
        
        # åˆ›å»ºé£ä¹¦æ•°æ®çš„å†…å®¹ç´¢å¼•
        feishu_content_map = {}
        for feishu_record in feishu_data:
            content = feishu_record.get('fields', {}).get('æ¨æ–‡åŸæ–‡å†…å®¹', '').strip()
            if content:
                feishu_content_map[content] = feishu_record
        
        # æŸ¥æ‰¾åŒ¹é…çš„è®°å½•
        for content, local_record in local_content_map.items():
            if content in feishu_content_map:
                feishu_record = feishu_content_map[content]
                
                # æ¯”å¯¹å­—æ®µå€¼
                field_matches = self._compare_record_fields(local_record, feishu_record)
                
                comparison_result['matched_records'].append({
                    'content': content[:100] + '...' if len(content) > 100 else content,
                    'local_record': local_record,
                    'feishu_record': feishu_record,
                    'field_matches': field_matches
                })
                
                # è®°å½•å­—æ®µä¸åŒ¹é…çš„æƒ…å†µ
                if not field_matches['all_match']:
                    comparison_result['field_mismatches'].append({
                        'content': content[:100] + '...' if len(content) > 100 else content,
                        'mismatched_fields': field_matches['mismatched_fields']
                    })
            else:
                # æœ¬åœ°æœ‰ä½†é£ä¹¦æ²¡æœ‰çš„è®°å½•
                comparison_result['missing_in_feishu'].append({
                    'content': content[:100] + '...' if len(content) > 100 else content,
                    'local_record': local_record
                })
        
        # æŸ¥æ‰¾é£ä¹¦ä¸­å¤šå‡ºçš„è®°å½•
        for content, feishu_record in feishu_content_map.items():
            if content not in local_content_map:
                comparison_result['extra_in_feishu'].append({
                    'content': content[:100] + '...' if len(content) > 100 else content,
                    'feishu_record': feishu_record
                })
        
        # ç”Ÿæˆæ‘˜è¦
        comparison_result['summary'] = {
            'total_local': len(local_data),
            'total_feishu': len(feishu_data),
            'matched_count': len(comparison_result['matched_records']),
            'missing_in_feishu_count': len(comparison_result['missing_in_feishu']),
            'extra_in_feishu_count': len(comparison_result['extra_in_feishu']),
            'field_mismatch_count': len(comparison_result['field_mismatches']),
            'sync_accuracy': len(comparison_result['matched_records']) / len(local_data) * 100 if local_data else 0
        }
        
        return comparison_result
    
    def _compare_record_fields(self, local_record: Dict[str, Any], feishu_record: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ¯”å¯¹å•æ¡è®°å½•çš„å­—æ®µå€¼
        
        Args:
            local_record: æœ¬åœ°è®°å½•
            feishu_record: é£ä¹¦è®°å½•
            
        Returns:
            å­—æ®µæ¯”å¯¹ç»“æœ
        """
        feishu_fields = feishu_record.get('fields', {})
        
        # éœ€è¦æ¯”å¯¹çš„å­—æ®µ
        compare_fields = [
            'æ¨æ–‡åŸæ–‡å†…å®¹', 'ä½œè€…ï¼ˆè´¦å·ï¼‰', 'æ¨æ–‡é“¾æ¥', 'è¯é¢˜æ ‡ç­¾ï¼ˆHashtagï¼‰',
            'ç±»å‹æ ‡ç­¾', 'è¯„è®º', 'ç‚¹èµ', 'è½¬å‘'
        ]
        
        matched_fields = []
        mismatched_fields = []
        
        for field_name in compare_fields:
            local_value = local_record.get(field_name, '')
            feishu_value = feishu_fields.get(field_name, '')
            
            # æ•°å€¼å­—æ®µç‰¹æ®Šå¤„ç†
            if field_name in ['è¯„è®º', 'ç‚¹èµ', 'è½¬å‘']:
                local_value = int(local_value) if local_value else 0
                feishu_value = int(feishu_value) if feishu_value else 0
            else:
                local_value = str(local_value).strip()
                feishu_value = str(feishu_value).strip()
            
            if local_value == feishu_value:
                matched_fields.append(field_name)
            else:
                mismatched_fields.append({
                    'field': field_name,
                    'local_value': local_value,
                    'feishu_value': feishu_value
                })
        
        return {
            'all_match': len(mismatched_fields) == 0,
            'matched_fields': matched_fields,
            'mismatched_fields': mismatched_fields
        }
    
    def validate_sync_data(self, task_id: int = None) -> Dict[str, Any]:
        """
        éªŒè¯åŒæ­¥æ•°æ®çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
        
        Args:
            task_id: ä»»åŠ¡IDï¼Œå¦‚æœä¸ºNoneåˆ™éªŒè¯æ‰€æœ‰æ•°æ®
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        print(f"\n{'='*80}")
        print(f"ğŸ” å¼€å§‹é£ä¹¦æ•°æ®éªŒè¯æµç¨‹")
        print(f"ğŸ“‹ éªŒè¯å‚æ•°:")
        print(f"   - ä»»åŠ¡ID: {task_id if task_id else 'å…¨éƒ¨ä»»åŠ¡'}")
        print(f"   - éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        
        self.logger.info(f"ğŸ” å¼€å§‹é£ä¹¦æ•°æ®éªŒè¯æµç¨‹ï¼Œä»»åŠ¡ID: {task_id}")
        
        try:
            # 0. æ£€æŸ¥é£ä¹¦é…ç½®å®Œæ•´æ€§
            print(f"\nğŸ”§ æ­¥éª¤0: æ£€æŸ¥é£ä¹¦é…ç½®")
            required_fields = ['app_id', 'app_secret', 'spreadsheet_token', 'table_id']
            missing_fields = [field for field in required_fields if not self.feishu_config.get(field)]
            
            if missing_fields:
                error_msg = f"é£ä¹¦é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {', '.join(missing_fields)}"
                print(f"âŒ {error_msg}")
                self.logger.error(error_msg)
                return {'success': False, 'error': error_msg}
            
            if not self.feishu_config.get('enabled', False):
                error_msg = "é£ä¹¦åŒæ­¥åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·åœ¨ç³»ç»Ÿé…ç½®ä¸­å¯ç”¨é£ä¹¦åŒæ­¥"
                print(f"âŒ {error_msg}")
                self.logger.error(error_msg)
                return {'success': False, 'error': error_msg}
            
            print(f"âœ… é£ä¹¦é…ç½®æ£€æŸ¥é€šè¿‡")
            print(f"   - App ID: {self.feishu_config['app_id'][:10]}...")
            print(f"   - è¡¨æ ¼Token: {self.feishu_config['spreadsheet_token'][:10]}...")
            print(f"   - è¡¨æ ¼ID: {self.feishu_config['table_id']}")
            print(f"   - å¯ç”¨çŠ¶æ€: {self.feishu_config.get('enabled', False)}")
            
            # 1. è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ
            print(f"\nğŸ”‘ æ­¥éª¤1: è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ")
            access_token = self.get_feishu_access_token()
            if not access_token:
                error_msg = "æ— æ³•è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ"
                print(f"âŒ {error_msg}")
                self.logger.error(error_msg)
                return {'success': False, 'error': error_msg}
            print(f"âœ… è®¿é—®ä»¤ç‰Œè·å–æˆåŠŸ")
            
            # 2. è·å–é£ä¹¦è¡¨æ ¼å­—æ®µä¿¡æ¯
            print(f"\nğŸ“‹ æ­¥éª¤2: è·å–é£ä¹¦è¡¨æ ¼å­—æ®µä¿¡æ¯")
            field_info = self.get_feishu_table_fields(access_token)
            if not field_info:
                error_msg = "æ— æ³•è·å–é£ä¹¦è¡¨æ ¼å­—æ®µä¿¡æ¯"
                print(f"âŒ {error_msg}")
                self.logger.error(error_msg)
                return {'success': False, 'error': error_msg}
            
            field_id_to_name = field_info['field_id_to_name']
            print(f"âœ… è·å–åˆ° {len(field_id_to_name)} ä¸ªå­—æ®µ")
            
            # 3. è·å–é£ä¹¦è¡¨æ ¼è®°å½•
            print(f"\nğŸ“Š æ­¥éª¤3: è·å–é£ä¹¦è¡¨æ ¼è®°å½•")
            feishu_records = self.get_feishu_table_records(access_token)
            print(f"âœ… è·å–åˆ° {len(feishu_records)} æ¡é£ä¹¦è®°å½•")
            
            # 4. è§£æé£ä¹¦è®°å½•
            print(f"\nğŸ”„ æ­¥éª¤4: è§£æé£ä¹¦è®°å½•")
            parsed_feishu_data = self.parse_feishu_records(feishu_records, field_id_to_name)
            print(f"âœ… è§£æå®Œæˆ {len(parsed_feishu_data)} æ¡è®°å½•")
            
            # 5. è·å–æœ¬åœ°æ•°æ®
            print(f"\nğŸ’¾ æ­¥éª¤5: è·å–æœ¬åœ°æ•°æ®")
            local_data = self.get_local_sync_data(task_id)
            print(f"âœ… è·å–åˆ° {len(local_data)} æ¡æœ¬åœ°è®°å½•")
            
            # 6. æ•°æ®æ¯”å¯¹
            print(f"\nğŸ” æ­¥éª¤6: æ•°æ®æ¯”å¯¹åˆ†æ")
            comparison_result = self.compare_data(local_data, parsed_feishu_data)
            
            # 7. è¾“å‡ºæ¯”å¯¹ç»“æœ
            self._print_comparison_result(comparison_result)
            
            return {
                'success': True,
                'comparison_result': comparison_result,
                'field_info': field_info,
                'validation_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            error_msg = f"æ•°æ®éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}"
            print(f"âŒ {error_msg}")
            self.logger.error(error_msg)
            return {'success': False, 'error': error_msg}
    
    def _print_comparison_result(self, result: Dict[str, Any]):
        """
        æ‰“å°æ¯”å¯¹ç»“æœ
        
        Args:
            result: æ¯”å¯¹ç»“æœå­—å…¸
        """
        summary = result['summary']
        
        print(f"\nğŸ“Š æ•°æ®æ¯”å¯¹ç»“æœæ‘˜è¦:")
        print(f"   - æœ¬åœ°è®°å½•æ•°: {summary['total_local']}")
        print(f"   - é£ä¹¦è®°å½•æ•°: {summary['total_feishu']}")
        print(f"   - åŒ¹é…è®°å½•æ•°: {summary['matched_count']}")
        print(f"   - é£ä¹¦ç¼ºå¤±è®°å½•æ•°: {summary['missing_in_feishu_count']}")
        print(f"   - é£ä¹¦å¤šä½™è®°å½•æ•°: {summary['extra_in_feishu_count']}")
        print(f"   - å­—æ®µä¸åŒ¹é…è®°å½•æ•°: {summary['field_mismatch_count']}")
        print(f"   - åŒæ­¥å‡†ç¡®ç‡: {summary['sync_accuracy']:.2f}%")
        
        # æ˜¾ç¤ºè¯¦ç»†çš„ä¸åŒ¹é…ä¿¡æ¯
        if result['missing_in_feishu']:
            print(f"\nâŒ é£ä¹¦ä¸­ç¼ºå¤±çš„è®°å½• ({len(result['missing_in_feishu'])} æ¡):")
            for i, missing in enumerate(result['missing_in_feishu'][:5]):
                print(f"   {i+1}. {missing['content']}")
            if len(result['missing_in_feishu']) > 5:
                print(f"   ... è¿˜æœ‰ {len(result['missing_in_feishu']) - 5} æ¡")
        
        if result['extra_in_feishu']:
            print(f"\nâš ï¸ é£ä¹¦ä¸­å¤šä½™çš„è®°å½• ({len(result['extra_in_feishu'])} æ¡):")
            for i, extra in enumerate(result['extra_in_feishu'][:5]):
                print(f"   {i+1}. {extra['content']}")
            if len(result['extra_in_feishu']) > 5:
                print(f"   ... è¿˜æœ‰ {len(result['extra_in_feishu']) - 5} æ¡")
        
        if result['field_mismatches']:
            print(f"\nğŸ”„ å­—æ®µå€¼ä¸åŒ¹é…çš„è®°å½• ({len(result['field_mismatches'])} æ¡):")
            for i, mismatch in enumerate(result['field_mismatches'][:3]):
                print(f"   {i+1}. {mismatch['content']}")
                for field_mismatch in mismatch['mismatched_fields'][:3]:
                    print(f"      - {field_mismatch['field']}: æœ¬åœ°='{field_mismatch['local_value']}' vs é£ä¹¦='{field_mismatch['feishu_value']}'")
            if len(result['field_mismatches']) > 3:
                print(f"   ... è¿˜æœ‰ {len(result['field_mismatches']) - 3} æ¡")
        
        # æ€»ä½“è¯„ä¼°
        if summary['sync_accuracy'] >= 95:
            print(f"\nâœ… æ•°æ®åŒæ­¥è´¨é‡: ä¼˜ç§€ (å‡†ç¡®ç‡ {summary['sync_accuracy']:.2f}%)")
        elif summary['sync_accuracy'] >= 85:
            print(f"\nâš ï¸ æ•°æ®åŒæ­¥è´¨é‡: è‰¯å¥½ (å‡†ç¡®ç‡ {summary['sync_accuracy']:.2f}%)")
        else:
            print(f"\nâŒ æ•°æ®åŒæ­¥è´¨é‡: éœ€è¦æ”¹è¿› (å‡†ç¡®ç‡ {summary['sync_accuracy']:.2f}%)")

def main():
    """
    ä¸»å‡½æ•° - æ‰§è¡Œæ•°æ®éªŒè¯
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='é£ä¹¦æ•°æ®éªŒè¯å™¨')
    parser.add_argument('--task-id', type=int, help='æŒ‡å®šä»»åŠ¡IDè¿›è¡ŒéªŒè¯')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO if args.verbose else logging.WARNING,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºéªŒè¯å™¨å¹¶æ‰§è¡ŒéªŒè¯
    validator = FeishuDataValidator()
    result = validator.validate_sync_data(task_id=args.task_id)
    
    if result['success']:
        print(f"\nğŸ‰ æ•°æ®éªŒè¯å®Œæˆ")
    else:
        print(f"\nâŒ æ•°æ®éªŒè¯å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        sys.exit(1)

if __name__ == "__main__":
    main()