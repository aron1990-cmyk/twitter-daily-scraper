#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sqlite3
import json
from enhanced_tweet_scraper import EnhancedTwitterScraper, load_feishu_config_from_file

def test_fixed_feishu_sync():
    """æµ‹è¯•ä¿®å¤åçš„é£ä¹¦åŒæ­¥åŠŸèƒ½"""
    print("ğŸ”§ æµ‹è¯•ä¿®å¤åçš„é£ä¹¦åŒæ­¥åŠŸèƒ½...")
    
    # åŠ è½½é£ä¹¦é…ç½®
    feishu_config = load_feishu_config_from_file()
    if not feishu_config:
        print("âŒ æ— æ³•åŠ è½½é£ä¹¦é…ç½®")
        return
    
    print(f"âœ… é£ä¹¦é…ç½®åŠ è½½æˆåŠŸï¼Œå¯ç”¨çŠ¶æ€: {feishu_config.get('enabled')}")
    
    # æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
    conn = sqlite3.connect('twitter_scraper.db')
    cursor = conn.cursor()
    
    # ç»Ÿè®¡æ¨æ–‡æ•°æ®
    cursor.execute("""
        SELECT 
            COUNT(*) as total,
            COUNT(CASE WHEN publish_time IS NOT NULL AND publish_time != '' THEN 1 END) as has_publish_time,
            COUNT(CASE WHEN scraped_at IS NOT NULL THEN 1 END) as has_scraped_at,
            COUNT(CASE WHEN synced_to_feishu = 1 THEN 1 END) as synced
        FROM tweet_data
    """)
    
    stats = cursor.fetchone()
    total, has_publish_time, has_scraped_at, synced = stats
    
    print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
    print(f"  æ€»æ¨æ–‡æ•°: {total}")
    print(f"  æœ‰å‘å¸ƒæ—¶é—´: {has_publish_time} ({has_publish_time/total*100:.1f}%)")
    print(f"  æœ‰æŠ“å–æ—¶é—´: {has_scraped_at} ({has_scraped_at/total*100:.1f}%)")
    print(f"  å·²åŒæ­¥: {synced} ({synced/total*100:.1f}%)")
    
    # è·å–æ ·ä¾‹æ•°æ®æµ‹è¯•æ ¼å¼åŒ–
    cursor.execute("""
        SELECT username, content, publish_time, likes, comments, retweets, link, hashtags, scraped_at
        FROM tweet_data 
        ORDER BY id DESC 
        LIMIT 3
    """)
    
    sample_tweets = cursor.fetchall()
    conn.close()
    
    # æµ‹è¯•æ ¼å¼åŒ–åŠŸèƒ½
    print("\nğŸ§ª æµ‹è¯•ä¿®å¤åçš„æ ¼å¼åŒ–åŠŸèƒ½...")
    scraper = EnhancedTwitterScraper()
    
    # æ„å»ºæµ‹è¯•æ•°æ®
    test_tweets = []
    for username, content, publish_time, likes, comments, retweets, link, hashtags, scraped_at in sample_tweets:
        tweet_dict = {
            'username': username,
            'content': content,
            'publish_time': publish_time,
            'likes': likes,
            'comments': comments,
            'retweets': retweets,
            'link': link,
            'hashtags': hashtags.split(',') if hashtags else [],
            'scraped_at': scraped_at
        }
        test_tweets.append(tweet_dict)
    
    formatted_tweets = scraper.format_tweets_for_feishu(test_tweets)
    
    print("\nğŸ“ æ ¼å¼åŒ–ç»“æœåˆ†æ:")
    for i, (original, formatted) in enumerate(zip(test_tweets, formatted_tweets), 1):
        print(f"\n  æ¨æ–‡ {i}:")
        print(f"    åŸå§‹å‘å¸ƒæ—¶é—´: {original.get('publish_time') or 'âŒ æ— '}")
        print(f"    åŸå§‹æŠ“å–æ—¶é—´: {original.get('scraped_at') or 'âŒ æ— '}")
        print(f"    æ ¼å¼åŒ–åå‘å¸ƒæ—¶é—´: {formatted.get('å‘å¸ƒæ—¶é—´') or 'âŒ æ— '}")
        print(f"    æ ¼å¼åŒ–ååˆ›å»ºæ—¶é—´: {formatted.get('åˆ›å»ºæ—¶é—´') or 'âŒ æ— '}")
        
        # æ£€æŸ¥ä¿®å¤æ•ˆæœ
        if not original.get('publish_time') and formatted.get('å‘å¸ƒæ—¶é—´'):
            print(f"    âœ… ä¿®å¤æˆåŠŸï¼šä½¿ç”¨æŠ“å–æ—¶é—´ä½œä¸ºå‘å¸ƒæ—¶é—´")
        elif original.get('publish_time') and formatted.get('å‘å¸ƒæ—¶é—´'):
            print(f"    âœ… æ­£å¸¸ï¼šä½¿ç”¨åŸå§‹å‘å¸ƒæ—¶é—´")
        else:
            print(f"    âŒ ä»æœ‰é—®é¢˜ï¼šå‘å¸ƒæ—¶é—´ä¸ºç©º")
    
    # å¦‚æœé…ç½®æœ‰æ•ˆï¼Œæ‰§è¡Œå®é™…åŒæ­¥æµ‹è¯•
    is_placeholder = (
        feishu_config.get('app_id') == 'your_feishu_app_id' or
        feishu_config.get('spreadsheet_token') == 'your_spreadsheet_token'
    )
    
    if not is_placeholder and feishu_config.get('enabled'):
        print("\nğŸš€ æ‰§è¡Œå®é™…é£ä¹¦åŒæ­¥æµ‹è¯•...")
        
        # é‡ç½®åŒæ­¥çŠ¶æ€
        conn = sqlite3.connect('twitter_scraper.db')
        cursor = conn.cursor()
        cursor.execute("UPDATE tweet_data SET synced_to_feishu = 0")
        conn.commit()
        conn.close()
        
        print("âœ… å·²é‡ç½®æ‰€æœ‰æ¨æ–‡çš„åŒæ­¥çŠ¶æ€")
        
        # æ‰§è¡ŒåŒæ­¥
        success = scraper.sync_to_feishu(feishu_config)
        
        if success:
            print("âœ… é£ä¹¦åŒæ­¥æˆåŠŸï¼")
            
            # æ£€æŸ¥åŒæ­¥ç»“æœ
            conn = sqlite3.connect('twitter_scraper.db')
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM tweet_data WHERE synced_to_feishu = 1")
            synced_count = cursor.fetchone()[0]
            conn.close()
            
            print(f"ğŸ“Š åŒæ­¥ç»“æœ: {synced_count} æ¡æ¨æ–‡å·²åŒæ­¥åˆ°é£ä¹¦")
        else:
            print("âŒ é£ä¹¦åŒæ­¥å¤±è´¥")
    else:
        print("\nâš ï¸ è·³è¿‡å®é™…åŒæ­¥æµ‹è¯•ï¼ˆé…ç½®ä¸ºå ä½ç¬¦æˆ–æœªå¯ç”¨ï¼‰")
    
    print("\nğŸ¯ ä¿®å¤æ€»ç»“:")
    print("  âœ… å·²ä¿®å¤ format_tweets_for_feishu å‡½æ•°")
    print("  âœ… å½“ publish_time ä¸ºç©ºæ—¶ï¼Œè‡ªåŠ¨ä½¿ç”¨ scraped_at ä½œä¸ºå›é€€")
    print("  âœ… è¿™å°†è§£å†³é£ä¹¦ä¸­æ—¶é—´å­—æ®µæ˜¾ç¤ºå¼‚å¸¸çš„é—®é¢˜")
    print("  ğŸ“ å»ºè®®ï¼šé‡æ–°åŒæ­¥æ‰€æœ‰æ•°æ®ä»¥åº”ç”¨ä¿®å¤")

if __name__ == "__main__":
    test_fixed_feishu_sync()