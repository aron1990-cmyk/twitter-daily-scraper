
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Twitteré‡‡é›†ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å™¨
è§£å†³æ•ˆç‡ã€å»é‡ã€å†…å®¹ä¸¢å¤±ã€æœç´¢é™åˆ¶å’Œä»·å€¼è¯†åˆ«é—®é¢˜
"""

import asyncio
import time
import threading
import logging
import hashlib
import re
from typing import List, Any, Callable, Optional, Dict, Set
from contextlib import asynccontextmanager
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from collections import defaultdict

# ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸ä¾èµ–aiohttp
AIOHTTP_AVAILABLE = False

try:
    from models import PerformanceMetrics
except ImportError:
    # å¦‚æœmodelsä¸å­˜åœ¨ï¼Œåˆ›å»ºç®€å•çš„æ•°æ®ç±»
    class PerformanceMetrics:
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)
        
        @property
        def success_rate(self):
            total = getattr(self, 'success_count', 0) + getattr(self, 'error_count', 0)
            return getattr(self, 'success_count', 0) / max(total, 1)

logger = logging.getLogger(__name__)


class AdvancedDeduplicator:
    """
    é«˜çº§å»é‡å™¨ - è§£å†³é—®é¢˜2ï¼šæŠ“å–çš„å†…å®¹éœ€è¦è¿›è¡ŒæŸ¥é‡
    """
    
    def __init__(self):
        self.content_hashes: Set[str] = set()
        self.link_cache: Set[str] = set()
        self.similarity_cache: Dict[str, str] = {}
        self.stats = {'duplicates_removed': 0, 'total_processed': 0}
    
    def generate_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡"""
        # æ¸…ç†å†…å®¹
        cleaned = re.sub(r'\s+', ' ', content.lower().strip())
        cleaned = re.sub(r'[^\w\s]', '', cleaned)  # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        return hashlib.md5(cleaned.encode('utf-8')).hexdigest()
    
    def calculate_similarity(self, content1: str, content2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå†…å®¹çš„ç›¸ä¼¼åº¦"""
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        return intersection / union if union > 0 else 0.0
    
    def is_duplicate(self, tweet: Dict[str, Any], similarity_threshold: float = 0.8) -> bool:
        """é«˜çº§å»é‡æ£€æµ‹"""
        self.stats['total_processed'] += 1
        
        content = tweet.get('content', '')
        link = tweet.get('link', '')
        
        # 1. é“¾æ¥å»é‡
        if link and link in self.link_cache:
            self.stats['duplicates_removed'] += 1
            return True
        
        # 2. å†…å®¹å“ˆå¸Œå»é‡
        content_hash = self.generate_content_hash(content)
        if content_hash in self.content_hashes:
            self.stats['duplicates_removed'] += 1
            return True
        
        # 3. ç›¸ä¼¼åº¦å»é‡
        for cached_hash, cached_content in self.similarity_cache.items():
            similarity = self.calculate_similarity(content, cached_content)
            if similarity >= similarity_threshold:
                self.stats['duplicates_removed'] += 1
                return True
        
        # è®°å½•æ–°å†…å®¹
        if link:
            self.link_cache.add(link)
        self.content_hashes.add(content_hash)
        
        # é™åˆ¶ç›¸ä¼¼åº¦ç¼“å­˜å¤§å°
        if len(self.similarity_cache) < 1000:
            self.similarity_cache[content_hash] = content
        
        return False


class TweetValueAnalyzer:
    """
    æ¨æ–‡ä»·å€¼åˆ†æå™¨ - è§£å†³é—®é¢˜5ï¼šå¦‚ä½•è¯†åˆ«ä»€ä¹ˆæ¨æ–‡æ˜¯æ²¡ç”¨çš„ï¼Œä»€ä¹ˆæ¨æ–‡æ˜¯æœ‰ç”¨çš„
    """
    
    def __init__(self):
        # é«˜ä»·å€¼å†…å®¹å…³é”®è¯
        self.value_keywords = {
            'tech': ['AI', 'GPT', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'ChatGPT', 'æ·±åº¦å­¦ä¹ ', 'ç®—æ³•', 'Python', 'JavaScript', 'åŒºå—é“¾', 'åŠ å¯†è´§å¸'],
            'business': ['åˆ›ä¸š', 'æŠ•èµ„', 'èèµ„', 'å•†ä¸šæ¨¡å¼', 'è¥é”€', 'å¢é•¿', 'å˜ç°', 'å•†ä¸š', 'å¸‚åœº'],
            'trends': ['è¶‹åŠ¿', 'æœªæ¥', 'é¢„æµ‹', 'åˆ†æ', 'æ´å¯Ÿ', 'æŠ¥å‘Š', 'æ•°æ®', 'ç ”ç©¶'],
            'engagement': ['è®¨è®º', 'åˆ†äº«', 'è§‚ç‚¹', 'æ€è€ƒ', 'ç»éªŒ', 'æ•™ç¨‹', 'æŒ‡å—', 'æŠ€å·§']
        }
        
        # æ— ä»·å€¼å†…å®¹æ¨¡å¼
        self.low_value_patterns = [
            r'^(è½¬å‘|RT)\s*@',  # çº¯è½¬å‘
            r'^[ğŸ˜€-ğŸ™]{3,}$',   # çº¯è¡¨æƒ…
            r'^\s*$',          # ç©ºå†…å®¹
            r'^(å¥½çš„|è°¢è°¢|ğŸ‘|â¤ï¸)\s*$',  # ç®€å•å›å¤
            r'^(æ—©å®‰|æ™šå®‰|åˆå®‰)\s*[ğŸ˜€-ğŸ™]*\s*$',  # é—®å€™è¯­
        ]
    
    def calculate_tweet_value_score(self, tweet: Dict[str, Any]) -> float:
        """è®¡ç®—æ¨æ–‡ä»·å€¼åˆ†æ•°"""
        score = 0.0
        content = tweet.get('content', '').lower()
        
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºä½ä»·å€¼å†…å®¹
        for pattern in self.low_value_patterns:
            if re.match(pattern, content):
                return 0.0  # ç›´æ¥åˆ¤å®šä¸ºæ— ä»·å€¼
        
        # 2. å†…å®¹é•¿åº¦è¯„åˆ†
        content_length = len(content)
        if content_length > 200:
            score += 2.0
        elif content_length > 100:
            score += 1.0
        elif content_length < 20:
            score -= 1.0
        
        # 3. å…³é”®è¯åŒ¹é…è¯„åˆ†
        for category, keywords in self.value_keywords.items():
            matches = sum(1 for keyword in keywords if keyword.lower() in content)
            if matches > 0:
                score += matches * 1.5
        
        # 4. äº’åŠ¨æ•°æ®è¯„åˆ†
        likes = tweet.get('likes', 0)
        comments = tweet.get('comments', 0)
        retweets = tweet.get('retweets', 0)
        
        total_engagement = likes + comments * 2 + retweets * 3
        if total_engagement > 100:
            score += 3.0
        elif total_engagement > 50:
            score += 2.0
        elif total_engagement > 10:
            score += 1.0
        
        # 5. åª’ä½“å†…å®¹è¯„åˆ†
        media = tweet.get('media', {})
        if media.get('images') or media.get('videos'):
            score += 1.5
        
        # 6. æ—¶é—´æ–°é²œåº¦è¯„åˆ†
        publish_time = tweet.get('publish_time')
        if publish_time:
            try:
                pub_time = datetime.fromisoformat(publish_time.replace('Z', '+00:00'))
                time_diff = datetime.now() - pub_time.replace(tzinfo=None)
                if time_diff < timedelta(hours=24):
                    score += 1.0
                elif time_diff < timedelta(days=7):
                    score += 0.5
            except:
                pass
        
        return max(0.0, score)
    
    def is_high_value_tweet(self, tweet: Dict[str, Any], threshold: float = 3.0) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜ä»·å€¼æ¨æ–‡"""
        return self.calculate_tweet_value_score(tweet) >= threshold


class EnhancedSearchOptimizer:
    """
    å¢å¼ºæœç´¢ä¼˜åŒ–å™¨ - è§£å†³é—®é¢˜4ï¼šæœç´¢å…³é”®è¯è·å¾—çš„æ¨æ–‡å¾ˆå°‘
    """
    
    def get_enhanced_search_queries(self, keyword: str) -> List[str]:
        """ç”Ÿæˆå¢å¼ºçš„æœç´¢æŸ¥è¯¢ï¼Œæé«˜æœç´¢ç»“æœæ•°é‡"""
        queries = []
        
        # 1. åŸºç¡€æŸ¥è¯¢
        queries.append(keyword)
        
        # 2. æ·»åŠ æ—¶é—´è¿‡æ»¤ï¼ˆæœ€è¿‘æ¨æ–‡ï¼‰
        queries.append(f"{keyword} since:2024-01-01")
        
        # 3. æ·»åŠ è¯­è¨€è¿‡æ»¤
        if any(ord(char) > 127 for char in keyword):  # åŒ…å«ä¸­æ–‡
            queries.append(f"{keyword} lang:zh")
        else:
            queries.append(f"{keyword} lang:en")
        
        # 4. æ·»åŠ æœ€å°äº’åŠ¨è¿‡æ»¤
        queries.append(f"{keyword} min_replies:1")
        queries.append(f"{keyword} min_faves:5")
        
        # 5. æ’é™¤è½¬å‘ï¼ˆè·å–åŸåˆ›å†…å®¹ï¼‰
        queries.append(f"{keyword} -filter:retweets")
        
        # 6. åŒ…å«é“¾æ¥çš„æ¨æ–‡
        queries.append(f"{keyword} filter:links")
        
        # 7. åŒ…å«åª’ä½“çš„æ¨æ–‡
        queries.append(f"{keyword} filter:media")
        
        return queries
    
    def optimize_scroll_strategy(self, current_tweets: int, target_tweets: int, 
                               scroll_attempts: int) -> Dict[str, Any]:
        """ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥ - è§£å†³é—®é¢˜3ï¼šå†…å®¹ä¸¢å¤±é—®é¢˜"""
        # è®¡ç®—æ»šåŠ¨æ•ˆç‡
        efficiency = current_tweets / max(scroll_attempts, 1)
        
        strategy = {
            'scroll_distance': 1500,  # é»˜è®¤æ»šåŠ¨è·ç¦»
            'wait_time': 0.3,         # é»˜è®¤ç­‰å¾…æ—¶é—´
            'max_scrolls': 20,        # æœ€å¤§æ»šåŠ¨æ¬¡æ•°
            'should_continue': True,
            'aggressive_mode': False   # æ¿€è¿›æ¨¡å¼
        }
        
        # æ ¹æ®æ•ˆç‡è°ƒæ•´ç­–ç•¥
        if efficiency < 0.5:  # æ•ˆç‡ä½ï¼Œå¯ç”¨æ¿€è¿›æ¨¡å¼
            strategy['scroll_distance'] = 2500  # å¤§å¹…å¢åŠ æ»šåŠ¨è·ç¦»
            strategy['wait_time'] = 0.8         # å¢åŠ ç­‰å¾…æ—¶é—´
            strategy['max_scrolls'] = 50        # å¤§å¹…å¢åŠ æœ€å¤§æ»šåŠ¨æ¬¡æ•°
            strategy['aggressive_mode'] = True
        elif efficiency > 2.0:  # æ•ˆç‡é«˜
            strategy['scroll_distance'] = 1000  # å‡å°‘æ»šåŠ¨è·ç¦»
            strategy['wait_time'] = 0.2         # å‡å°‘ç­‰å¾…æ—¶é—´
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢æ»šåŠ¨
        if scroll_attempts > strategy['max_scrolls']:
            strategy['should_continue'] = False
        
        # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡æ•°é‡çš„80%ï¼Œå¯ä»¥è€ƒè™‘åœæ­¢
        if current_tweets >= target_tweets * 0.8:
            strategy['should_continue'] = False
        
        return strategy


class HighSpeedCollector:
    """
    é«˜é€Ÿé‡‡é›†å™¨ - è§£å†³é—®é¢˜1ï¼š1å°æ—¶å†…é‡‡é›†1500æ¡æ¨æ–‡
    """
    
    def __init__(self):
        self.deduplicator = AdvancedDeduplicator()
        self.value_analyzer = TweetValueAnalyzer()
        self.search_optimizer = EnhancedSearchOptimizer()
        self.stats = {
            'total_collected': 0,
            'high_value_tweets': 0,
            'processing_time': 0,
            'collection_rate': 0  # æ¨æ–‡/åˆ†é’Ÿ
        }
    
    def calculate_target_rate(self, target_tweets: int = 1500, time_limit_hours: int = 1) -> float:
        """è®¡ç®—ç›®æ ‡é‡‡é›†é€Ÿç‡"""
        return target_tweets / (time_limit_hours * 60)  # æ¨æ–‡/åˆ†é’Ÿ
    
    def process_tweets_batch(self, tweets: List[Dict[str, Any]], 
                           enable_dedup: bool = True,
                           enable_value_filter: bool = True) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†æ¨æ–‡ï¼šå»é‡ã€ä»·å€¼ç­›é€‰ã€ä¼˜åŒ–"""
        start_time = time.time()
        processed_tweets = []
        
        for tweet in tweets:
            # å»é‡æ£€æŸ¥
            if enable_dedup and self.deduplicator.is_duplicate(tweet):
                continue
            
            # ä»·å€¼ç­›é€‰
            if enable_value_filter:
                value_score = self.value_analyzer.calculate_tweet_value_score(tweet)
                tweet['value_score'] = value_score
                
                if self.value_analyzer.is_high_value_tweet(tweet):
                    self.stats['high_value_tweets'] += 1
                    processed_tweets.append(tweet)
                elif value_score > 1.0:  # ä¸­ç­‰ä»·å€¼ä¹Ÿä¿ç•™
                    processed_tweets.append(tweet)
            else:
                tweet['value_score'] = self.value_analyzer.calculate_tweet_value_score(tweet)
                processed_tweets.append(tweet)
        
        # æŒ‰ä»·å€¼åˆ†æ•°æ’åº
        processed_tweets.sort(key=lambda x: x.get('value_score', 0), reverse=True)
        
        self.stats['total_collected'] += len(processed_tweets)
        self.stats['processing_time'] += time.time() - start_time
        
        # è®¡ç®—é‡‡é›†é€Ÿç‡
        if self.stats['processing_time'] > 0:
            self.stats['collection_rate'] = (self.stats['total_collected'] / self.stats['processing_time']) * 60
        
        return processed_tweets
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            'collection_stats': self.stats.copy(),
            'deduplication_stats': self.deduplicator.stats.copy(),
            'efficiency_metrics': {
                'deduplication_rate': self.deduplicator.stats['duplicates_removed'] / max(self.deduplicator.stats['total_processed'], 1),
                'high_value_rate': self.stats['high_value_tweets'] / max(self.stats['total_collected'], 1),
                'collection_rate_per_minute': self.stats['collection_rate'],
                'target_rate_1500_per_hour': 25.0,  # ç›®æ ‡ï¼š25æ¨æ–‡/åˆ†é’Ÿ
                'rate_achievement': (self.stats['collection_rate'] / 25.0) * 100 if self.stats['collection_rate'] > 0 else 0
            }
        }


class AsyncBatchProcessor:
    """å¼‚æ­¥æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 10, batch_size: int = 50):
        self.max_workers = max_workers
        self.batch_size = batch_size
        self.semaphore = asyncio.Semaphore(max_workers)
    
    async def process_batch(self, items: List[Any], processor: Callable) -> List[Any]:
        """æ‰¹é‡å¤„ç†é¡¹ç›®"""
        results = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(items), self.batch_size):
            batch = items[i:i + self.batch_size]
            batch_tasks = [self._process_item(item, processor) for item in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            results.extend(batch_results)
        
        return results
    
    async def _process_item(self, item: Any, processor: Callable) -> Any:
        """å¤„ç†å•ä¸ªé¡¹ç›®"""
        async with self.semaphore:
            try:
                if asyncio.iscoroutinefunction(processor):
                    return await processor(item)
                else:
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(None, processor, item)
            except Exception as e:
                logger.error(f"å¤„ç†é¡¹ç›®å¤±è´¥: {e}")
                return e



class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨"""
    
    def __init__(self, max_memory_mb: int = 1024):
        self.max_memory_mb = max_memory_mb
        self._data_cache = {}
        self._cache_size = 0
    
    def cache_data(self, key: str, data: Any, size_mb: float):
        """ç¼“å­˜æ•°æ®"""
        # æ£€æŸ¥å†…å­˜é™åˆ¶
        if self._cache_size + size_mb > self.max_memory_mb:
            self._evict_cache()
        
        self._data_cache[key] = data
        self._cache_size += size_mb
    
    def get_cached_data(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        return self._data_cache.get(key)
    
    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        return self._cache_size
    
    def _evict_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        # ç®€å•çš„LRUç­–ç•¥ï¼šæ¸…ç†ä¸€åŠç¼“å­˜
        keys_to_remove = list(self._data_cache.keys())[:len(self._data_cache) // 2]
        for key in keys_to_remove:
            del self._data_cache[key]
        
        self._cache_size = self._cache_size // 2
        logger.info(f"ç¼“å­˜æ¸…ç†å®Œæˆï¼Œå½“å‰å¤§å°: {self._cache_size}MB")

class ThreadSafeQueue:
    """çº¿ç¨‹å®‰å…¨é˜Ÿåˆ—"""
    
    def __init__(self, maxsize: int = 1000):
        self._queue = Queue(maxsize=maxsize)
        self._shutdown = threading.Event()
    
    def put(self, item: Any, timeout: Optional[float] = None):
        """æ·»åŠ é¡¹ç›®åˆ°é˜Ÿåˆ—"""
        if not self._shutdown.is_set():
            self._queue.put(item, timeout=timeout)
    
    def get(self, timeout: Optional[float] = None) -> Optional[Any]:
        """ä»é˜Ÿåˆ—è·å–é¡¹ç›®"""
        try:
            return self._queue.get(timeout=timeout)
        except Empty:
            return None
    
    def shutdown(self):
        """å…³é—­é˜Ÿåˆ—"""
        self._shutdown.set()
    
    def is_shutdown(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å…³é—­"""
        return self._shutdown.is_set()

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.metrics: List[PerformanceMetrics] = []
    
    @asynccontextmanager
    async def profile(self, operation_name: str):
        """æ€§èƒ½åˆ†æä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        import psutil
        
        start_time = time.time()
        start_memory = psutil.virtual_memory().percent
        start_cpu = psutil.cpu_percent()
        
        success_count = 0
        error_count = 0
        
        def _create_counter():
            """åˆ›å»ºè®¡æ•°å™¨"""
            nonlocal success_count, error_count
            
            class Counter:
                def success(self):
                    nonlocal success_count
                    success_count += 1
                
                def error(self):
                    nonlocal error_count
                    error_count += 1
            
            return Counter()
        
        try:
            yield _create_counter()
        finally:
            end_time = time.time()
            end_memory = psutil.virtual_memory().percent
            end_cpu = psutil.cpu_percent()
            
            metrics = PerformanceMetrics(
                start_time=start_time,
                end_time=end_time,
                duration=end_time - start_time,
                memory_usage=end_memory - start_memory,
                cpu_usage=end_cpu - start_cpu,
                success_count=success_count,
                error_count=error_count
            )
            
            self.metrics.append(metrics)
            
            logger.info(
                f"æ€§èƒ½åˆ†æ - {operation_name}",
                extra={
                    'duration': metrics.duration,
                    'memory_delta': metrics.memory_usage,
                    'cpu_delta': metrics.cpu_usage,
                    'success_rate': metrics.success_rate
                }
            )
    
    def get_average_metrics(self) -> Optional[Dict[str, float]]:
        """è·å–å¹³å‡æ€§èƒ½æŒ‡æ ‡"""
        if not self.metrics:
            return None
        
        return {
            'avg_duration': sum(m.duration for m in self.metrics) / len(self.metrics),
            'avg_memory_usage': sum(m.memory_usage for m in self.metrics) / len(self.metrics),
            'avg_cpu_usage': sum(m.cpu_usage for m in self.metrics) / len(self.metrics),
            'avg_success_rate': sum(m.success_rate for m in self.metrics) / len(self.metrics)
        }

# å…¨å±€å®ä¾‹
batch_processor = AsyncBatchProcessor()
memory_optimizer = MemoryOptimizer()
performance_profiler = PerformanceProfiler()
