#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„æ¨æ–‡å†…å®¹æå–
è§£å†³æ¨æ–‡å†…å®¹é‡å¤å’Œæ ¼å¼é—®é¢˜
"""

import asyncio
import logging
import sys
import os
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ads_browser_launcher import AdsPowerLauncher
from twitter_parser import TwitterParser

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# AdsPoweré…ç½®
ADS_POWER_CONFIG = {
    'user_id': 'k11p9ypc',
    'open_tabs': 1,
    'launch_args': [],
    'headless': False,
    'disable_password_filling': False,
    'clear_cache_after_closing': False,
    'enable_password_saving': False
}

class OptimizedContentExtractor:
    """ä¼˜åŒ–çš„æ¨æ–‡å†…å®¹æå–å™¨"""
    
    def __init__(self, parser: TwitterParser):
        self.parser = parser
        self.logger = logging.getLogger(__name__)
    
    def clean_tweet_content(self, content: str) -> str:
        """æ¸…ç†æ¨æ–‡å†…å®¹ï¼Œå»é™¤é‡å¤å’Œæ ¼å¼é—®é¢˜"""
        if not content:
            return ""
        
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content.strip())
        
        # å»é™¤é‡å¤çš„ç”¨æˆ·åæ¨¡å¼ (å¦‚: "Elon Musk Elon Musk @elonmusk")
        content = re.sub(r'(\w+\s+\w+)\s+\1', r'\1', content)
        
        # å»é™¤é‡å¤çš„æ•°å­—æ¨¡å¼ (å¦‚: "4,8K 4,8K 4,8K")
        content = re.sub(r'(\d+[,.]?\d*[KMB]?)\s+\1(\s+\1)*', r'\1', content)
        
        # å»é™¤é‡å¤çš„ç¬¦å·æ¨¡å¼
        content = re.sub(r'(Â·\s*)+', 'Â· ', content)
        
        # å»é™¤æœ«å°¾çš„ç»Ÿè®¡æ•°æ®æ¨¡å¼
        content = re.sub(r'\s*Â·\s*[\d,KMB.\s]+$', '', content)
        
        # å»é™¤å¼€å¤´çš„ç”¨æˆ·åé‡å¤
        content = re.sub(r'^(@?\w+\s+){2,}', '', content)
        
        # å»é™¤å¤šä½™çš„ç‚¹å’Œç©ºæ ¼
        content = re.sub(r'\s*Â·\s*$', '', content)
        
        return content.strip()
    
    def extract_clean_username(self, element) -> str:
        """æå–å¹²å‡€çš„ç”¨æˆ·å"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            username_selectors = [
                '[data-testid="User-Name"] [dir="ltr"]',
                '[data-testid="User-Name"] span',
                'a[href^="/"][role="link"] span',
                '[dir="ltr"] span'
            ]
            
            for selector in username_selectors:
                username_element = element.query_selector(selector)
                if username_element:
                    username = username_element.text_content().strip()
                    # æ¸…ç†ç”¨æˆ·å
                    username = re.sub(r'^@', '', username)
                    username = re.sub(r'\s.*', '', username)  # åªä¿ç•™ç¬¬ä¸€ä¸ªè¯
                    if username and not re.match(r'^\d+[KMB]?$', username):
                        return username
            
            # ä»é“¾æ¥ä¸­æå–ç”¨æˆ·å
            link_element = element.query_selector('a[href^="/"][role="link"]')
            if link_element:
                href = link_element.get_attribute('href')
                if href:
                    match = re.match(r'^/([^/]+)', href)
                    if match:
                        return match.group(1)
            
            return 'unknown'
            
        except Exception as e:
            self.logger.debug(f"æå–ç”¨æˆ·åå¤±è´¥: {e}")
            return 'unknown'
    
    def extract_clean_content(self, element) -> str:
        """æå–å¹²å‡€çš„æ¨æ–‡å†…å®¹"""
        try:
            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                '[data-testid="tweetText"]',
                '[lang] span',
                'div[dir="auto"] span'
            ]
            
            content_parts = []
            
            for selector in content_selectors:
                content_elements = element.query_selector_all(selector)
                for content_element in content_elements:
                    text = content_element.text_content().strip()
                    if text and text not in content_parts:
                        content_parts.append(text)
            
            # åˆå¹¶å†…å®¹
            raw_content = ' '.join(content_parts)
            
            # æ¸…ç†å†…å®¹
            clean_content = self.clean_tweet_content(raw_content)
            
            return clean_content if clean_content else 'No content available'
            
        except Exception as e:
            self.logger.debug(f"æå–æ¨æ–‡å†…å®¹å¤±è´¥: {e}")
            return 'No content available'
    
    def extract_engagement_data(self, element) -> Dict[str, int]:
        """æå–äº’åŠ¨æ•°æ®"""
        engagement = {'likes': 0, 'comments': 0, 'retweets': 0}
        
        try:
            # æŸ¥æ‰¾äº’åŠ¨æ•°æ®
            engagement_selectors = {
                'likes': ['[data-testid="like"]', '[aria-label*="like"]'],
                'comments': ['[data-testid="reply"]', '[aria-label*="repl"]'],
                'retweets': ['[data-testid="retweet"]', '[aria-label*="repost"]']
            }
            
            for metric, selectors in engagement_selectors.items():
                for selector in selectors:
                    metric_element = element.query_selector(selector)
                    if metric_element:
                        # æŸ¥æ‰¾æ•°å­—
                        text = metric_element.text_content()
                        numbers = re.findall(r'[\d,]+[KMB]?', text)
                        if numbers:
                            engagement[metric] = self.parse_number(numbers[0])
                            break
            
            return engagement
            
        except Exception as e:
            self.logger.debug(f"æå–äº’åŠ¨æ•°æ®å¤±è´¥: {e}")
            return engagement
    
    def parse_number(self, num_str: str) -> int:
        """è§£ææ•°å­—å­—ç¬¦ä¸² (å¦‚: 1.2K -> 1200)"""
        try:
            num_str = num_str.replace(',', '')
            if num_str.endswith('K'):
                return int(float(num_str[:-1]) * 1000)
            elif num_str.endswith('M'):
                return int(float(num_str[:-1]) * 1000000)
            elif num_str.endswith('B'):
                return int(float(num_str[:-1]) * 1000000000)
            else:
                return int(num_str)
        except (ValueError, IndexError):
            return 0
    
    def extract_media_content(self, element) -> Dict[str, List[Dict]]:
        """æå–åª’ä½“å†…å®¹"""
        media = {'images': [], 'videos': []}
        
        try:
            # æå–å›¾ç‰‡
            img_elements = element.query_selector_all('img[src*="pbs.twimg.com"]')
            for img in img_elements:
                src = img.get_attribute('src')
                alt = img.get_attribute('alt') or 'Image'
                if src:
                    media['images'].append({
                        'type': 'image',
                        'url': src,
                        'description': alt,
                        'original_url': src
                    })
            
            # æå–è§†é¢‘
            video_elements = element.query_selector_all('video, [data-testid="videoPlayer"]')
            for video in video_elements:
                poster = video.get_attribute('poster')
                if poster:
                    media['videos'].append({
                        'type': 'video',
                        'poster': poster,
                        'description': 'Video content'
                    })
            
            return media
            
        except Exception as e:
            self.logger.debug(f"æå–åª’ä½“å†…å®¹å¤±è´¥: {e}")
            return media
    
    async def parse_tweet_element_optimized(self, element) -> Optional[Dict[str, Any]]:
        """ä¼˜åŒ–çš„æ¨æ–‡å…ƒç´ è§£æ"""
        try:
            # æå–åŸºæœ¬ä¿¡æ¯
            username = self.extract_clean_username(element)
            content = self.extract_clean_content(element)
            
            # æå–é“¾æ¥
            link = ''
            try:
                link_element = await element.query_selector('a[href*="/status/"]')
                if link_element:
                    href = await link_element.get_attribute('href')
                    if href:
                        if href.startswith('/'):
                            link = f'https://x.com{href}'
                        else:
                            link = href
            except Exception:
                pass
            
            # æå–æ—¶é—´
            publish_time = ''
            try:
                time_element = await element.query_selector('time')
                if time_element:
                    datetime_attr = await time_element.get_attribute('datetime')
                    if datetime_attr:
                        publish_time = datetime_attr
            except Exception:
                pass
            
            # æå–äº’åŠ¨æ•°æ®
            engagement = self.extract_engagement_data(element)
            
            # æå–åª’ä½“å†…å®¹
            media = self.extract_media_content(element)
            
            # ç¡®å®šå¸–å­ç±»å‹
            post_type = 'çº¯æ–‡æœ¬'
            if media['images']:
                post_type = 'å›¾æ–‡'
            elif media['videos']:
                post_type = 'è§†é¢‘'
            
            # æ„å»ºæ¨æ–‡æ•°æ®
            tweet_data = {
                'username': username,
                'content': content,
                'publish_time': publish_time,
                'link': link,
                'likes': engagement['likes'],
                'comments': engagement['comments'],
                'retweets': engagement['retweets'],
                'media': media,
                'post_type': post_type
            }
            
            # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
            if username != 'unknown' and (content != 'No content available' or media['images'] or media['videos']):
                return tweet_data
            
            return None
            
        except Exception as e:
            self.logger.debug(f"è§£ææ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return None

async def test_optimized_content_extraction():
    """æµ‹è¯•ä¼˜åŒ–çš„å†…å®¹æå–"""
    launcher = None
    parser = None
    
    try:
        # å¯åŠ¨æµè§ˆå™¨
        logger.info("ğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        launcher = AdsPowerLauncher(ADS_POWER_CONFIG)
        browser_info = launcher.start_browser()
        launcher.wait_for_browser_ready()
        debug_port = launcher.get_debug_port()
        logger.info(f"âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼Œè°ƒè¯•ç«¯å£: {debug_port}")
        
        # åˆå§‹åŒ–TwitterParser
        logger.info("ğŸ”§ åˆå§‹åŒ–TwitterParser...")
        parser = TwitterParser(debug_port=debug_port)
        await parser.initialize()
        logger.info("âœ… TwitterParseråˆå§‹åŒ–å®Œæˆ")
        
        # å¯¼èˆªåˆ°æµ‹è¯•é¡µé¢
        logger.info("ğŸ”„ å¯¼èˆªåˆ°@elonmuské¡µé¢...")
        try:
            await parser.navigate_to_profile('elonmusk')
        except Exception:
            await parser.page.goto('https://x.com/elonmusk', wait_until='domcontentloaded')
        
        await asyncio.sleep(5)
        
        # åˆ›å»ºä¼˜åŒ–å†…å®¹æå–å™¨
        extractor = OptimizedContentExtractor(parser)
        
        # è·å–æ¨æ–‡å…ƒç´ 
        logger.info("ğŸ” è·å–æ¨æ–‡å…ƒç´ ...")
        await parser.page.wait_for_selector('[data-testid="tweet"]', timeout=10000)
        tweet_elements = await parser.page.query_selector_all('[data-testid="tweet"]')
        
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(tweet_elements)} ä¸ªæ¨æ–‡å…ƒç´ ")
        
        # å¯¹æ¯”æµ‹è¯•ï¼šåŸå§‹è§£æ vs ä¼˜åŒ–è§£æ
        original_tweets = []
        optimized_tweets = []
        
        for i, element in enumerate(tweet_elements[:5]):  # æµ‹è¯•å‰5æ¡
            logger.info(f"\nğŸ” æµ‹è¯•æ¨æ–‡ {i+1}:")
            
            # åŸå§‹è§£æ
            try:
                original_tweet = await parser.parse_tweet_element(element)
                original_tweets.append(original_tweet)
                logger.info(f"  ğŸ“ åŸå§‹å†…å®¹: {original_tweet.get('content', 'N/A')[:100]}...")
            except Exception as e:
                logger.warning(f"  âŒ åŸå§‹è§£æå¤±è´¥: {e}")
                original_tweets.append(None)
            
            # ä¼˜åŒ–è§£æ
            try:
                optimized_tweet = await extractor.parse_tweet_element_optimized(element)
                optimized_tweets.append(optimized_tweet)
                if optimized_tweet:
                    logger.info(f"  âœ¨ ä¼˜åŒ–å†…å®¹: {optimized_tweet.get('content', 'N/A')[:100]}...")
                    logger.info(f"  ğŸ‘¤ ç”¨æˆ·å: {optimized_tweet.get('username', 'N/A')}")
                    logger.info(f"  ğŸ’– äº’åŠ¨: {optimized_tweet.get('likes', 0)} èµ, {optimized_tweet.get('comments', 0)} è¯„è®º")
                else:
                    logger.warning("  âŒ ä¼˜åŒ–è§£æè¿”å›ç©ºç»“æœ")
            except Exception as e:
                logger.warning(f"  âŒ ä¼˜åŒ–è§£æå¤±è´¥: {e}")
                optimized_tweets.append(None)
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        comparison_report = {
            'timestamp': datetime.now().isoformat(),
            'test_type': 'content_extraction_optimization',
            'total_tweets_tested': len(tweet_elements[:5]),
            'original_success_count': len([t for t in original_tweets if t]),
            'optimized_success_count': len([t for t in optimized_tweets if t]),
            'improvements': [],
            'original_tweets': original_tweets,
            'optimized_tweets': optimized_tweets
        }
        
        # åˆ†ææ”¹è¿›
        for i, (orig, opt) in enumerate(zip(original_tweets, optimized_tweets)):
            if orig and opt:
                improvement = {
                    'tweet_index': i + 1,
                    'content_length_change': len(opt.get('content', '')) - len(orig.get('content', '')),
                    'content_cleaned': orig.get('content', '') != opt.get('content', ''),
                    'username_improved': orig.get('username', 'unknown') != opt.get('username', 'unknown'),
                    'engagement_data_complete': all([
                        opt.get('likes', 0) > 0,
                        opt.get('comments', 0) >= 0,
                        opt.get('retweets', 0) >= 0
                    ])
                }
                comparison_report['improvements'].append(improvement)
        
        # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
        report_file = f"content_extraction_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ’¾ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # è¾“å‡ºæµ‹è¯•ç»“æœ
        logger.info("\nğŸ“Š å†…å®¹æå–ä¼˜åŒ–æµ‹è¯•ç»“æœ:")
        logger.info(f"  ğŸ¯ æµ‹è¯•æ¨æ–‡æ•°é‡: {comparison_report['total_tweets_tested']}")
        logger.info(f"  âœ… åŸå§‹è§£ææˆåŠŸ: {comparison_report['original_success_count']}")
        logger.info(f"  ğŸš€ ä¼˜åŒ–è§£ææˆåŠŸ: {comparison_report['optimized_success_count']}")
        
        # è®¡ç®—æ”¹è¿›æŒ‡æ ‡
        content_improvements = sum(1 for imp in comparison_report['improvements'] if imp['content_cleaned'])
        username_improvements = sum(1 for imp in comparison_report['improvements'] if imp['username_improved'])
        engagement_complete = sum(1 for imp in comparison_report['improvements'] if imp['engagement_data_complete'])
        
        logger.info(f"  ğŸ§¹ å†…å®¹æ¸…ç†æ”¹è¿›: {content_improvements}/{len(comparison_report['improvements'])}")
        logger.info(f"  ğŸ‘¤ ç”¨æˆ·åæå–æ”¹è¿›: {username_improvements}/{len(comparison_report['improvements'])}")
        logger.info(f"  ğŸ’– å®Œæ•´äº’åŠ¨æ•°æ®: {engagement_complete}/{len(comparison_report['improvements'])}")
        
        # è¯„ä¼°ä¼˜åŒ–æ•ˆæœ
        total_improvements = content_improvements + username_improvements + engagement_complete
        max_possible_improvements = len(comparison_report['improvements']) * 3
        
        if max_possible_improvements > 0:
            improvement_rate = total_improvements / max_possible_improvements
            logger.info(f"  ğŸ“ˆ æ€»ä½“æ”¹è¿›ç‡: {improvement_rate:.1%}")
            
            if improvement_rate >= 0.7:
                logger.info("ğŸ‰ å†…å®¹æå–ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼")
                return True
            elif improvement_rate >= 0.4:
                logger.info("âœ… å†…å®¹æå–ä¼˜åŒ–æœ‰æ•ˆæœ")
                return True
            else:
                logger.warning("âš ï¸ å†…å®¹æå–ä¼˜åŒ–æ•ˆæœæœ‰é™")
                return False
        else:
            logger.warning("âš ï¸ æ— æ³•è¯„ä¼°ä¼˜åŒ–æ•ˆæœ")
            return False
            
    except Exception as e:
        logger.error(f"âŒ å†…å®¹æå–ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False
        
    finally:
        # æ¸…ç†èµ„æº
        if parser:
            try:
                await parser.close()
                logger.info("âœ… TwitterParserå·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­TwitterParseræ—¶å‡ºé”™: {e}")
        
        if launcher:
            try:
                launcher.stop_browser()
                logger.info("âœ… æµè§ˆå™¨å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    success = asyncio.run(test_optimized_content_extraction())
    if success:
        logger.info("\nğŸŠ å†…å®¹æå–ä¼˜åŒ–æµ‹è¯•æˆåŠŸï¼")
    else:
        logger.error("\nğŸ’¥ å†…å®¹æå–ä¼˜åŒ–æµ‹è¯•å¤±è´¥")
        sys.exit(1)