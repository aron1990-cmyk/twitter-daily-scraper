#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤æ¨æ–‡æŠ“å–æ•°é‡ä¸è¶³é—®é¢˜

ä¸»è¦é—®é¢˜åˆ†æï¼š
1. æ¨æ–‡è§£æå¤±è´¥ç‡é«˜ - é€‰æ‹©å™¨ä¸å¤Ÿå…¨é¢ï¼ŒéªŒè¯æ¡ä»¶è¿‡äºä¸¥æ ¼
2. å»é‡é€»è¾‘è¿‡äºä¸¥æ ¼ - åŸºäºå†…å®¹å‰50å­—ç¬¦å»é‡å¯èƒ½è¯¯åˆ ä¸åŒæ¨æ–‡
3. æ»šåŠ¨ç­–ç•¥ä¸å¤Ÿç§¯æ - è¿ç»­3æ¬¡æ— æ–°æ¨æ–‡å°±åœæ­¢ï¼Œä½†å¯èƒ½é¡µé¢è¿˜åœ¨åŠ è½½
4. ç­‰å¾…æ—¶é—´ä¸è¶³ - é¡µé¢åŠ è½½éœ€è¦æ›´å¤šæ—¶é—´
5. æ¨æ–‡å…ƒç´ è¯†åˆ«ä¸å‡†ç¡® - å¯èƒ½åŒ…å«å¹¿å‘Šã€æ¨èç­‰éæ¨æ–‡å…ƒç´ 

è§£å†³æ–¹æ¡ˆï¼š
1. å¢å¼ºæ¨æ–‡è§£æå™¨çš„å®¹é”™æ€§
2. ä¼˜åŒ–å»é‡ç­–ç•¥
3. æ”¹è¿›æ»šåŠ¨ç­–ç•¥
4. å¢åŠ è°ƒè¯•ä¿¡æ¯
5. æ·»åŠ æ¨æ–‡è´¨é‡æ£€æµ‹
"""

import os
import re
import logging
from typing import Dict, List, Any, Optional

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def fix_tweet_scraping_issues():
    """ä¿®å¤æ¨æ–‡æŠ“å–æ•°é‡ä¸è¶³çš„é—®é¢˜"""
    logger.info("å¼€å§‹ä¿®å¤æ¨æ–‡æŠ“å–æ•°é‡ä¸è¶³é—®é¢˜...")
    
    # 1. ä¿®å¤æ¨æ–‡è§£æé€»è¾‘
    fix_tweet_parsing_logic()
    
    # 2. ä¿®å¤å»é‡ç­–ç•¥
    fix_deduplication_strategy()
    
    # 3. ä¿®å¤æ»šåŠ¨ç­–ç•¥
    fix_scrolling_strategy()
    
    # 4. æ·»åŠ æ¨æ–‡è´¨é‡æ£€æµ‹
    add_tweet_quality_detection()
    
    logger.info("æ¨æ–‡æŠ“å–ä¿®å¤å®Œæˆï¼")

def fix_tweet_parsing_logic():
    """ä¿®å¤æ¨æ–‡è§£æé€»è¾‘"""
    logger.info("ä¿®å¤æ¨æ–‡è§£æé€»è¾‘...")
    
    parser_file = '/Users/aron/twitter-daily-scraper/twitter_parser.py'
    
    # è¯»å–åŸæ–‡ä»¶
    with open(parser_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # å¢å¼ºçš„æ¨æ–‡è§£æå‡½æ•°
    enhanced_parse_function = '''
    async def parse_tweet_element_enhanced(self, element) -> Optional[Dict[str, Any]]:
        """å¢å¼ºçš„æ¨æ–‡å…ƒç´ è§£æ - è§£å†³æ•°é‡ä¸è¶³é—®é¢˜"""
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ¨æ–‡å…ƒç´ 
            if not await self.is_valid_tweet_element(element):
                return None
            
            # æå–åŸºç¡€ä¿¡æ¯
            tweet_data = {
                'username': await self.extract_username_enhanced(element),
                'content': await self.extract_content_enhanced(element),
                'link': await self.extract_tweet_link_enhanced(element),
                'publish_time': await self.extract_publish_time_enhanced(element),
                'likes': 0,
                'comments': 0,
                'retweets': 0,
                'media': {'images': [], 'videos': []}
            }
            
            # æå–äº’åŠ¨æ•°æ®
            engagement = await self.extract_engagement_enhanced(element)
            tweet_data.update(engagement)
            
            # æå–åª’ä½“å†…å®¹
            media = await self.extract_media_content_enhanced(element)
            tweet_data['media'] = media
            
            # æ”¹è¿›çš„å»é‡æ£€æŸ¥
            if await self.is_duplicate_tweet_enhanced(tweet_data):
                self.logger.debug(f"æ¨æ–‡é‡å¤ï¼Œè·³è¿‡: {tweet_data.get('link', 'no_link')}")
                return None
            
            # æ”¾å®½éªŒè¯æ¡ä»¶ - åªè¦æœ‰åŸºæœ¬ä¿¡æ¯å°±ä¿ç•™
            if self.is_valid_tweet_data_enhanced(tweet_data):
                return tweet_data
            
            return None
            
        except Exception as e:
            self.logger.debug(f"è§£ææ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return None
    
    async def is_valid_tweet_element(self, element) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ¨æ–‡å…ƒç´ """
        try:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨æ–‡çš„åŸºæœ¬ç»“æ„
            has_user_info = await element.query_selector('[data-testid="User-Name"]') is not None
            has_content_area = await element.query_selector('[data-testid="tweetText"]') is not None
            has_time = await element.query_selector('time') is not None
            has_actions = await element.query_selector('[role="group"]') is not None
            
            # æ’é™¤å¹¿å‘Šå’Œæ¨èå†…å®¹
            element_text = await element.text_content()
            is_ad = any(keyword in element_text.lower() for keyword in ['promoted', 'æ¨å¹¿', 'ad', 'å¹¿å‘Š'])
            
            # è‡³å°‘è¦æœ‰ç”¨æˆ·ä¿¡æ¯æˆ–å†…å®¹åŒºåŸŸï¼Œä¸”ä¸æ˜¯å¹¿å‘Š
            return (has_user_info or has_content_area or has_time or has_actions) and not is_ad
            
        except Exception:
            return True  # å‡ºé”™æ—¶ä¿å®ˆå¤„ç†ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆå…ƒç´ 
    
    async def extract_username_enhanced(self, element) -> str:
        """å¢å¼ºçš„ç”¨æˆ·åæå–"""
        try:
            # æ‰©å±•é€‰æ‹©å™¨åˆ—è¡¨
            selectors = [
                '[data-testid="User-Name"] [dir="ltr"]',
                '[data-testid="User-Name"] span',
                '[data-testid="User-Names"] [dir="ltr"]',
                '[data-testid="User-Names"] span',
                'a[href^="/"][role="link"] span',
                '[dir="ltr"] span',
                'div[dir="ltr"] span',
                'span[dir="ltr"]'
            ]
            
            for selector in selectors:
                try:
                    elements = await element.query_selector_all(selector)
                    for elem in elements:
                        text = await elem.text_content()
                        if text and text.strip():
                            username = self.clean_username(text.strip())
                            if username and username != 'unknown':
                                return username
                except Exception:
                    continue
            
            # ä»é“¾æ¥ä¸­æå–
            try:
                link_elem = await element.query_selector('a[href^="/"][role="link"]')
                if link_elem:
                    href = await link_elem.get_attribute('href')
                    if href:
                        match = re.match(r'^/([^/]+)', href)
                        if match:
                            return match.group(1)
            except Exception:
                pass
            
            return 'unknown'
            
        except Exception:
            return 'unknown'
    
    def clean_username(self, text: str) -> str:
        """æ¸…ç†ç”¨æˆ·å"""
        if not text:
            return 'unknown'
        
        # ç§»é™¤@ç¬¦å·
        text = re.sub(r'^@+', '', text)
        
        # åªä¿ç•™ç¬¬ä¸€ä¸ªå•è¯
        text = text.split()[0] if text.split() else text
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯æ•°å­—å’Œä¸‹åˆ’çº¿
        text = re.sub(r'[^a-zA-Z0-9_]', '', text)
        
        # æ’é™¤æ˜æ˜¾çš„æ•°å­—ï¼ˆå¦‚ç‚¹èµæ•°ç­‰ï¼‰
        if re.match(r'^\d+[KMB]?$', text):
            return 'unknown'
        
        return text if text else 'unknown'
    
    async def extract_content_enhanced(self, element) -> str:
        """å¢å¼ºçš„å†…å®¹æå–"""
        try:
            content_parts = []
            
            # æ‰©å±•å†…å®¹é€‰æ‹©å™¨
            selectors = [
                '[data-testid="tweetText"]',
                '[data-testid="tweetText"] span',
                '[lang] span',
                'div[dir="auto"] span',
                'div[dir="ltr"] span',
                'div[dir="rtl"] span',
                'span[dir="auto"]',
                'span[dir="ltr"]',
                'span[dir="rtl"]',
                'div[lang] span'
            ]
            
            for selector in selectors:
                try:
                    elements = await element.query_selector_all(selector)
                    for elem in elements:
                        text = await elem.text_content()
                        if text and text.strip():
                            clean_text = text.strip()
                            if clean_text not in content_parts and len(clean_text) > 2:
                                content_parts.append(clean_text)
                except Exception:
                    continue
            
            if content_parts:
                content = ' '.join(content_parts)
                return self.clean_tweet_content_enhanced(content)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•ä»æ•´ä¸ªå…ƒç´ æå–
            try:
                full_text = await element.text_content()
                if full_text:
                    lines = [line.strip() for line in full_text.split('\n') if line.strip()]
                    # è¿‡æ»¤æ‰ç”¨æˆ·åã€æ—¶é—´ç­‰ä¿¡æ¯ï¼Œä¿ç•™ä¸»è¦å†…å®¹
                    content_lines = []
                    for line in lines:
                        if (len(line) > 10 and 
                            not line.startswith('@') and 
                            not re.match(r'^\d+[hms]$', line) and
                            not re.match(r'^\d+[KMB]?$', line)):
                            content_lines.append(line)
                    
                    if content_lines:
                        return ' '.join(content_lines[:3])  # å–å‰3è¡Œ
            except Exception:
                pass
            
            return 'No content available'
            
        except Exception:
            return 'No content available'
    
    def clean_tweet_content_enhanced(self, content: str) -> str:
        """å¢å¼ºçš„å†…å®¹æ¸…ç†"""
        if not content:
            return ""
        
        # åŸºç¡€æ¸…ç†
        content = re.sub(r'\s+', ' ', content.strip())
        
        # ç§»é™¤æ˜æ˜¾çš„é‡å¤æ¨¡å¼
        content = re.sub(r'\b(\w+)\s+\1\b', r'\1', content)
        
        # ç§»é™¤æœ«å°¾çš„ç»Ÿè®¡ä¿¡æ¯
        content = re.sub(r'\s*[Â·â€¦]+\s*\d+[KMB]?\s*$', '', content)
        
        # ç§»é™¤å¼€å¤´çš„é‡å¤ç”¨æˆ·å
        content = re.sub(r'^(@?\w+)\s+\1\s+', r'\1 ', content)
        
        return content.strip()
    
    async def extract_tweet_link_enhanced(self, element) -> str:
        """å¢å¼ºçš„é“¾æ¥æå–"""
        try:
            # å¤šç§é“¾æ¥é€‰æ‹©å™¨
            selectors = [
                'a[href*="/status/"]',
                'a[href*="/status/"][role="link"]',
                'time[datetime] a',
                'time a'
            ]
            
            for selector in selectors:
                try:
                    link_elem = await element.query_selector(selector)
                    if link_elem:
                        href = await link_elem.get_attribute('href')
                        if href and '/status/' in href:
                            if href.startswith('/'):
                                return f'https://x.com{href}'
                            return href
                except Exception:
                    continue
            
            return ''
            
        except Exception:
            return ''
    
    async def is_duplicate_tweet_enhanced(self, tweet_data: Dict[str, Any]) -> bool:
        """å¢å¼ºçš„å»é‡æ£€æŸ¥"""
        try:
            # ä¼˜å…ˆä½¿ç”¨é“¾æ¥å»é‡
            link = tweet_data.get('link', '')
            if link:
                tweet_id = self.extract_tweet_id(link)
                if tweet_id:
                    if not hasattr(self, 'seen_tweet_ids_enhanced'):
                        self.seen_tweet_ids_enhanced = set()
                    
                    if tweet_id in self.seen_tweet_ids_enhanced:
                        return True
                    self.seen_tweet_ids_enhanced.add(tweet_id)
                    return False
            
            # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œä½¿ç”¨å†…å®¹å»é‡ï¼ˆæ›´å®½æ¾çš„ç­–ç•¥ï¼‰
            content = tweet_data.get('content', '')
            if content and len(content) > 20:
                # ä½¿ç”¨å†…å®¹çš„å“ˆå¸Œå€¼è€Œä¸æ˜¯å‰50å­—ç¬¦
                import hashlib
                content_hash = hashlib.md5(content.encode()).hexdigest()
                
                if not hasattr(self, 'seen_content_hashes'):
                    self.seen_content_hashes = set()
                
                if content_hash in self.seen_content_hashes:
                    return True
                self.seen_content_hashes.add(content_hash)
            
            return False
            
        except Exception:
            return False
    
    def is_valid_tweet_data_enhanced(self, tweet_data: Dict[str, Any]) -> bool:
        """å¢å¼ºçš„æ¨æ–‡æ•°æ®éªŒè¯ - æ›´å®½æ¾çš„æ¡ä»¶"""
        try:
            username = tweet_data.get('username', '')
            content = tweet_data.get('content', '')
            link = tweet_data.get('link', '')
            
            # åªè¦æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶å°±è®¤ä¸ºæœ‰æ•ˆï¼š
            # 1. æœ‰ç”¨æˆ·åä¸”ä¸æ˜¯unknown
            # 2. æœ‰å†…å®¹ä¸”é•¿åº¦å¤§äº5
            # 3. æœ‰æœ‰æ•ˆé“¾æ¥
            # 4. æœ‰åª’ä½“å†…å®¹
            # 5. æœ‰ä»»ä½•äº’åŠ¨æ•°æ®
            
            has_username = username and username != 'unknown'
            has_content = content and content != 'No content available' and len(content.strip()) > 5
            has_link = link and '/status/' in link
            has_media = (tweet_data.get('media', {}).get('images') or 
                        tweet_data.get('media', {}).get('videos'))
            has_engagement = (tweet_data.get('likes', 0) > 0 or 
                            tweet_data.get('comments', 0) > 0 or 
                            tweet_data.get('retweets', 0) > 0)
            
            return has_username or has_content or has_link or has_media or has_engagement
            
        except Exception:
            return False
'''
    
    # åœ¨æ–‡ä»¶ä¸­æ·»åŠ å¢å¼ºçš„è§£æå‡½æ•°
    if 'parse_tweet_element_enhanced' not in content:
        # æ‰¾åˆ°åˆé€‚çš„ä½ç½®æ’å…¥æ–°å‡½æ•°
        insert_pos = content.find('async def parse_tweet_element_optimized')
        if insert_pos != -1:
            content = content[:insert_pos] + enhanced_parse_function + '\n\n    ' + content[insert_pos:]
        else:
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œæ·»åŠ åˆ°ç±»çš„æœ«å°¾
            class_end = content.rfind('class TwitterParser')
            if class_end != -1:
                next_class = content.find('\nclass ', class_end + 1)
                if next_class != -1:
                    content = content[:next_class] + enhanced_parse_function + '\n' + content[next_class:]
                else:
                    content += enhanced_parse_function
    
    # å†™å›æ–‡ä»¶
    with open(parser_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    logger.info("æ¨æ–‡è§£æé€»è¾‘ä¿®å¤å®Œæˆ")

def fix_deduplication_strategy():
    """ä¿®å¤å»é‡ç­–ç•¥"""
    logger.info("ä¿®å¤å»é‡ç­–ç•¥...")
    
    parser_file = '/Users/aron/twitter-daily-scraper/twitter_parser.py'
    
    with open(parser_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä¿®å¤åŸæœ‰çš„å»é‡é€»è¾‘
    old_dedup_pattern = r'tweet_id = tweet_data\.get\(\'link\', \'\'\'\) or tweet_data\.get\(\'content\', \'\'\'\)\[:50\]'
    new_dedup_logic = '''# æ”¹è¿›çš„å»é‡é€»è¾‘
            link = tweet_data.get('link', '')
            if link:
                tweet_id = self.extract_tweet_id(link)
                if tweet_id:
                    if tweet_id not in self.seen_tweet_ids:
                        self.seen_tweet_ids.add(tweet_id)
                        tweets_data.append(tweet_data)
                        new_tweets_parsed += 1
                        self.logger.debug(f"æ–°æŠ“å–æ¨æ–‡: @{tweet_data.get('username', 'unknown')}")
                    else:
                        self.logger.debug(f"æ¨æ–‡é‡å¤(é“¾æ¥): {tweet_id}")
                else:
                    # æ²¡æœ‰æœ‰æ•ˆIDï¼Œä½¿ç”¨å†…å®¹å“ˆå¸Œ
                    content = tweet_data.get('content', '')
                    if content and len(content) > 10:
                        import hashlib
                        content_hash = hashlib.md5(content.encode()).hexdigest()
                        if content_hash not in getattr(self, 'seen_content_hashes', set()):
                            if not hasattr(self, 'seen_content_hashes'):
                                self.seen_content_hashes = set()
                            self.seen_content_hashes.add(content_hash)
                            tweets_data.append(tweet_data)
                            new_tweets_parsed += 1
                            self.logger.debug(f"æ–°æŠ“å–æ¨æ–‡(å†…å®¹): @{tweet_data.get('username', 'unknown')}")
                        else:
                            self.logger.debug(f"æ¨æ–‡é‡å¤(å†…å®¹): {content[:30]}...")
            else:
                # æ²¡æœ‰é“¾æ¥ï¼Œç›´æ¥æ·»åŠ ï¼ˆé£é™©è¾ƒä½çš„é‡å¤ï¼‰
                tweets_data.append(tweet_data)
                new_tweets_parsed += 1
                self.logger.debug(f"æ–°æŠ“å–æ¨æ–‡(æ— é“¾æ¥): @{tweet_data.get('username', 'unknown')}")'''
    
    # æ›¿æ¢å»é‡é€»è¾‘
    if 'tweet_id not in self.seen_tweet_ids:' in content:
        # æ‰¾åˆ°å¹¶æ›¿æ¢æ•´ä¸ªå»é‡ä»£ç å—
        pattern = r'tweet_id = tweet_data\.get\(\'link\', \'\'\'\) or tweet_data\.get\(\'content\', \'\'\'\)\[:50\]\s*if tweet_id not in self\.seen_tweet_ids:[^}]+?self\.logger\.debug\(f"æ–°æŠ“å–æ¨æ–‡: @\{tweet_data\.get\(\'username\', \'unknown\'\)\}"\)'
        
        import re
        content = re.sub(pattern, new_dedup_logic, content, flags=re.DOTALL)
    
    with open(parser_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    logger.info("å»é‡ç­–ç•¥ä¿®å¤å®Œæˆ")

def fix_scrolling_strategy():
    """ä¿®å¤æ»šåŠ¨ç­–ç•¥"""
    logger.info("ä¿®å¤æ»šåŠ¨ç­–ç•¥...")
    
    parser_file = '/Users/aron/twitter-daily-scraper/twitter_parser.py'
    
    with open(parser_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä¿®æ”¹æ»šåŠ¨å‚æ•°
    # 1. å¢åŠ æœ€å¤§æ»šåŠ¨æ¬¡æ•°
    content = re.sub(r'max_scroll_attempts = 20', 'max_scroll_attempts = 30', content)
    
    # 2. å‡å°‘è¿ç»­æ— æ–°æ¨æ–‡çš„é˜ˆå€¼
    content = re.sub(r'if no_new_tweets_count >= 3:', 'if no_new_tweets_count >= 5:', content)
    
    # 3. å¢åŠ ç­‰å¾…æ—¶é—´
    content = re.sub(r'await asyncio\.sleep\(1\)', 'await asyncio.sleep(2)', content)
    
    # 4. å¢åŠ æ»šåŠ¨è·ç¦»
    content = re.sub(r'window\.scrollBy\(0, 800\)', 'window.scrollBy(0, 1200)', content)
    
    with open(parser_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    logger.info("æ»šåŠ¨ç­–ç•¥ä¿®å¤å®Œæˆ")

def add_tweet_quality_detection():
    """æ·»åŠ æ¨æ–‡è´¨é‡æ£€æµ‹"""
    logger.info("æ·»åŠ æ¨æ–‡è´¨é‡æ£€æµ‹...")
    
    parser_file = '/Users/aron/twitter-daily-scraper/twitter_parser.py'
    
    with open(parser_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ·»åŠ æ¨æ–‡è´¨é‡æ£€æµ‹å‡½æ•°
    quality_detection_code = '''
    def detect_tweet_quality(self, tweet_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æµ‹æ¨æ–‡è´¨é‡å¹¶æ·»åŠ è´¨é‡æ ‡è®°"""
        try:
            quality_score = 0
            quality_issues = []
            
            # æ£€æŸ¥ç”¨æˆ·å
            username = tweet_data.get('username', '')
            if username and username != 'unknown':
                quality_score += 20
            else:
                quality_issues.append('ç¼ºå°‘ç”¨æˆ·å')
            
            # æ£€æŸ¥å†…å®¹
            content = tweet_data.get('content', '')
            if content and content != 'No content available':
                if len(content) > 10:
                    quality_score += 30
                elif len(content) > 5:
                    quality_score += 15
                    quality_issues.append('å†…å®¹è¿‡çŸ­')
                else:
                    quality_issues.append('å†…å®¹å¤ªçŸ­')
            else:
                quality_issues.append('ç¼ºå°‘å†…å®¹')
            
            # æ£€æŸ¥é“¾æ¥
            link = tweet_data.get('link', '')
            if link and '/status/' in link:
                quality_score += 25
            else:
                quality_issues.append('ç¼ºå°‘æœ‰æ•ˆé“¾æ¥')
            
            # æ£€æŸ¥æ—¶é—´
            if tweet_data.get('publish_time'):
                quality_score += 10
            else:
                quality_issues.append('ç¼ºå°‘å‘å¸ƒæ—¶é—´')
            
            # æ£€æŸ¥äº’åŠ¨æ•°æ®
            has_engagement = (tweet_data.get('likes', 0) > 0 or 
                            tweet_data.get('comments', 0) > 0 or 
                            tweet_data.get('retweets', 0) > 0)
            if has_engagement:
                quality_score += 15
            else:
                quality_issues.append('ç¼ºå°‘äº’åŠ¨æ•°æ®')
            
            # æ·»åŠ è´¨é‡ä¿¡æ¯
            tweet_data['quality_score'] = quality_score
            tweet_data['quality_issues'] = quality_issues
            tweet_data['quality_level'] = (
                'high' if quality_score >= 80 else
                'medium' if quality_score >= 50 else
                'low'
            )
            
            return tweet_data
            
        except Exception as e:
            self.logger.debug(f"è´¨é‡æ£€æµ‹å¤±è´¥: {e}")
            tweet_data['quality_score'] = 0
            tweet_data['quality_issues'] = ['è´¨é‡æ£€æµ‹å¤±è´¥']
            tweet_data['quality_level'] = 'unknown'
            return tweet_data
'''
    
    if 'detect_tweet_quality' not in content:
        # åœ¨ç±»çš„æœ«å°¾æ·»åŠ è´¨é‡æ£€æµ‹å‡½æ•°
        insert_pos = content.rfind('    def ')
        if insert_pos != -1:
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå‡½æ•°çš„ç»“æŸä½ç½®
            next_def = content.find('\n    def ', insert_pos + 1)
            if next_def == -1:
                next_def = content.find('\nclass ', insert_pos)
            if next_def == -1:
                next_def = len(content)
            
            content = content[:next_def] + quality_detection_code + '\n' + content[next_def:]
        else:
            content += quality_detection_code
    
    with open(parser_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    logger.info("æ¨æ–‡è´¨é‡æ£€æµ‹æ·»åŠ å®Œæˆ")

def create_enhanced_scraping_test():
    """åˆ›å»ºå¢å¼ºæŠ“å–æµ‹è¯•è„šæœ¬"""
    logger.info("åˆ›å»ºå¢å¼ºæŠ“å–æµ‹è¯•è„šæœ¬...")
    
    test_script = '''
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¿®å¤åçš„æ¨æ–‡æŠ“å–åŠŸèƒ½
"""

import asyncio
import logging
from twitter_parser import TwitterParser

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_enhanced_scraping():
    """æµ‹è¯•å¢å¼ºçš„æ¨æ–‡æŠ“å–"""
    parser = None
    try:
        logger.info("å¼€å§‹æµ‹è¯•å¢å¼ºæ¨æ–‡æŠ“å–...")
        
        # åˆ›å»ºè§£æå™¨
        parser = TwitterParser()
        await parser.init_browser()
        
        # æµ‹è¯•ç”¨æˆ·æ¨æ–‡æŠ“å–
        test_username = "socialmedia2day"
        target_tweets = 50
        
        logger.info(f"æµ‹è¯•æŠ“å–ç”¨æˆ· @{test_username} çš„ {target_tweets} æ¡æ¨æ–‡")
        
        tweets = await parser.scrape_user_tweets(test_username, target_tweets)
        
        logger.info(f"æŠ“å–ç»“æœ: ç›®æ ‡ {target_tweets} æ¡ï¼Œå®é™…è·å¾— {len(tweets)} æ¡")
        
        if len(tweets) < target_tweets:
            shortage = target_tweets - len(tweets)
            logger.warning(f"ä»ç„¶å­˜åœ¨æ•°é‡ä¸è¶³é—®é¢˜ï¼Œç¼ºå°‘ {shortage} æ¡æ¨æ–‡")
        else:
            logger.info("æŠ“å–æ•°é‡è¾¾åˆ°ç›®æ ‡ï¼")
        
        # åˆ†ææ¨æ–‡è´¨é‡
        if hasattr(parser, 'detect_tweet_quality'):
            quality_stats = {'high': 0, 'medium': 0, 'low': 0, 'unknown': 0}
            for tweet in tweets:
                tweet = parser.detect_tweet_quality(tweet)
                quality_level = tweet.get('quality_level', 'unknown')
                quality_stats[quality_level] += 1
            
            logger.info(f"æ¨æ–‡è´¨é‡åˆ†å¸ƒ: {quality_stats}")
        
        # æ˜¾ç¤ºå‰5æ¡æ¨æ–‡çš„è¯¦ç»†ä¿¡æ¯
        logger.info("å‰5æ¡æ¨æ–‡è¯¦æƒ…:")
        for i, tweet in enumerate(tweets[:5], 1):
            logger.info(f"æ¨æ–‡ {i}:")
            logger.info(f"  ç”¨æˆ·: @{tweet.get('username', 'unknown')}")
            logger.info(f"  å†…å®¹: {tweet.get('content', 'No content')[:100]}...")
            logger.info(f"  é“¾æ¥: {tweet.get('link', 'No link')}")
            logger.info(f"  äº’åŠ¨: ğŸ‘{tweet.get('likes', 0)} ğŸ’¬{tweet.get('comments', 0)} ğŸ”„{tweet.get('retweets', 0)}")
            if 'quality_score' in tweet:
                logger.info(f"  è´¨é‡: {tweet['quality_level']} ({tweet['quality_score']}/100)")
            logger.info("")
        
        return len(tweets)
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        return 0
    finally:
        if parser:
            await parser.close()

if __name__ == "__main__":
    result = asyncio.run(test_enhanced_scraping())
    print(f"\næµ‹è¯•å®Œæˆï¼Œå…±æŠ“å– {result} æ¡æ¨æ–‡")
'''
    
    with open('/Users/aron/twitter-daily-scraper/test_enhanced_scraping.py', 'w', encoding='utf-8') as f:
        f.write(test_script)
    
    logger.info("æµ‹è¯•è„šæœ¬åˆ›å»ºå®Œæˆ")

if __name__ == "__main__":
    fix_tweet_scraping_issues()
    create_enhanced_scraping_test()
    
    print("\n=== ä¿®å¤å®Œæˆ ===")
    print("ä¸»è¦ä¿®å¤å†…å®¹:")
    print("1. å¢å¼ºæ¨æ–‡è§£æå™¨çš„å®¹é”™æ€§å’Œé€‰æ‹©å™¨è¦†ç›–")
    print("2. ä¼˜åŒ–å»é‡ç­–ç•¥ï¼Œå‡å°‘è¯¯åˆ ")
    print("3. æ”¹è¿›æ»šåŠ¨ç­–ç•¥ï¼Œå¢åŠ æŠ“å–æœºä¼š")
    print("4. æ·»åŠ æ¨æ–‡è´¨é‡æ£€æµ‹")
    print("5. å¢åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—")
    print("\nå»ºè®®è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯ä¿®å¤æ•ˆæœ:")
    print("python3 test_enhanced_scraping.py")