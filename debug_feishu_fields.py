#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sqlite3
from enhanced_tweet_scraper import EnhancedTwitterScraper, load_feishu_config_from_file
from cloud_sync import CloudSyncManager

def analyze_local_data_only():
    """ä»…åˆ†æžæœ¬åœ°æ•°æ®çš„å­—æ®µæ˜ å°„"""
    print("\nðŸ“Š åˆ†æžæœ¬åœ°æŽ¨æ–‡æ•°æ®...")
    
    # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æŽ¨æ–‡æ ·ä¾‹
    conn = sqlite3.connect('twitter_scraper.db')
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT username, content, publish_time, likes, comments, retweets, link, hashtags, scraped_at
        FROM tweet_data 
        ORDER BY id DESC 
        LIMIT 5
    """)
    
    tweets = cursor.fetchall()
    
    # ç»Ÿè®¡æ—¶é—´å­—æ®µæƒ…å†µ
    cursor.execute("""
        SELECT 
            COUNT(*) as total,
            COUNT(CASE WHEN publish_time IS NOT NULL AND publish_time != '' THEN 1 END) as has_publish_time,
            COUNT(CASE WHEN scraped_at IS NOT NULL THEN 1 END) as has_scraped_at
        FROM tweet_data
    """)
    
    stats = cursor.fetchone()
    conn.close()
    
    total, has_publish_time, has_scraped_at = stats
    
    print(f"\nðŸ“ˆ æ•°æ®åº“ç»Ÿè®¡:")
    print(f"  æ€»æŽ¨æ–‡æ•°: {total}")
    print(f"  æœ‰å‘å¸ƒæ—¶é—´çš„æŽ¨æ–‡: {has_publish_time} ({has_publish_time/total*100:.1f}%)")
    print(f"  æœ‰æŠ“å–æ—¶é—´çš„æŽ¨æ–‡: {has_scraped_at} ({has_scraped_at/total*100:.1f}%)")
    
    print("\nðŸ“ æŽ¨æ–‡æ ·ä¾‹:")
    for i, (username, content, publish_time, likes, comments, retweets, link, hashtags, scraped_at) in enumerate(tweets, 1):
        print(f"  {i}. ç”¨æˆ·: {username}")
        print(f"     å†…å®¹: {content[:50]}...")
        print(f"     å‘å¸ƒæ—¶é—´: {publish_time or 'âŒ æ— '}")
        print(f"     æŠ“å–æ—¶é—´: {scraped_at or 'âŒ æ— '}")
        print(f"     äº’åŠ¨: ðŸ‘{likes} ðŸ’¬{comments} ðŸ”„{retweets}")
        print()
    
    # æ¨¡æ‹Ÿæ ¼å¼åŒ–æ•°æ®
    print("ðŸ“ æ¨¡æ‹Ÿé£žä¹¦æ•°æ®æ ¼å¼åŒ–...")
    scraper = EnhancedTwitterScraper()
    
    # æž„å»ºæµ‹è¯•æ•°æ®
    test_tweets = []
    for username, content, publish_time, likes, comments, retweets, link, hashtags, scraped_at in tweets:
        tweet_dict = {
            'username': username,
            'content': content,
            'publish_time': publish_time,
            'likes': likes,
            'comments': comments,
            'retweets': retweets,
            'link': link,
            'hashtags': hashtags.split(',') if hashtags else [],
            'scraped_at': scraped_at
        }
        test_tweets.append(tweet_dict)
    
    formatted_tweets = scraper.format_tweets_for_feishu(test_tweets)
    
    print("\nðŸŽ¯ æ ¼å¼åŒ–åŽçš„å­—æ®µåˆ†æž:")
    if formatted_tweets:
        sample_tweet = formatted_tweets[0]
        print("é£žä¹¦åŒæ­¥å­—æ®µæ˜ å°„:")
        for key, value in sample_tweet.items():
            status = "âœ… æœ‰å€¼" if value else "âŒ ç©ºå€¼"
            print(f"  - {key}: {status}")
            if not value and key in ['å‘å¸ƒæ—¶é—´', 'åˆ›å»ºæ—¶é—´']:
                print(f"    âš ï¸ æ—¶é—´å­—æ®µä¸ºç©ºå¯èƒ½å¯¼è‡´é£žä¹¦æ˜¾ç¤ºå¼‚å¸¸")
    
    print("\nðŸ” é—®é¢˜è¯Šæ–­:")
    empty_publish_time = total - has_publish_time
    if empty_publish_time > 0:
        print(f"  âŒ {empty_publish_time} æ¡æŽ¨æ–‡ç¼ºå°‘å‘å¸ƒæ—¶é—´")
        print(f"     è¿™å¯èƒ½å¯¼è‡´é£žä¹¦ä¸­è¿™äº›æŽ¨æ–‡çš„æ—¶é—´å­—æ®µæ˜¾ç¤ºå¼‚å¸¸")
    
    if has_publish_time > 0:
        print(f"  âœ… {has_publish_time} æ¡æŽ¨æ–‡æœ‰å‘å¸ƒæ—¶é—´")
        print(f"     è¿™äº›æŽ¨æ–‡åœ¨é£žä¹¦ä¸­åº”è¯¥æ˜¾ç¤ºæ­£å¸¸")
    
    print("\nðŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®:")
    if empty_publish_time > 0:
        print("  1. ä¿®å¤æŠ“å–å™¨ä»¥æ­£ç¡®èŽ·å–æŽ¨æ–‡å‘å¸ƒæ—¶é—´")
        print("  2. å¯¹äºŽå·²æœ‰çš„æ— æ—¶é—´æŽ¨æ–‡ï¼Œå¯ä»¥ä½¿ç”¨æŠ“å–æ—¶é—´ä½œä¸ºæ›¿ä»£")
        print("  3. é‡æ–°æŠ“å–è¿™äº›æŽ¨æ–‡ä»¥èŽ·å–å®Œæ•´çš„æ—¶é—´ä¿¡æ¯")

def debug_feishu_field_mapping():
    """è°ƒè¯•é£žä¹¦å­—æ®µæ˜ å°„é—®é¢˜"""
    print("ðŸ” è°ƒè¯•é£žä¹¦å­—æ®µæ˜ å°„é—®é¢˜...")
    
    # åŠ è½½é£žä¹¦é…ç½®
    feishu_config = load_feishu_config_from_file()
    if not feishu_config:
        print("âŒ æ— æ³•åŠ è½½é£žä¹¦é…ç½®")
        return
    
    if not feishu_config.get('enabled'):
        print("âš ï¸ é£žä¹¦é…ç½®æœªå¯ç”¨ï¼Œä½†ç»§ç»­åˆ†æžå­—æ®µæ˜ å°„...")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå ä½ç¬¦é…ç½®
    is_placeholder = (
        feishu_config.get('app_id') == 'your_feishu_app_id' or
        feishu_config.get('spreadsheet_token') == 'your_spreadsheet_token'
    )
    
    if is_placeholder:
        print("âš ï¸ æ£€æµ‹åˆ°å ä½ç¬¦é…ç½®ï¼Œè·³è¿‡é£žä¹¦APIè°ƒç”¨ï¼Œä»…åˆ†æžæœ¬åœ°æ•°æ®...")
        analyze_local_data_only()
        return
    
    # åˆ›å»ºåŒæ­¥ç®¡ç†å™¨
    sync_manager = CloudSyncManager()
    sync_manager.setup_feishu(
        feishu_config.get('app_id'),
        feishu_config.get('app_secret')
    )
    
    # èŽ·å–é£žä¹¦è¡¨æ ¼å­—æ®µä¿¡æ¯
    print("\nðŸ“‹ èŽ·å–é£žä¹¦è¡¨æ ¼å­—æ®µä¿¡æ¯...")
    fields_info = sync_manager._get_feishu_table_fields(
        feishu_config.get('spreadsheet_token'),
        feishu_config.get('table_id')
    )
    
    if fields_info:
        print(f"âœ… é£žä¹¦è¡¨æ ¼å­—æ®µèŽ·å–æˆåŠŸï¼Œå…± {len(fields_info)} ä¸ªå­—æ®µ:")
        for i, field in enumerate(fields_info, 1):
            field_name = field.get('field_name', 'æœªçŸ¥')
            field_type = field.get('type', 'æœªçŸ¥')
            print(f"  {i}. {field_name} ({field_type})")
    else:
        print("âŒ æ— æ³•èŽ·å–é£žä¹¦è¡¨æ ¼å­—æ®µ")
        return
    
    # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æŽ¨æ–‡æ ·ä¾‹
    print("\nðŸ“Š æ£€æŸ¥æ•°æ®åº“ä¸­çš„æŽ¨æ–‡æ ·ä¾‹...")
    conn = sqlite3.connect('twitter_scraper.db')
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT username, content, publish_time, likes, comments, retweets, link, hashtags
        FROM tweet_data 
        ORDER BY id DESC 
        LIMIT 3
    """)
    
    tweets = cursor.fetchall()
    conn.close()
    
    print("æ•°æ®åº“ä¸­çš„æŽ¨æ–‡æ ·ä¾‹:")
    for i, (username, content, publish_time, likes, comments, retweets, link, hashtags) in enumerate(tweets, 1):
        print(f"  {i}. ç”¨æˆ·: {username}")
        print(f"     å†…å®¹: {content[:50]}...")
        print(f"     å‘å¸ƒæ—¶é—´: {publish_time or 'æ— '}")
        print(f"     äº’åŠ¨: ðŸ‘{likes} ðŸ’¬{comments} ðŸ”„{retweets}")
        print(f"     é“¾æŽ¥: {link[:50]}..." if link else "     é“¾æŽ¥: æ— ")
        print(f"     æ ‡ç­¾: {hashtags}")
        print()
    
    # æ¨¡æ‹Ÿæ ¼å¼åŒ–æ•°æ®
    print("ðŸ“ æ¨¡æ‹Ÿæ ¼å¼åŒ–æ•°æ®ç”¨äºŽé£žä¹¦åŒæ­¥...")
    scraper = EnhancedTwitterScraper()
    
    # æž„å»ºæµ‹è¯•æ•°æ®
    test_tweets = []
    for username, content, publish_time, likes, comments, retweets, link, hashtags in tweets:
        tweet_dict = {
            'username': username,
            'content': content,
            'publish_time': publish_time,
            'likes': likes,
            'comments': comments,
            'retweets': retweets,
            'link': link,
            'hashtags': hashtags.split(',') if hashtags else []
        }
        test_tweets.append(tweet_dict)
    
    formatted_tweets = scraper.format_tweets_for_feishu(test_tweets)
    
    print("æ ¼å¼åŒ–åŽçš„é£žä¹¦æ•°æ®æ ·ä¾‹:")
    for i, tweet in enumerate(formatted_tweets, 1):
        print(f"  {i}. æ ¼å¼åŒ–æ•°æ®:")
        for key, value in tweet.items():
            print(f"     {key}: {value}")
        print()
    
    # æ£€æŸ¥å­—æ®µåŒ¹é…æƒ…å†µ
    print("ðŸ” æ£€æŸ¥å­—æ®µåŒ¹é…æƒ…å†µ...")
    feishu_field_names = [field.get('field_name') for field in fields_info]
    formatted_field_names = list(formatted_tweets[0].keys()) if formatted_tweets else []
    
    print("é£žä¹¦è¡¨æ ¼å­—æ®µ:")
    for field in feishu_field_names:
        print(f"  - {field}")
    
    print("\næ ¼å¼åŒ–æ•°æ®å­—æ®µ:")
    for field in formatted_field_names:
        print(f"  - {field}")
    
    print("\nå­—æ®µåŒ¹é…åˆ†æž:")
    matched_fields = []
    unmatched_feishu_fields = []
    unmatched_data_fields = []
    
    for field in formatted_field_names:
        if field in feishu_field_names:
            matched_fields.append(field)
        else:
            unmatched_data_fields.append(field)
    
    for field in feishu_field_names:
        if field not in formatted_field_names:
            unmatched_feishu_fields.append(field)
    
    print(f"âœ… åŒ¹é…çš„å­—æ®µ ({len(matched_fields)}):")
    for field in matched_fields:
        print(f"  - {field}")
    
    print(f"\nâš ï¸ é£žä¹¦è¡¨æ ¼ä¸­å­˜åœ¨ä½†æ•°æ®ä¸­æ²¡æœ‰çš„å­—æ®µ ({len(unmatched_feishu_fields)}):")
    for field in unmatched_feishu_fields:
        print(f"  - {field}")
    
    print(f"\nâŒ æ•°æ®ä¸­å­˜åœ¨ä½†é£žä¹¦è¡¨æ ¼ä¸­æ²¡æœ‰çš„å­—æ®µ ({len(unmatched_data_fields)}):")
    for field in unmatched_data_fields:
        print(f"  - {field}")
    
    # åˆ†æžé—®é¢˜
    print("\nðŸŽ¯ é—®é¢˜åˆ†æž:")
    if 'å‘å¸ƒæ—¶é—´' in unmatched_feishu_fields:
        print("  - é£žä¹¦è¡¨æ ¼æœ‰'å‘å¸ƒæ—¶é—´'å­—æ®µä½†æ•°æ®ä¸­æ²¡æœ‰ä½¿ç”¨")
    if 'åˆ›å»ºæ—¶é—´' in unmatched_feishu_fields:
        print("  - é£žä¹¦è¡¨æ ¼æœ‰'åˆ›å»ºæ—¶é—´'å­—æ®µä½†æ•°æ®ä¸­æ²¡æœ‰ä½¿ç”¨")
    
    time_related_issues = []
    for tweet in formatted_tweets:
        if not tweet.get('å‘å¸ƒæ—¶é—´'):
            time_related_issues.append(tweet.get('ä½œè€…ï¼ˆè´¦å·ï¼‰', 'æœªçŸ¥ç”¨æˆ·'))
    
    if time_related_issues:
        print(f"  - {len(time_related_issues)} æ¡æŽ¨æ–‡ç¼ºå°‘å‘å¸ƒæ—¶é—´æ•°æ®")
        print(f"    æ¶‰åŠç”¨æˆ·: {', '.join(time_related_issues[:5])}{'...' if len(time_related_issues) > 5 else ''}")

if __name__ == '__main__':
    debug_feishu_field_mapping()