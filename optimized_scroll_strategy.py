#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„æ»šåŠ¨ç­–ç•¥
è§£å†³æ¨æ–‡åŠ è½½æ•ˆç‡ä½å’Œé‡å¤å†…å®¹é—®é¢˜
"""

import asyncio
import logging
import sys
import os
import json
from datetime import datetime
from typing import Dict, List, Set, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ads_browser_launcher import AdsPowerLauncher
from twitter_parser import TwitterParser
from data_extractor import DataExtractor
from tweet_filter import TweetFilter

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# AdsPoweré…ç½®
ADS_POWER_CONFIG = {
    'user_id': 'k11p9ypc',
    'open_tabs': 1,
    'launch_args': [],
    'headless': False,
    'disable_password_filling': False,
    'clear_cache_after_closing': False,
    'enable_password_saving': False
}

class OptimizedScrollStrategy:
    """ä¼˜åŒ–çš„æ»šåŠ¨ç­–ç•¥ç±»"""
    
    def __init__(self, parser: TwitterParser):
        self.parser = parser
        self.logger = logging.getLogger(__name__)
        self.seen_tweet_ids: Set[str] = set()
        self.scroll_metrics = {
            'total_scrolls': 0,
            'successful_loads': 0,
            'duplicate_detections': 0,
            'efficiency_score': 0.0
        }
    
    def extract_tweet_id(self, tweet_link: str) -> str:
        """ä»æ¨æ–‡é“¾æ¥ä¸­æå–ID"""
        try:
            if '/status/' in tweet_link:
                return tweet_link.split('/status/')[-1].split('?')[0]
            return ''
        except Exception:
            return ''
    
    async def get_current_tweet_elements(self):
        """è·å–å½“å‰é¡µé¢çš„æ¨æ–‡å…ƒç´ """
        try:
            # ç­‰å¾…æ¨æ–‡å…ƒç´ åŠ è½½
            await self.parser.page.wait_for_selector('[data-testid="tweet"]', timeout=5000)
            return await self.parser.page.query_selector_all('[data-testid="tweet"]')
        except Exception as e:
            self.logger.warning(f"è·å–æ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return []
    
    async def check_for_new_tweets(self, previous_count: int) -> Dict[str, Any]:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡åŠ è½½"""
        current_elements = await self.get_current_tweet_elements()
        current_count = len(current_elements)
        
        new_tweets_count = max(0, current_count - previous_count)
        
        # æ£€æŸ¥é‡å¤æ¨æ–‡
        duplicate_count = 0
        if current_elements:
            for element in current_elements[-new_tweets_count:] if new_tweets_count > 0 else current_elements:
                try:
                    # å°è¯•è·å–æ¨æ–‡é“¾æ¥æ¥æ£€æµ‹é‡å¤
                    link_element = await element.query_selector('a[href*="/status/"]')
                    if link_element:
                        href = await link_element.get_attribute('href')
                        if href:
                            tweet_id = self.extract_tweet_id(href)
                            if tweet_id:
                                if tweet_id in self.seen_tweet_ids:
                                    duplicate_count += 1
                                else:
                                    self.seen_tweet_ids.add(tweet_id)
                except Exception:
                    continue
        
        return {
            'current_count': current_count,
            'new_tweets': new_tweets_count,
            'duplicates': duplicate_count,
            'unique_new': max(0, new_tweets_count - duplicate_count)
        }
    
    async def smart_scroll(self, distance: int = 1000, wait_time: float = 2.0):
        """æ™ºèƒ½æ»šåŠ¨ï¼ŒåŒ…å«ç„¦ç‚¹æ£€æŸ¥å’Œå¹³æ»‘æ»šåŠ¨"""
        try:
            # ç¡®ä¿é¡µé¢ç„¦ç‚¹
            await self.parser.ensure_page_focus()
            
            # è·å–å½“å‰æ»šåŠ¨ä½ç½®
            current_scroll = await self.parser.page.evaluate('window.pageYOffset')
            
            # å¹³æ»‘æ»šåŠ¨
            await self.parser.page.evaluate(f'''
                window.scrollTo({{
                    top: {current_scroll + distance},
                    behavior: 'smooth'
                }});
            ''')
            
            # ç­‰å¾…æ»šåŠ¨å®Œæˆå’Œå†…å®¹åŠ è½½
            await asyncio.sleep(wait_time)
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾é¡µé¢åº•éƒ¨
            scroll_info = await self.parser.page.evaluate('''
                ({
                    scrollTop: window.pageYOffset,
                    scrollHeight: document.documentElement.scrollHeight,
                    clientHeight: window.innerHeight
                })
            ''')
            
            is_at_bottom = (scroll_info['scrollTop'] + scroll_info['clientHeight'] >= 
                          scroll_info['scrollHeight'] - 100)
            
            return {
                'scrolled': True,
                'at_bottom': is_at_bottom,
                'scroll_position': scroll_info['scrollTop']
            }
            
        except Exception as e:
            self.logger.warning(f"æ»šåŠ¨å¤±è´¥: {e}")
            return {'scrolled': False, 'at_bottom': False, 'scroll_position': 0}
    
    async def adaptive_scroll_strategy(self, target_tweets: int = 15, max_attempts: int = 20):
        """è‡ªé€‚åº”æ»šåŠ¨ç­–ç•¥"""
        self.logger.info(f"ğŸš€ å¼€å§‹è‡ªé€‚åº”æ»šåŠ¨ç­–ç•¥ï¼Œç›®æ ‡: {target_tweets} æ¡æ¨æ–‡")
        
        scroll_attempt = 0
        stagnant_rounds = 0
        last_unique_count = 0
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°
        base_scroll_distance = 800
        base_wait_time = 1.5
        
        while scroll_attempt < max_attempts:
            # è·å–æ»šåŠ¨å‰çš„æ¨æ–‡çŠ¶æ€
            pre_scroll_info = await self.check_for_new_tweets(0)
            current_unique_tweets = len(self.seen_tweet_ids)
            
            self.logger.info(f"ğŸ“Š æ»šåŠ¨å°è¯• {scroll_attempt + 1}/{max_attempts}ï¼Œå½“å‰å”¯ä¸€æ¨æ–‡: {current_unique_tweets}/{target_tweets}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if current_unique_tweets >= target_tweets:
                self.logger.info(f"ğŸ¯ è¾¾åˆ°ç›®æ ‡æ¨æ–‡æ•°é‡: {current_unique_tweets}")
                break
            
            # æ£€æŸ¥åœæ»æƒ…å†µ
            if current_unique_tweets == last_unique_count:
                stagnant_rounds += 1
            else:
                stagnant_rounds = 0
            
            last_unique_count = current_unique_tweets
            
            # æ ¹æ®åœæ»æƒ…å†µè°ƒæ•´ç­–ç•¥
            if stagnant_rounds >= 3:
                # æ¿€è¿›æ¨¡å¼ï¼šå¢åŠ æ»šåŠ¨è·ç¦»ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
                scroll_distance = base_scroll_distance * 2
                wait_time = base_wait_time * 0.7
                self.logger.debug(f"ğŸ”¥ æ¿€è¿›æ¨¡å¼ï¼šæ»šåŠ¨è·ç¦» {scroll_distance}ï¼Œç­‰å¾…æ—¶é—´ {wait_time:.1f}s")
            elif stagnant_rounds >= 6:
                # è¶…æ¿€è¿›æ¨¡å¼ï¼šå¤§å¹…æ»šåŠ¨
                scroll_distance = base_scroll_distance * 3
                wait_time = base_wait_time * 0.5
                self.logger.debug(f"âš¡ è¶…æ¿€è¿›æ¨¡å¼ï¼šæ»šåŠ¨è·ç¦» {scroll_distance}ï¼Œç­‰å¾…æ—¶é—´ {wait_time:.1f}s")
            else:
                # æ­£å¸¸æ¨¡å¼
                scroll_distance = base_scroll_distance
                wait_time = base_wait_time
            
            # æ‰§è¡Œæ»šåŠ¨
            scroll_result = await self.smart_scroll(scroll_distance, wait_time)
            
            if not scroll_result['scrolled']:
                self.logger.warning("æ»šåŠ¨å¤±è´¥ï¼Œå°è¯•ç»§ç»­")
                await asyncio.sleep(1)
            
            # æ£€æŸ¥æ»šåŠ¨åçš„æ•ˆæœ
            post_scroll_info = await self.check_for_new_tweets(pre_scroll_info['current_count'])
            
            # æ›´æ–°æŒ‡æ ‡
            self.scroll_metrics['total_scrolls'] += 1
            if post_scroll_info['unique_new'] > 0:
                self.scroll_metrics['successful_loads'] += 1
            
            self.scroll_metrics['duplicate_detections'] += post_scroll_info['duplicates']
            
            self.logger.debug(f"ğŸ“ˆ æœ¬è½®æ•ˆæœ: +{post_scroll_info['unique_new']} å”¯ä¸€æ¨æ–‡, +{post_scroll_info['duplicates']} é‡å¤")
            
            # å¦‚æœåˆ°è¾¾é¡µé¢åº•éƒ¨ä¸”è¿ç»­å¤šè½®æ— æ–°å†…å®¹ï¼Œè€ƒè™‘åˆ·æ–°
            if scroll_result['at_bottom'] and stagnant_rounds >= 5:
                self.logger.info("ğŸ”„ åˆ°è¾¾é¡µé¢åº•éƒ¨ä¸”é•¿æ—¶é—´æ— æ–°å†…å®¹ï¼Œå°è¯•åˆ·æ–°é¡µé¢")
                try:
                    await self.parser.page.reload(wait_until='domcontentloaded')
                    await asyncio.sleep(3)
                    stagnant_rounds = 0
                    # é‡æ–°æ”¶é›†å·²è§è¿‡çš„æ¨æ–‡ID
                    await self.rebuild_seen_tweets()
                except Exception as e:
                    self.logger.warning(f"é¡µé¢åˆ·æ–°å¤±è´¥: {e}")
            
            scroll_attempt += 1
        
        # è®¡ç®—æ•ˆç‡åˆ†æ•°
        if self.scroll_metrics['total_scrolls'] > 0:
            self.scroll_metrics['efficiency_score'] = (
                self.scroll_metrics['successful_loads'] / self.scroll_metrics['total_scrolls']
            )
        
        final_unique_tweets = len(self.seen_tweet_ids)
        self.logger.info(f"ğŸ“Š æ»šåŠ¨ç­–ç•¥å®Œæˆ: {final_unique_tweets} æ¡å”¯ä¸€æ¨æ–‡ï¼Œ{scroll_attempt} æ¬¡æ»šåŠ¨")
        self.logger.info(f"ğŸ“ˆ æ•ˆç‡æŒ‡æ ‡: {self.scroll_metrics}")
        
        return {
            'final_tweet_count': final_unique_tweets,
            'scroll_attempts': scroll_attempt,
            'metrics': self.scroll_metrics
        }
    
    async def rebuild_seen_tweets(self):
        """é‡æ–°æ„å»ºå·²è§æ¨æ–‡IDé›†åˆ"""
        try:
            self.seen_tweet_ids.clear()
            current_elements = await self.get_current_tweet_elements()
            
            for element in current_elements:
                try:
                    link_element = await element.query_selector('a[href*="/status/"]')
                    if link_element:
                        href = await link_element.get_attribute('href')
                        if href:
                            tweet_id = self.extract_tweet_id(href)
                            if tweet_id:
                                self.seen_tweet_ids.add(tweet_id)
                except Exception:
                    continue
            
            self.logger.debug(f"é‡å»ºå·²è§æ¨æ–‡IDé›†åˆ: {len(self.seen_tweet_ids)} æ¡")
        except Exception as e:
            self.logger.warning(f"é‡å»ºå·²è§æ¨æ–‡IDå¤±è´¥: {e}")

async def test_optimized_scroll_strategy():
    """æµ‹è¯•ä¼˜åŒ–çš„æ»šåŠ¨ç­–ç•¥"""
    launcher = None
    parser = None
    
    try:
        # å¯åŠ¨æµè§ˆå™¨
        logger.info("ğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        launcher = AdsPowerLauncher(ADS_POWER_CONFIG)
        browser_info = launcher.start_browser()
        launcher.wait_for_browser_ready()
        debug_port = launcher.get_debug_port()
        logger.info(f"âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼Œè°ƒè¯•ç«¯å£: {debug_port}")
        
        # åˆå§‹åŒ–TwitterParser
        logger.info("ğŸ”§ åˆå§‹åŒ–TwitterParser...")
        parser = TwitterParser(debug_port=debug_port)
        await parser.initialize()
        logger.info("âœ… TwitterParseråˆå§‹åŒ–å®Œæˆ")
        
        # å¯¼èˆªåˆ°æµ‹è¯•é¡µé¢
        logger.info("ğŸ”„ å¯¼èˆªåˆ°@elonmuské¡µé¢...")
        try:
            await parser.navigate_to_profile('elonmusk')
        except Exception:
            await parser.page.goto('https://x.com/elonmusk', wait_until='domcontentloaded')
        
        await asyncio.sleep(5)
        
        # åˆ›å»ºä¼˜åŒ–æ»šåŠ¨ç­–ç•¥å®ä¾‹
        scroll_strategy = OptimizedScrollStrategy(parser)
        
        # æ‰§è¡Œä¼˜åŒ–æ»šåŠ¨
        target_tweets = 20
        result = await scroll_strategy.adaptive_scroll_strategy(target_tweets, max_attempts=15)
        
        # è·å–æœ€ç»ˆæ¨æ–‡å¹¶è§£æ
        logger.info("ğŸ” å¼€å§‹è§£ææ¨æ–‡æ•°æ®...")
        final_elements = await scroll_strategy.get_current_tweet_elements()
        
        parsed_tweets = []
        for i, element in enumerate(final_elements[:target_tweets]):
            try:
                tweet_data = await parser.parse_tweet_element(element)
                if tweet_data and (tweet_data.get('content') or tweet_data.get('username') != 'unknown'):
                    parsed_tweets.append(tweet_data)
                    logger.info(f"  âœ… æ¨æ–‡ {i+1}: @{tweet_data.get('username', 'unknown')} - {tweet_data.get('content', 'No content')[:50]}...")
            except Exception as e:
                logger.warning(f"  âŒ æ¨æ–‡ {i+1}: è§£æå¤±è´¥ - {e}")
        
        # åº”ç”¨è¿‡æ»¤å™¨
        tweet_filter = TweetFilter()
        filtered_tweets = []
        for tweet in parsed_tweets:
            try:
                if tweet_filter.should_include_tweet(tweet):
                    filtered_tweets.append(tweet)
            except Exception as e:
                logger.warning(f"è¿‡æ»¤æ¨æ–‡æ—¶å‡ºé”™: {e}")
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        test_report = {
            'timestamp': datetime.now().isoformat(),
            'test_type': 'optimized_scroll_strategy',
            'target_tweets': target_tweets,
            'scroll_result': result,
            'final_elements_count': len(final_elements),
            'parsed_tweets_count': len(parsed_tweets),
            'filtered_tweets_count': len(filtered_tweets),
            'parse_success_rate': len(parsed_tweets) / max(len(final_elements), 1) * 100,
            'filter_pass_rate': len(filtered_tweets) / max(len(parsed_tweets), 1) * 100,
            'overall_efficiency': len(filtered_tweets) / max(result['scroll_attempts'], 1),
            'sample_tweets': filtered_tweets[:3]
        }
        
        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        report_file = f"optimized_scroll_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(test_report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # è¾“å‡ºæµ‹è¯•ç»“æœ
        logger.info("\nğŸ“Š ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥æµ‹è¯•ç»“æœ:")
        logger.info(f"  ğŸ¯ ç›®æ ‡æ¨æ–‡æ•°é‡: {target_tweets}")
        logger.info(f"  ğŸ“ˆ æœ€ç»ˆå…ƒç´ æ•°é‡: {len(final_elements)}")
        logger.info(f"  âœ… æˆåŠŸè§£ææ¨æ–‡: {len(parsed_tweets)}")
        logger.info(f"  ğŸ›ï¸ è¿‡æ»¤åæ¨æ–‡: {len(filtered_tweets)}")
        logger.info(f"  ğŸ”„ æ»šåŠ¨æ¬¡æ•°: {result['scroll_attempts']}")
        logger.info(f"  ğŸ“Š è§£ææˆåŠŸç‡: {test_report['parse_success_rate']:.1f}%")
        logger.info(f"  ğŸ¯ è¿‡æ»¤é€šè¿‡ç‡: {test_report['filter_pass_rate']:.1f}%")
        logger.info(f"  âš¡ æ•´ä½“æ•ˆç‡: {test_report['overall_efficiency']:.2f} æ¨æ–‡/æ»šåŠ¨")
        
        # ä¸ä¹‹å‰çš„ç»“æœæ¯”è¾ƒ
        improvement_score = test_report['overall_efficiency']
        if improvement_score >= 1.0:
            logger.info(f"ğŸ‰ ä¼˜åŒ–æˆåŠŸï¼æ•ˆç‡æå‡æ˜æ˜¾ (æ•ˆç‡åˆ†æ•°: {improvement_score:.2f})")
            return True
        elif improvement_score >= 0.6:
            logger.info(f"âœ… ä¼˜åŒ–æœ‰æ•ˆï¼Œç³»ç»Ÿæ€§èƒ½è‰¯å¥½ (æ•ˆç‡åˆ†æ•°: {improvement_score:.2f})")
            return True
        else:
            logger.warning(f"âš ï¸ ä¼˜åŒ–æ•ˆæœæœ‰é™ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ (æ•ˆç‡åˆ†æ•°: {improvement_score:.2f})")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
        return False
        
    finally:
        # æ¸…ç†èµ„æº
        if parser:
            try:
                await parser.close()
                logger.info("âœ… TwitterParserå·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­TwitterParseræ—¶å‡ºé”™: {e}")
        
        if launcher:
            try:
                launcher.stop_browser()
                logger.info("âœ… æµè§ˆå™¨å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    success = asyncio.run(test_optimized_scroll_strategy())
    if success:
        logger.info("\nğŸŠ ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥æµ‹è¯•æˆåŠŸï¼")
    else:
        logger.error("\nğŸ’¥ ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥æµ‹è¯•å¤±è´¥")
        sys.exit(1)