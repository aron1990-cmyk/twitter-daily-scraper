#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„å®æ—¶è§£ææµ‹è¯•
ä¸“æ³¨äºéªŒè¯å®æ—¶è§£æã€å¢é‡ä¿å­˜å’Œæµ‹è¯•é€»è¾‘
"""

import asyncio
import logging
import json
from datetime import datetime
from typing import Dict, List, Any, Set

# æ¨¡æ‹Ÿæ¨æ–‡æ•°æ®
MOCK_TWEETS = [
    {
        "id": "1234567890123456789",
        "username": "elonmusk",
        "content": "Mars is looking good for settlement",
        "timestamp": "2025-07-21T12:00:00Z",
        "likes": 15420,
        "retweets": 3240,
        "replies": 890
    },
    {
        "id": "1234567890123456790",
        "username": "elonmusk", 
        "content": "Tesla production update: Q4 looking strong",
        "timestamp": "2025-07-21T11:30:00Z",
        "likes": 8930,
        "retweets": 1560,
        "replies": 445
    },
    {
        "id": "1234567890123456791",
        "username": "elonmusk",
        "content": "Neuralink progress is accelerating",
        "timestamp": "2025-07-21T11:00:00Z",
        "likes": 12340,
        "retweets": 2890,
        "replies": 670
    },
    {
        "id": "1234567890123456792",
        "username": "elonmusk",
        "content": "SpaceX Starship test flight scheduled for next month",
        "timestamp": "2025-07-21T10:30:00Z",
        "likes": 18750,
        "retweets": 4320,
        "replies": 1230
    },
    {
        "id": "1234567890123456793",
        "username": "elonmusk",
        "content": "AI safety research is crucial for humanity's future",
        "timestamp": "2025-07-21T10:00:00Z",
        "likes": 9870,
        "retweets": 2140,
        "replies": 580
    }
]

class MockRealtimeParser:
    """æ¨¡æ‹Ÿå®æ—¶è§£æå™¨ï¼Œç”¨äºæµ‹è¯•å®æ—¶è§£æé€»è¾‘"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # å®æ—¶è§£æçŠ¶æ€
        self.parsed_tweets: List[Dict[str, Any]] = []
        self.seen_tweet_ids: Set[str] = set()
        self.parsing_stats = {
            'total_scrolls': 0,
            'tweets_parsed': 0,
            'duplicates_skipped': 0,
            'parsing_errors': 0,
            'incremental_saves': 0
        }
        
        # æ¨¡æ‹Ÿæ»šåŠ¨çŠ¶æ€
        self.current_scroll_position = 0
        self.tweets_per_scroll = 2  # æ¯æ¬¡æ»šåŠ¨æ˜¾ç¤º2æ¡æ–°æ¨æ–‡
    
    async def simulate_scroll_and_parse_realtime(self, target_tweets: int = 10, max_attempts: int = 20) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå®æ—¶æ»šåŠ¨å’Œè§£ææ¨æ–‡"""
        self.logger.info(f"ğŸš€ å¼€å§‹æ¨¡æ‹Ÿå®æ—¶æ»šåŠ¨è§£æï¼Œç›®æ ‡: {target_tweets} æ¡æ¨æ–‡")
        
        scroll_attempt = 0
        stagnant_rounds = 0
        
        while scroll_attempt < max_attempts and len(self.parsed_tweets) < target_tweets:
            self.parsing_stats['total_scrolls'] = scroll_attempt + 1
            
            self.logger.info(f"ğŸ“Š æ»šåŠ¨å°è¯• {scroll_attempt + 1}/{max_attempts}ï¼Œå·²è§£æ: {len(self.parsed_tweets)}/{target_tweets}")
            
            # æ¨¡æ‹Ÿè·å–å½“å‰å¯è§çš„æ¨æ–‡
            visible_tweets = self._get_visible_tweets_after_scroll()
            
            # å®æ—¶è§£ææ–°å‡ºç°çš„æ¨æ–‡
            new_tweets_parsed = await self._parse_new_tweets_realtime(visible_tweets)
            
            if new_tweets_parsed > 0:
                self.logger.info(f"âœ… æœ¬è½®è§£æäº† {new_tweets_parsed} æ¡æ–°æ¨æ–‡ï¼Œæ€»è®¡: {len(self.parsed_tweets)}")
                stagnant_rounds = 0
                
                # å¢é‡ä¿å­˜ï¼ˆæ¯è§£æ5æ¡æ¨æ–‡ä¿å­˜ä¸€æ¬¡ï¼‰
                if len(self.parsed_tweets) % 5 == 0:
                    await self._incremental_save()
            else:
                stagnant_rounds += 1
                self.logger.debug(f"âš ï¸ æœ¬è½®æœªå‘ç°æ–°æ¨æ–‡ï¼Œåœæ»è½®æ•°: {stagnant_rounds}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if len(self.parsed_tweets) >= target_tweets:
                self.logger.info(f"ğŸ¯ è¾¾åˆ°ç›®æ ‡æ¨æ–‡æ•°é‡: {len(self.parsed_tweets)}")
                break
            
            # æ¨¡æ‹Ÿæ»šåŠ¨å»¶è¿Ÿ
            await asyncio.sleep(0.5)
            scroll_attempt += 1
        
        # æœ€ç»ˆä¿å­˜
        final_file = await self._final_save()
        
        # ç”Ÿæˆç»“æœæ‘˜è¦
        result = {
            'parsed_tweets_count': len(self.parsed_tweets),
            'target_tweets': target_tweets,
            'scroll_attempts': scroll_attempt,
            'target_reached': len(self.parsed_tweets) >= target_tweets,
            'efficiency': len(self.parsed_tweets) / max(scroll_attempt, 1),
            'parsing_stats': self.parsing_stats.copy(),
            'parsed_tweets': self.parsed_tweets.copy(),
            'final_save_file': final_file
        }
        
        self.logger.info(f"ğŸ“Š æ¨¡æ‹Ÿå®æ—¶è§£æå®Œæˆ: {len(self.parsed_tweets)} æ¡æ¨æ–‡ï¼Œ{scroll_attempt} æ¬¡æ»šåŠ¨")
        return result
    
    def _get_visible_tweets_after_scroll(self) -> List[Dict[str, Any]]:
        """æ¨¡æ‹Ÿæ»šåŠ¨åè·å–å¯è§çš„æ¨æ–‡"""
        # æ¨¡æ‹Ÿæ¯æ¬¡æ»šåŠ¨æ˜¾ç¤ºæ–°çš„æ¨æ–‡
        start_idx = self.current_scroll_position
        end_idx = min(start_idx + self.tweets_per_scroll, len(MOCK_TWEETS))
        
        visible_tweets = MOCK_TWEETS[start_idx:end_idx]
        self.current_scroll_position = end_idx
        
        self.logger.debug(f"ğŸ“± æ»šåŠ¨åå¯è§æ¨æ–‡: {len(visible_tweets)} æ¡ (ä½ç½® {start_idx}-{end_idx})")
        return visible_tweets
    
    async def _parse_new_tweets_realtime(self, visible_tweets: List[Dict[str, Any]]) -> int:
        """å®æ—¶è§£ææ–°å‡ºç°çš„æ¨æ–‡"""
        new_tweets_count = 0
        
        for tweet in visible_tweets:
            tweet_id = tweet['id']
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è§£æè¿‡
            if tweet_id in self.seen_tweet_ids:
                self.parsing_stats['duplicates_skipped'] += 1
                self.logger.debug(f"â­ï¸ è·³è¿‡é‡å¤æ¨æ–‡: {tweet_id[:8]}...")
                continue
            
            try:
                # æ¨¡æ‹Ÿè§£æè¿‡ç¨‹
                parsed_tweet = await self._parse_tweet_safe(tweet)
                if parsed_tweet:
                    self.parsed_tweets.append(parsed_tweet)
                    self.seen_tweet_ids.add(tweet_id)
                    self.parsing_stats['tweets_parsed'] += 1
                    new_tweets_count += 1
                    
                    self.logger.info(f"âœ… å®æ—¶è§£ææ–°æ¨æ–‡: @{parsed_tweet['username']} - {tweet_id[:8]}... - {parsed_tweet['content'][:30]}...")
                
            except Exception as e:
                self.parsing_stats['parsing_errors'] += 1
                self.logger.warning(f"âŒ è§£ææ¨æ–‡å¤±è´¥ {tweet_id[:8]}...: {e}")
                continue
        
        return new_tweets_count
    
    async def _parse_tweet_safe(self, tweet: Dict[str, Any]) -> Dict[str, Any]:
        """å®‰å…¨åœ°è§£ææ¨æ–‡æ•°æ®"""
        # æ¨¡æ‹Ÿè§£æå»¶è¿Ÿ
        await asyncio.sleep(0.1)
        
        # æ·»åŠ è§£ææ—¶é—´æˆ³å’Œè´¨é‡æŒ‡æ ‡
        parsed_tweet = tweet.copy()
        parsed_tweet['parsed_at'] = datetime.now().isoformat()
        parsed_tweet['content_length'] = len(tweet.get('content', ''))
        parsed_tweet['has_metrics'] = all(key in tweet for key in ['likes', 'retweets', 'replies'])
        
        return parsed_tweet
    
    async def _incremental_save(self) -> str:
        """å¢é‡ä¿å­˜è§£æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'realtime_incremental_{timestamp}.json'
        
        incremental_data = {
            'save_type': 'incremental',
            'timestamp': datetime.now().isoformat(),
            'total_tweets': len(self.parsed_tweets),
            'parsing_stats': self.parsing_stats.copy(),
            'latest_tweets': self.parsed_tweets[-5:] if len(self.parsed_tweets) >= 5 else self.parsed_tweets
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(incremental_data, f, ensure_ascii=False, indent=2)
            
            self.parsing_stats['incremental_saves'] += 1
            self.logger.info(f"ğŸ’¾ å¢é‡ä¿å­˜å®Œæˆ: {filename} ({len(self.parsed_tweets)} æ¡æ¨æ–‡)")
            return filename
        except Exception as e:
            self.logger.error(f"å¢é‡ä¿å­˜å¤±è´¥: {e}")
            return ""
    
    async def _final_save(self) -> str:
        """æœ€ç»ˆä¿å­˜æ‰€æœ‰è§£æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'realtime_final_results_{timestamp}.json'
        
        final_data = {
            'save_type': 'final',
            'timestamp': datetime.now().isoformat(),
            'total_tweets': len(self.parsed_tweets),
            'unique_tweets': len(self.seen_tweet_ids),
            'parsing_stats': self.parsing_stats.copy(),
            'quality_metrics': self._calculate_quality_metrics(),
            'all_tweets': self.parsed_tweets
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“„ æœ€ç»ˆç»“æœå·²ä¿å­˜: {filename}")
            return filename
        except Exception as e:
            self.logger.error(f"æœ€ç»ˆä¿å­˜å¤±è´¥: {e}")
            return ""
    
    def _calculate_quality_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡"""
        if not self.parsed_tweets:
            return {}
        
        total_tweets = len(self.parsed_tweets)
        
        # å†…å®¹è´¨é‡
        has_content = sum(1 for t in self.parsed_tweets if t.get('content'))
        has_username = sum(1 for t in self.parsed_tweets if t.get('username'))
        has_metrics = sum(1 for t in self.parsed_tweets if t.get('has_metrics', False))
        
        # è§£ææ•ˆç‡
        total_attempts = self.parsing_stats['tweets_parsed'] + self.parsing_stats['duplicates_skipped'] + self.parsing_stats['parsing_errors']
        
        return {
            'content_completeness': has_content / total_tweets,
            'username_completeness': has_username / total_tweets,
            'metrics_completeness': has_metrics / total_tweets,
            'parsing_success_rate': self.parsing_stats['tweets_parsed'] / max(total_attempts, 1),
            'duplicate_rate': self.parsing_stats['duplicates_skipped'] / max(total_attempts, 1),
            'error_rate': self.parsing_stats['parsing_errors'] / max(total_attempts, 1),
            'efficiency_per_scroll': self.parsing_stats['tweets_parsed'] / max(self.parsing_stats['total_scrolls'], 1)
        }
    
    def get_parsing_summary(self) -> Dict[str, Any]:
        """è·å–è§£ææ‘˜è¦"""
        return {
            'total_parsed': len(self.parsed_tweets),
            'unique_tweets': len(self.seen_tweet_ids),
            'parsing_stats': self.parsing_stats.copy(),
            'quality_metrics': self._calculate_quality_metrics()
        }


class RealtimeParsingTest:
    """å®æ—¶è§£ææµ‹è¯•ç±»"""
    
    def __init__(self, target_tweets: int = 10):
        self.target_tweets = target_tweets
        self.logger = logging.getLogger(__name__)
        self.parser = MockRealtimeParser()
    
    async def test_realtime_parsing_logic(self) -> Dict[str, Any]:
        """æµ‹è¯•å®æ—¶è§£æé€»è¾‘"""
        test_result = {
            'test_name': 'realtime_parsing_logic_test',
            'success': False,
            'details': {},
            'errors': []
        }
        
        try:
            self.logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•å®æ—¶è§£æé€»è¾‘ï¼ˆç›®æ ‡: {self.target_tweets}æ¡ï¼‰...")
            
            # æ‰§è¡Œæ¨¡æ‹Ÿå®æ—¶æ»šåŠ¨è§£æ
            parse_result = await self.parser.simulate_scroll_and_parse_realtime(
                target_tweets=self.target_tweets,
                max_attempts=self.target_tweets
            )
            
            # è·å–è§£ææ‘˜è¦
            parsing_summary = self.parser.get_parsing_summary()
            
            test_result['details'] = {
                'target_tweets': self.target_tweets,
                'parsed_tweets': parse_result['parsed_tweets_count'],
                'scroll_attempts': parse_result['scroll_attempts'],
                'target_reached': parse_result['target_reached'],
                'efficiency': parse_result['efficiency'],
                'parsing_summary': parsing_summary,
                'final_save_file': parse_result['final_save_file'],
                'sample_tweet_keys': list(parse_result['parsed_tweets'][0].keys()) if parse_result['parsed_tweets'] else []
            }
            
            # éªŒè¯å®æ—¶è§£æçš„å…³é”®ç‰¹æ€§
            realtime_validation = self._validate_realtime_features(parse_result, parsing_summary)
            test_result['details']['realtime_validation'] = realtime_validation
            
            # åˆ¤æ–­æµ‹è¯•æˆåŠŸ
            if (parse_result['parsed_tweets_count'] > 0 and 
                realtime_validation['incremental_saves_working'] and
                realtime_validation['duplicate_detection_working']):
                test_result['success'] = True
                self.logger.info(f"âœ… å®æ—¶è§£æé€»è¾‘æµ‹è¯•æˆåŠŸ: {parse_result['parsed_tweets_count']} æ¡æ¨æ–‡")
            else:
                test_result['errors'].append("å®æ—¶è§£æå…³é”®ç‰¹æ€§éªŒè¯å¤±è´¥")
                self.logger.error("âŒ å®æ—¶è§£æé€»è¾‘æµ‹è¯•å¤±è´¥")
            
        except Exception as e:
            test_result['errors'].append(str(e))
            self.logger.error(f"âŒ å®æ—¶è§£æé€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        
        return test_result
    
    def _validate_realtime_features(self, parse_result: Dict[str, Any], parsing_summary: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å®æ—¶è§£æçš„å…³é”®ç‰¹æ€§"""
        validation = {
            'incremental_saves_working': False,
            'duplicate_detection_working': False,
            'realtime_parsing_working': False,
            'quality_metrics_working': False
        }
        
        # æ£€æŸ¥å¢é‡ä¿å­˜
        if parsing_summary['parsing_stats']['incremental_saves'] > 0:
            validation['incremental_saves_working'] = True
            self.logger.info(f"âœ… å¢é‡ä¿å­˜åŠŸèƒ½æ­£å¸¸: {parsing_summary['parsing_stats']['incremental_saves']} æ¬¡ä¿å­˜")
        
        # æ£€æŸ¥é‡å¤æ£€æµ‹
        if parsing_summary['parsing_stats']['duplicates_skipped'] >= 0:  # å³ä½¿ä¸º0ä¹Ÿè¯´æ˜æ£€æµ‹æœºåˆ¶åœ¨å·¥ä½œ
            validation['duplicate_detection_working'] = True
            self.logger.info(f"âœ… é‡å¤æ£€æµ‹åŠŸèƒ½æ­£å¸¸: {parsing_summary['parsing_stats']['duplicates_skipped']} æ¡é‡å¤")
        
        # æ£€æŸ¥å®æ—¶è§£æ
        if parse_result['parsed_tweets_count'] > 0 and parse_result['efficiency'] > 0:
            validation['realtime_parsing_working'] = True
            self.logger.info(f"âœ… å®æ—¶è§£æåŠŸèƒ½æ­£å¸¸: æ•ˆç‡ {parse_result['efficiency']:.2f} æ¨æ–‡/æ»šåŠ¨")
        
        # æ£€æŸ¥è´¨é‡æŒ‡æ ‡
        quality_metrics = parsing_summary.get('quality_metrics', {})
        if quality_metrics and quality_metrics.get('content_completeness', 0) > 0:
            validation['quality_metrics_working'] = True
            self.logger.info(f"âœ… è´¨é‡æŒ‡æ ‡åŠŸèƒ½æ­£å¸¸: å†…å®¹å®Œæ•´æ€§ {quality_metrics['content_completeness']:.1%}")
        
        return validation


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(__name__)
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test = RealtimeParsingTest(target_tweets=8)
    
    try:
        # è¿è¡Œå®æ—¶è§£æé€»è¾‘æµ‹è¯•
        result = await test.test_realtime_parsing_logic()
        
        # è¾“å‡ºæµ‹è¯•ç»“æœ
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š å®æ—¶è§£æé€»è¾‘æµ‹è¯•ç»“æœ")
        logger.info("="*60)
        logger.info(f"æµ‹è¯•çŠ¶æ€: {'âœ… æˆåŠŸ' if result['success'] else 'âŒ å¤±è´¥'}")
        
        if result['success']:
            details = result['details']
            logger.info(f"ç›®æ ‡æ¨æ–‡æ•°: {details['target_tweets']}")
            logger.info(f"å®é™…è§£ææ•°: {details['parsed_tweets']}")
            logger.info(f"æ»šåŠ¨æ¬¡æ•°: {details['scroll_attempts']}")
            logger.info(f"è§£ææ•ˆç‡: {details['efficiency']:.2f} æ¨æ–‡/æ»šåŠ¨")
            
            # å®æ—¶è§£æç‰¹æ€§éªŒè¯ç»“æœ
            validation = details['realtime_validation']
            logger.info("\nğŸ” å®æ—¶è§£æç‰¹æ€§éªŒè¯:")
            for feature, status in validation.items():
                status_icon = "âœ…" if status else "âŒ"
                logger.info(f"  {status_icon} {feature}: {status}")
            
            # è´¨é‡æŒ‡æ ‡
            quality = details['parsing_summary']['quality_metrics']
            logger.info("\nğŸ“ˆ æ•°æ®è´¨é‡æŒ‡æ ‡:")
            logger.info(f"  å†…å®¹å®Œæ•´æ€§: {quality['content_completeness']:.1%}")
            logger.info(f"  è§£ææˆåŠŸç‡: {quality['parsing_success_rate']:.1%}")
            logger.info(f"  é‡å¤æ£€æµ‹ç‡: {quality['duplicate_rate']:.1%}")
            logger.info(f"  æ»šåŠ¨æ•ˆç‡: {quality['efficiency_per_scroll']:.2f}")
            
            logger.info(f"\nğŸ“„ ç»“æœæ–‡ä»¶: {details['final_save_file']}")
        else:
            logger.error(f"é”™è¯¯ä¿¡æ¯: {result['errors']}")
        
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(main())