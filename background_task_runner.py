#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åå°ä»»åŠ¡è¿è¡Œå™¨
ç”¨äºåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­æ‰§è¡ŒæŠ“å–ä»»åŠ¡ï¼Œç¡®ä¿ä»»åŠ¡ä¸å—çª—å£åˆ‡æ¢å½±å“
"""

import sys
import os
import json
import asyncio
import logging
from datetime import datetime
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# å¯¼å…¥æ•°æ®åº“å’Œåº”ç”¨é…ç½®
from web_app import app, db, ScrapingTask, TweetData
from ads_browser_launcher import AdsPowerLauncher
from twitter_parser import TwitterParser
from cloud_sync import CloudSyncManager
from excel_writer import ExcelWriter
from exception_handler import ExceptionHandler, resilient_task_execution

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('background_task.log', encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

def load_adspower_config_from_db():
    """
    ä»æ•°æ®åº“åŠ è½½AdsPoweré…ç½®
    
    Returns:
        AdsPoweré…ç½®å­—å…¸
    """
    try:
        import sqlite3
        conn = sqlite3.connect('instance/twitter_scraper.db')
        cursor = conn.cursor()
        
        # æŸ¥è¯¢AdsPowerç›¸å…³é…ç½®
        cursor.execute("SELECT key, value FROM system_config WHERE key LIKE '%adspower%'")
        configs = cursor.fetchall()
        
        config_dict = {}
        api_host = None
        api_port = None
        
        for key, value in configs:
            if key == 'adspower_api_host':
                api_host = value
            elif key == 'adspower_api_port':
                api_port = value
            elif key == 'adspower_user_id':
                config_dict['user_id'] = value
            elif key == 'adspower_group_id':
                config_dict['group_id'] = value
            elif key == 'adspower_api_status':
                config_dict['api_status'] = value
            elif key == 'adspower_api_key':
                config_dict['api_key'] = value
        
        # æ„å»ºå®Œæ•´çš„API URL
        if api_host and api_port:
            config_dict['local_api_url'] = f"http://{api_host}:{api_port}"
        else:
            config_dict['local_api_url'] = 'http://local.adspower.net:50325'
        
        conn.close()
        
        # å¦‚æœæ²¡æœ‰ç”¨æˆ·IDï¼Œä½¿ç”¨é»˜è®¤å€¼
        if 'user_id' not in config_dict or not config_dict['user_id']:
            config_dict['user_id'] = 'k11p9ypc'
        
        print(f"ä»æ•°æ®åº“åŠ è½½AdsPoweré…ç½®: {config_dict}")
        return config_dict
        
    except Exception as e:
        print(f"ä»æ•°æ®åº“åŠ è½½AdsPoweré…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return {
            'local_api_url': 'http://local.adspower.net:50325',
            'user_id': 'k11p9ypc',
            'group_id': ''
        }

@resilient_task_execution()
async def execute_scraping_task(task_id: int, user_id: str):
    """æ‰§è¡ŒæŠ“å–ä»»åŠ¡çš„æ ¸å¿ƒé€»è¾‘"""
    logger = logging.getLogger(__name__)
    
    with app.app_context():
        logger.info(f"="*60)
        logger.info(f"ğŸš€ ä»»åŠ¡å¯åŠ¨æµç¨‹å¼€å§‹ - ä»»åŠ¡ID: {task_id}")
        logger.info(f"="*60)
        
        # è·å–ä»»åŠ¡
        logger.info(f"ğŸ“‹ æ­¥éª¤1: è·å–ä»»åŠ¡ä¿¡æ¯")
        task = ScrapingTask.query.get(task_id)
        if not task:
            logger.error(f"âŒ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
            raise Exception(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
        
        logger.info(f"âœ… ä»»åŠ¡ä¿¡æ¯è·å–æˆåŠŸ:")
        logger.info(f"   - ä»»åŠ¡åç§°: {task.name}")
        logger.info(f"   - ä»»åŠ¡ID: {task.id}")
        logger.info(f"   - æœ€å¤§æ¨æ–‡æ•°: {task.max_tweets}")
        logger.info(f"   - æœ€å°ç‚¹èµæ•°: {task.min_likes}")
        logger.info(f"   - æœ€å°è¯„è®ºæ•°: {task.min_comments}")
        logger.info(f"   - æœ€å°è½¬å‘æ•°: {task.min_retweets}")
        logger.info(f"   - ä»»åŠ¡çŠ¶æ€: {task.status}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        logger.info(f"ğŸ“ æ­¥éª¤2: æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­")
        task.status = 'running'
        task.started_at = datetime.utcnow()
        db.session.commit()
        logger.info(f"âœ… ä»»åŠ¡çŠ¶æ€æ›´æ–°æˆåŠŸï¼Œå¼€å§‹æ—¶é—´: {task.started_at}")
        
        try:
            # è§£æé…ç½®
            logger.info(f"ğŸ”§ æ­¥éª¤3: è§£æä»»åŠ¡é…ç½®")
            target_accounts = json.loads(task.target_accounts or '[]')
            target_keywords = json.loads(task.target_keywords or '[]')
            logger.info(f"âœ… é…ç½®è§£æå®Œæˆ:")
            logger.info(f"   - ç›®æ ‡è´¦å·æ•°é‡: {len(target_accounts)}")
            logger.info(f"   - ç›®æ ‡è´¦å·åˆ—è¡¨: {target_accounts}")
            logger.info(f"   - ç›®æ ‡å…³é”®è¯æ•°é‡: {len(target_keywords)}")
            logger.info(f"   - ç›®æ ‡å…³é”®è¯åˆ—è¡¨: {target_keywords}")
            
            # å¯åŠ¨æµè§ˆå™¨
            logger.info(f"ğŸŒ æ­¥éª¤4: å¯åŠ¨AdsPoweræµè§ˆå™¨")
            logger.info(f"   - ä½¿ç”¨ç”¨æˆ·ID: {user_id}")
            
            # ä»æ•°æ®åº“åŠ è½½é…ç½®
            adspower_config = load_adspower_config_from_db()
            logger.info(f"   - AdsPoweré…ç½®åŠ è½½å®Œæˆ")
            logger.info(f"   - API URL: {adspower_config.get('local_api_url', 'N/A')}")
            logger.info(f"   - ç”¨æˆ·ID: {adspower_config.get('user_id', 'N/A')}")
            browser_manager = AdsPowerLauncher(adspower_config)
            
            try:
                # è¿›è¡Œå®Œæ•´çš„å¥åº·æ£€æŸ¥å’Œæµè§ˆå™¨å¯åŠ¨
                logger.info(f"ğŸ” æ­¥éª¤4.1: è¿›è¡ŒAdsPowerå¥åº·æ£€æŸ¥")
                logger.info(f"   - æ£€æŸ¥AdsPoweræœåŠ¡çŠ¶æ€")
                logger.info(f"   - æ£€æŸ¥ç”¨æˆ·é…ç½®æ–‡ä»¶")
                logger.info(f"   - éªŒè¯æµè§ˆå™¨ç¯å¢ƒ")
                
                browser_info = browser_manager.start_browser(user_id, skip_health_check=False)
                if not browser_info:
                    logger.error(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥ï¼šæœªè¿”å›æµè§ˆå™¨ä¿¡æ¯")
                    raise Exception("æµè§ˆå™¨å¯åŠ¨å¤±è´¥ï¼šæœªè¿”å›æµè§ˆå™¨ä¿¡æ¯")
                
                logger.info(f"âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ:")
                logger.info(f"   - æµè§ˆå™¨ä¿¡æ¯: {browser_info}")
                logger.info(f"   - WebSocketç«¯å£: {browser_info.get('ws', {}).get('puppeteer', 'N/A')}")
                logger.info(f"   - è°ƒè¯•ç«¯å£: {browser_info.get('debug_port', 'N/A')}")
                logger.info(f"   - æµè§ˆå™¨çŠ¶æ€: å·²å¯åŠ¨å¹¶å°±ç»ª")
                
            except Exception as e:
                logger.error(f"AdsPoweræµè§ˆå™¨å¯åŠ¨å¤±è´¥: {str(e)}")
                
                # è·å–è¯¦ç»†çš„å¥åº·æŠ¥å‘Š
                try:
                    health_report = browser_manager.get_health_report()
                    logger.error(f"ç³»ç»Ÿå¥åº·æŠ¥å‘Š: {health_report}")
                    
                    # å°è¯•è‡ªåŠ¨ä¿®å¤
                    logger.info("å°è¯•è‡ªåŠ¨ä¿®å¤ç³»ç»Ÿé—®é¢˜...")
                    if browser_manager.auto_optimize_system():
                        logger.info("ç³»ç»Ÿä¼˜åŒ–å®Œæˆï¼Œé‡æ–°å°è¯•å¯åŠ¨æµè§ˆå™¨...")
                        browser_info = browser_manager.start_browser(user_id, skip_health_check=True)
                        if browser_info:
                            logger.info("æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼ˆä¿®å¤åï¼‰")
                        else:
                            raise Exception("æµè§ˆå™¨å¯åŠ¨å¤±è´¥ï¼ˆä¿®å¤åä»ç„¶å¤±è´¥ï¼‰")
                    else:
                        raise Exception(f"AdsPoweræµè§ˆå™¨å¯åŠ¨å¤±è´¥ä¸”è‡ªåŠ¨ä¿®å¤å¤±è´¥: {str(e)}")
                        
                except Exception as repair_error:
                    logger.error(f"è‡ªåŠ¨ä¿®å¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(repair_error)}")
                    raise Exception(f"AdsPoweræµè§ˆå™¨å¯åŠ¨å¤±è´¥: {str(e)}ã€‚ä¿®å¤å°è¯•ä¹Ÿå¤±è´¥: {str(repair_error)}")
            
            debug_port = browser_info.get('ws', {}).get('puppeteer')
            
            # è¿æ¥è§£æå™¨
            logger.info(f"ğŸ”— æ­¥éª¤5: è¿æ¥Twitterè§£æå™¨")
            logger.info(f"   - ä½¿ç”¨è°ƒè¯•ç«¯å£: {debug_port}")
            parser = TwitterParser(debug_port)
            await parser.connect_browser()
            
            # ç¡®ä¿ä¼˜åŒ–åŠŸèƒ½å·²å¯ç”¨
            parser.enable_optimizations()
            logger.info(f"âœ… Twitterè§£æå™¨è¿æ¥æˆåŠŸï¼Œä¼˜åŒ–åŠŸèƒ½å·²å¯ç”¨")
            logger.info(f"   - ä¼˜åŒ–çŠ¶æ€: {parser.optimization_enabled}")
            logger.info(f"   - å·²è§æ¨æ–‡IDé›†åˆå¤§å°: {len(parser.seen_tweet_ids)}")
            logger.info(f"   - å†…å®¹ç¼“å­˜å¤§å°: {len(parser.content_cache)}")
            
            all_tweets = []
            content_shortage_details = []  # è®°å½•å†…å®¹ä¸è¶³çš„è¯¦ç»†ä¿¡æ¯
            
            # æŠ“å–æ¨æ–‡
            logger.info(f"ğŸ“Š æ­¥éª¤6: å¼€å§‹æ•°æ®æŠ“å–")
            logger.info(f"   - æ€»è®¡éœ€è¦æŠ“å– {len(target_accounts)} ä¸ªè´¦å·")
            logger.info(f"   - æ€»è®¡éœ€è¦æœç´¢ {len(target_keywords)} ä¸ªå…³é”®è¯")
            
            for i, account in enumerate(target_accounts, 1):
                try:
                    # æ¸…ç†ç”¨æˆ·åï¼Œå»é™¤@ç¬¦å·
                    clean_username = account.lstrip('@') if account.startswith('@') else account
                    logger.info(f"ğŸ“± æ­¥éª¤6.{i}: æŠ“å–åšä¸» @{clean_username} çš„æ¨æ–‡")
                    logger.info(f"   - è¿›åº¦: {i}/{len(target_accounts)}")
                    logger.info(f"   - ç›®æ ‡æ¨æ–‡æ•°: {task.max_tweets}")
                    logger.info(f"   - åŸå§‹è¾“å…¥: {account}")
                    logger.info(f"   - æ¸…ç†åç”¨æˆ·å: {clean_username}")
                    
                    await parser.navigate_to_profile(clean_username)
                    logger.info(f"   - å·²å¯¼èˆªåˆ°ç”¨æˆ·ä¸»é¡µ")
                    
                    # æ„å»ºç­›é€‰æ¡ä»¶
                    filter_criteria = {
                        'min_likes': task.min_likes,
                        'min_comments': task.min_comments,
                        'min_retweets': task.min_retweets
                    }
                    logger.info(f"   - ç­›é€‰æ¡ä»¶: æœ€å°ç‚¹èµ{task.min_likes}, æœ€å°è¯„è®º{task.min_comments}, æœ€å°è½¬å‘{task.min_retweets}")
                    
                    # ä½¿ç”¨å¸¦ç­›é€‰æ¡ä»¶çš„æŠ“å–æ–¹æ³•
                    tweets = await parser.scrape_tweets(max_tweets=task.max_tweets, filter_criteria=filter_criteria)
                    logger.info(f"   - æŠ“å–åˆ°æ»¡è¶³æ¡ä»¶çš„æ¨æ–‡æ•°: {len(tweets)}")
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æ•°é‡
                    if len(tweets) < task.max_tweets:
                        shortage_count = task.max_tweets - len(tweets)
                        shortage_info = f"åšä¸» @{clean_username}: ç›®æ ‡{task.max_tweets}æ¡ï¼Œå®é™…{len(tweets)}æ¡ï¼Œä¸è¶³{shortage_count}æ¡"
                        content_shortage_details.append(shortage_info)
                        logger.warning(f"âš ï¸ åšä¸» @{clean_username} æ»¡è¶³æ¡ä»¶çš„æ¨æ–‡ä¸è¶³ï¼š")
                        logger.warning(f"   - ç›®æ ‡æ•°é‡: {task.max_tweets} æ¡")
                        logger.warning(f"   - å®é™…æ»¡è¶³æ¡ä»¶æ•°é‡: {len(tweets)} æ¡")
                        logger.warning(f"   - ä¸è¶³ {shortage_count} æ¡")
                        logger.info(f"ğŸ“Š å°†ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ {len(tweets)} æ¡æ»¡è¶³æ¡ä»¶çš„æ¨æ–‡")
                    
                    # ç›´æ¥æ·»åŠ åˆ°ç»“æœä¸­ï¼ˆå·²ç»è¿‡ç­›é€‰ï¼‰
                    all_tweets.extend(tweets)
                    logger.info(f"âœ… åšä¸» @{clean_username} æŠ“å–å®Œæˆ:")
                    logger.info(f"   - æ»¡è¶³æ¡ä»¶æ¨æ–‡æ•°: {len(tweets)}")
                    logger.info(f"   - ç´¯è®¡æœ‰æ•ˆæ¨æ–‡æ•°: {len(all_tweets)}")
                    
                    # å¦‚æœæ»¡è¶³æ¡ä»¶çš„æ¨æ–‡ä¸è¶³ï¼Œè®°å½•åˆ°ä»»åŠ¡ç»“æœä¸­
                    if len(tweets) < task.max_tweets:
                        logger.info(f"ğŸ“ è®°å½•æ»¡è¶³æ¡ä»¶æ¨æ–‡ä¸è¶³ä¿¡æ¯åˆ°ä»»åŠ¡ç»“æœ")
                    
                except Exception as e:
                    logger.error(f"âŒ æŠ“å–åšä¸» @{clean_username} å¤±è´¥: {e}")
                    continue
            
            # æŠ“å–å…³é”®è¯æ¨æ–‡
            if target_keywords:
                logger.info(f"ğŸ” æ­¥éª¤7: å¼€å§‹å…³é”®è¯æœç´¢")
                for j, keyword in enumerate(target_keywords, 1):
                    try:
                        logger.info(f"ğŸ” æ­¥éª¤7.{j}: æœç´¢å…³é”®è¯ '{keyword}'")
                        logger.info(f"   - è¿›åº¦: {j}/{len(target_keywords)}")
                        logger.info(f"   - ç›®æ ‡æ¨æ–‡æ•°: {task.max_tweets}")
                        
                        await parser.search_tweets(keyword)
                        logger.info(f"   - å·²å¯¼èˆªåˆ°æœç´¢é¡µé¢")
                        
                        # æ„å»ºç­›é€‰æ¡ä»¶
                        filter_criteria = {
                            'min_likes': task.min_likes,
                            'min_comments': task.min_comments,
                            'min_retweets': task.min_retweets
                        }
                        logger.info(f"   - ç­›é€‰æ¡ä»¶: æœ€å°ç‚¹èµ{task.min_likes}, æœ€å°è¯„è®º{task.min_comments}, æœ€å°è½¬å‘{task.min_retweets}")
                        
                        # ä½¿ç”¨å¸¦ç­›é€‰æ¡ä»¶çš„æŠ“å–æ–¹æ³•
                        tweets = await parser.scrape_tweets(max_tweets=task.max_tweets, filter_criteria=filter_criteria)
                        logger.info(f"   - æŠ“å–åˆ°æ»¡è¶³æ¡ä»¶çš„æ¨æ–‡æ•°: {len(tweets)}")
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æ•°é‡
                        if len(tweets) < task.max_tweets:
                            shortage_count = task.max_tweets - len(tweets)
                            shortage_info = f"å…³é”®è¯ '{keyword}': ç›®æ ‡{task.max_tweets}æ¡ï¼Œå®é™…{len(tweets)}æ¡ï¼Œä¸è¶³{shortage_count}æ¡"
                            content_shortage_details.append(shortage_info)
                            logger.warning(f"âš ï¸ å…³é”®è¯ '{keyword}' æ»¡è¶³æ¡ä»¶çš„æ¨æ–‡ä¸è¶³ï¼š")
                            logger.warning(f"   - ç›®æ ‡æ•°é‡: {task.max_tweets} æ¡")
                            logger.warning(f"   - å®é™…æ»¡è¶³æ¡ä»¶æ•°é‡: {len(tweets)} æ¡")
                            logger.warning(f"   - ä¸è¶³ {shortage_count} æ¡")
                            logger.info(f"ğŸ“Š å°†ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ {len(tweets)} æ¡æ»¡è¶³æ¡ä»¶çš„æ¨æ–‡")
                        
                        # ç›´æ¥æ·»åŠ åˆ°ç»“æœä¸­ï¼ˆå·²ç»è¿‡ç­›é€‰ï¼‰
                        all_tweets.extend(tweets)
                        logger.info(f"âœ… å…³é”®è¯ '{keyword}' æœç´¢å®Œæˆ:")
                        logger.info(f"   - æ»¡è¶³æ¡ä»¶æ¨æ–‡æ•°: {len(tweets)}")
                        logger.info(f"   - ç´¯è®¡æœ‰æ•ˆæ¨æ–‡æ•°: {len(all_tweets)}")
                        
                    except Exception as e:
                        logger.error(f"âŒ æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                        continue
            else:
                logger.info(f"â„¹ï¸ è·³è¿‡å…³é”®è¯æœç´¢ï¼šæœªé…ç½®å…³é”®è¯")
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            logger.info(f"ğŸ’¾ æ­¥éª¤8: ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“")
            logger.info(f"   - æ€»è®¡æŠ“å–æ¨æ–‡æ•°: {len(all_tweets)}")
            logger.info(f"   - å¼€å§‹å»é‡å’Œä¿å­˜")
            
            saved_count = 0
            duplicate_count = 0
            error_count = 0
            
            for i, tweet in enumerate(all_tweets, 1):
                try:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = db.session.query(TweetData).filter_by(
                        username=tweet.get('username', ''),
                        content=tweet.get('content', '')[:500]  # æˆªå–å‰500å­—ç¬¦æ¯”è¾ƒ
                    ).first()
                    
                    if not existing:
                        tweet_data = TweetData(
                            task_id=task_id,
                            username=tweet.get('username', ''),
                            content=tweet.get('content', ''),
                            likes=tweet.get('likes', 0),
                            comments=tweet.get('comments', 0),
                            retweets=tweet.get('retweets', 0),
                            publish_time=tweet.get('publish_time', ''),
                            link=tweet.get('link', ''),
                            hashtags=json.dumps(tweet.get('hashtags', [])) if tweet.get('hashtags') else None
                        )
                        db.session.add(tweet_data)
                        saved_count += 1
                        if i % 10 == 0 or i == len(all_tweets):
                            logger.info(f"   - å¤„ç†è¿›åº¦: {i}/{len(all_tweets)}, å·²ä¿å­˜: {saved_count}, é‡å¤: {duplicate_count}")
                    else:
                        duplicate_count += 1
                        
                except Exception as e:
                    error_count += 1
                    logger.error(f"âŒ ä¿å­˜æ¨æ–‡å¤±è´¥ (ç¬¬{i}æ¡): {e}")
                    continue
            
            db.session.commit()
            logger.info(f"âœ… æ•°æ®åº“ä¿å­˜å®Œæˆ:")
            logger.info(f"   - æ€»å¤„ç†æ¨æ–‡æ•°: {len(all_tweets)}")
            logger.info(f"   - æˆåŠŸä¿å­˜æ•°: {saved_count}")
            logger.info(f"   - é‡å¤è·³è¿‡æ•°: {duplicate_count}")
            logger.info(f"   - ä¿å­˜å¤±è´¥æ•°: {error_count}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            logger.info(f"ğŸ“ æ­¥éª¤9: æ›´æ–°ä»»åŠ¡çŠ¶æ€")
            task.status = 'completed'
            task.completed_at = datetime.utcnow()
            task.result_count = saved_count
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹ä¸è¶³çš„æƒ…å†µï¼Œå¹¶æ·»åŠ åˆ°ä»»åŠ¡å¤‡æ³¨ä¸­
            if content_shortage_details:
                # å°†è¯¦ç»†çš„å†…å®¹ä¸è¶³ä¿¡æ¯æ·»åŠ åˆ°ä»»åŠ¡å¤‡æ³¨
                if hasattr(task, 'notes'):
                    existing_notes = task.notes or ''
                    shortage_note = f"[å†…å®¹ä¸è¶³æé†’] {'; '.join(content_shortage_details)}"
                    task.notes = f"{existing_notes}\n{shortage_note}" if existing_notes else shortage_note
                
                logger.warning(f"âš ï¸ ä»»åŠ¡å®Œæˆä½†éƒ¨åˆ†å†…å®¹ä¸è¶³ï¼š")
                for detail in content_shortage_details:
                    logger.warning(f"   - {detail}")
            else:
                logger.info(f"âœ… æ‰€æœ‰å†…å®¹æŠ“å–å……è¶³ï¼Œæ— å†…å®¹ä¸è¶³æƒ…å†µ")
            
            db.session.commit()
            logger.info(f"âœ… ä»»åŠ¡çŠ¶æ€æ›´æ–°å®Œæˆ:")
            logger.info(f"   - çŠ¶æ€: {task.status}")
            logger.info(f"   - å®Œæˆæ—¶é—´: {task.completed_at}")
            logger.info(f"   - ç»“æœæ•°é‡: {task.result_count}")
            if content_shortage_details:
                logger.info(f"   - å†…å®¹ä¸è¶³è¯¦æƒ…: {len(content_shortage_details)} é¡¹")
                for detail in content_shortage_details:
                    logger.info(f"     * {detail}")
            else:
                logger.info(f"   - å†…å®¹æŠ“å–çŠ¶æ€: å……è¶³")
            
            # å¯¼å‡ºExcel
            logger.info(f"ğŸ“Š æ­¥éª¤10: å¯¼å‡ºExcelæ–‡ä»¶")
            excel_writer = ExcelWriter()
            output_file = os.path.join("data", f"tweets_{task_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")
            logger.info(f"   - å¯¼å‡ºæ–‡ä»¶è·¯å¾„: {output_file}")
            logger.info(f"   - å¯¼å‡ºæ¨æ–‡æ•°é‡: {len(all_tweets)}")
            
            excel_writer.write_tweets_to_excel(all_tweets, output_file)
            logger.info(f"âœ… Excelæ–‡ä»¶å¯¼å‡ºå®Œæˆ: {output_file}")
            
            # åŒæ­¥åˆ°äº‘ç«¯ï¼ˆé£ä¹¦ï¼‰
            logger.info(f"â˜ï¸ æ­¥éª¤11: åŒæ­¥æ•°æ®åˆ°é£ä¹¦")
            try:
                # æ£€æŸ¥é£ä¹¦é…ç½®
                from web_app import FEISHU_CONFIG
                logger.info(f"   - æ£€æŸ¥é£ä¹¦é…ç½®çŠ¶æ€")
                logger.info(f"   - é£ä¹¦åŒæ­¥å¯ç”¨: {FEISHU_CONFIG.get('enabled', False)}")
                
                if not FEISHU_CONFIG.get('enabled'):
                    logger.info(f"â„¹ï¸ é£ä¹¦åŒæ­¥æœªå¯ç”¨ï¼Œè·³è¿‡åŒæ­¥")
                else:
                    # æ£€æŸ¥å¿…è¦çš„é£ä¹¦é…ç½®å‚æ•°
                    spreadsheet_token = FEISHU_CONFIG.get('spreadsheet_token')
                    table_id = FEISHU_CONFIG.get('table_id')
                    
                    if not spreadsheet_token:
                        logger.warning(f"âš ï¸ é£ä¹¦è¡¨æ ¼Tokenæœªé…ç½®ï¼Œè·³è¿‡åŒæ­¥")
                    elif not table_id:
                        logger.warning(f"âš ï¸ é£ä¹¦è¡¨æ ¼IDæœªé…ç½®ï¼Œè·³è¿‡åŒæ­¥")
                    else:
                        logger.info(f"   - å¼€å§‹é£ä¹¦åŒæ­¥ï¼Œæ¨æ–‡æ•°é‡: {len(all_tweets)}")
                        logger.info(f"   - è¡¨æ ¼Token: {spreadsheet_token[:10]}...")
                        logger.info(f"   - è¡¨æ ¼ID: {table_id}")
                        logger.info(f"   - æœ€å¤§é‡è¯•æ¬¡æ•°: 3")
                        logger.info(f"   - å¤±è´¥æ—¶ç»§ç»­æ‰§è¡Œ: True")
                        
                        # æ„å»ºé£ä¹¦é…ç½®
                        feishu_sync_config = {
                            'feishu': {
                                'app_id': FEISHU_CONFIG.get('app_id'),
                                'app_secret': FEISHU_CONFIG.get('app_secret'),
                                'base_url': 'https://open.feishu.cn/open-apis'
                            }
                        }
                        
                        cloud_sync = CloudSyncManager(feishu_sync_config)
                        sync_result = cloud_sync.sync_to_feishu(
                            all_tweets, 
                            spreadsheet_token=spreadsheet_token,
                            table_id=table_id,
                            max_retries=3, 
                            continue_on_failure=True
                        )
                        
                        if sync_result:
                            logger.info(f"âœ… é£ä¹¦åŒæ­¥å®Œæˆ")
                            logger.info(f"   - åŒæ­¥çŠ¶æ€: æˆåŠŸ")
                            logger.info(f"   - åŒæ­¥æ¨æ–‡æ•°: {len(all_tweets)}")
                            
                            # æ›´æ–°æ•°æ®åº“ä¸­çš„åŒæ­¥çŠ¶æ€
                            try:
                                synced_tweets = db.session.query(TweetData).filter_by(task_id=task_id).all()
                                for tweet_data in synced_tweets:
                                    tweet_data.synced_to_feishu = True
                                db.session.commit()
                                logger.info(f"âœ… æ•°æ®åº“åŒæ­¥çŠ¶æ€æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {len(synced_tweets)} æ¡è®°å½•")
                            except Exception as update_e:
                                logger.warning(f"âš ï¸ æ›´æ–°æ•°æ®åº“åŒæ­¥çŠ¶æ€å¤±è´¥: {update_e}")
                        else:
                            logger.warning(f"âš ï¸ é£ä¹¦åŒæ­¥å¤±è´¥ï¼Œä½†ä»»åŠ¡ç»§ç»­æ‰§è¡Œ")
                        
            except Exception as e:
                logger.warning(f"âš ï¸ é£ä¹¦åŒæ­¥å¼‚å¸¸ï¼Œä½†ä»»åŠ¡ç»§ç»­æ‰§è¡Œ: {e}")
                logger.warning(f"   - å¼‚å¸¸ç±»å‹: {type(e).__name__}")
                logger.warning(f"   - å¼‚å¸¸è¯¦æƒ…: {str(e)}")
                import traceback
                logger.warning(f"   - å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            
            logger.info(f"ğŸ‰ æ­¥éª¤12: ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
            logger.info(f"âœ… ä»»åŠ¡æ‰§è¡Œæ€»ç»“:")
            logger.info(f"   - ä»»åŠ¡ID: {task_id}")
            logger.info(f"   - ä»»åŠ¡åç§°: {task.name}")
            logger.info(f"   - æŠ“å–æ¨æ–‡æ€»æ•°: {len(all_tweets)}")
            logger.info(f"   - ä¿å­˜æ¨æ–‡æ•°: {saved_count}")
            logger.info(f"   - å¯¼å‡ºæ–‡ä»¶: {output_file}")
            logger.info(f"   - æ‰§è¡Œæ—¶é•¿: {datetime.utcnow() - task.started_at}")
            logger.info(f"="*60)
            
            return output_file
            
        except Exception as e:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            task.status = 'failed'
            task.completed_at = datetime.utcnow()
            task.error_message = str(e)
            db.session.commit()
            raise
        
        finally:
            # å…³é—­æµè§ˆå™¨
            try:
                browser_manager.stop_browser(user_id)
            except:
                pass

async def run_background_task(config_file: str):
    """è¿è¡Œåå°ä»»åŠ¡"""
    logger = setup_logging()
    task_config = None
    
    try:
        # è¯»å–ä»»åŠ¡é…ç½®
        with open(config_file, 'r', encoding='utf-8') as f:
            task_config = json.load(f)
        
        task_id = task_config['task_id']
        user_id = task_config['kwargs']['user_id']
        
        logger.info(f"å¼€å§‹æ‰§è¡Œåå°ä»»åŠ¡ {task_id}ï¼Œç”¨æˆ·ID: {user_id}")
        
        # æ‰§è¡Œä»»åŠ¡
        result = await execute_scraping_task(task_id, user_id)
        
        logger.info(f"åå°ä»»åŠ¡ {task_id} æ‰§è¡Œå®Œæˆï¼Œç»“æœ: {result}")
        
        # ä¿å­˜ç»“æœ
        result_file = f"task_result_{task_id}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({
                'task_id': task_id,
                'success': True,
                'completed_at': datetime.now().isoformat(),
                'result_file': result
            }, f, ensure_ascii=False, indent=2)
        
        return 0
        
    except Exception as e:
        logger.error(f"åå°ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        task_id = task_config.get('task_id', 'unknown') if task_config else 'unknown'
        error_file = f"task_error_{task_id}.json"
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump({
                'task_id': task_id,
                'error': str(e),
                'failed_at': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        return 1
    
    finally:
        # æ¸…ç†é…ç½®æ–‡ä»¶
        try:
            if Path(config_file).exists():
                Path(config_file).unlink()
        except:
            pass

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python background_task_runner.py <config_file>")
        sys.exit(1)
    
    config_file = sys.argv[1]
    
    if not Path(config_file).exists():
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        sys.exit(1)
    
    # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
    try:
        exit_code = asyncio.run(run_background_task(config_file))
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()