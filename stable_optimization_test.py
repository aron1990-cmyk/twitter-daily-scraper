#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨³å®šçš„ä¼˜åŒ–æµ‹è¯•è„šæœ¬
åŒ…å«é”™è¯¯å¤„ç†å’Œé‡è¿æœºåˆ¶
"""

import asyncio
import logging
import sys
import os
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ads_browser_launcher import AdsPowerLauncher
from twitter_parser import TwitterParser
from tweet_filter import TweetFilter

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# AdsPoweré…ç½®
ADS_POWER_CONFIG = {
    'user_id': 'k11p9ypc',
    'open_tabs': 1,
    'launch_args': [],
    'headless': False,
    'disable_password_filling': False,
    'clear_cache_after_closing': False,
    'enable_password_saving': False
}

class StableOptimizationTester:
    """ç¨³å®šçš„ä¼˜åŒ–æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.launcher = None
        self.parser = None
        self.test_results = {
            'timestamp': datetime.now().isoformat(),
            'optimization_tests': [],
            'performance_metrics': {},
            'recommendations': []
        }
    
    async def initialize_browser(self, max_retries: int = 3) -> bool:
        """åˆå§‹åŒ–æµè§ˆå™¨ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                self.logger.info(f"ğŸš€ å¯åŠ¨æµè§ˆå™¨ (å°è¯• {attempt + 1}/{max_retries})...")
                
                # å¯åŠ¨æµè§ˆå™¨
                self.launcher = AdsPowerLauncher(ADS_POWER_CONFIG)
                browser_info = self.launcher.start_browser()
                self.launcher.wait_for_browser_ready()
                debug_port = self.launcher.get_debug_port()
                
                self.logger.info(f"âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼Œè°ƒè¯•ç«¯å£: {debug_port}")
                
                # åˆå§‹åŒ–TwitterParser
                self.parser = TwitterParser(debug_port=debug_port)
                await self.parser.initialize()
                
                self.logger.info("âœ… TwitterParseråˆå§‹åŒ–å®Œæˆ")
                return True
                
            except Exception as e:
                self.logger.warning(f"âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                await self.cleanup_resources()
                if attempt < max_retries - 1:
                    await asyncio.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
                
        return False
    
    async def cleanup_resources(self):
        """æ¸…ç†èµ„æº"""
        if self.parser:
            try:
                await self.parser.close()
                self.parser = None
            except Exception as e:
                self.logger.debug(f"å…³é—­parseræ—¶å‡ºé”™: {e}")
        
        if self.launcher:
            try:
                self.launcher.stop_browser()
                self.launcher = None
            except Exception as e:
                self.logger.debug(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}")
    
    def clean_tweet_content(self, content: str) -> str:
        """æ¸…ç†æ¨æ–‡å†…å®¹"""
        if not content:
            return ""
        
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content.strip())
        
        # å»é™¤é‡å¤çš„ç”¨æˆ·åæ¨¡å¼
        content = re.sub(r'(\w+\s+\w+)\s+\1', r'\1', content)
        
        # å»é™¤é‡å¤çš„æ•°å­—æ¨¡å¼
        content = re.sub(r'(\d+[,.]?\d*[KMB]?)\s+\1(\s+\1)*', r'\1', content)
        
        # å»é™¤é‡å¤çš„ç¬¦å·æ¨¡å¼
        content = re.sub(r'(Â·\s*)+', 'Â· ', content)
        
        # å»é™¤æœ«å°¾çš„ç»Ÿè®¡æ•°æ®æ¨¡å¼
        content = re.sub(r'\s*Â·\s*[\d,KMB.\s]+$', '', content)
        
        # å»é™¤å¼€å¤´çš„ç”¨æˆ·åé‡å¤
        content = re.sub(r'^(@?\w+\s+){2,}', '', content)
        
        # å»é™¤å¤šä½™çš„ç‚¹å’Œç©ºæ ¼
        content = re.sub(r'\s*Â·\s*$', '', content)
        
        return content.strip()
    
    async def test_scroll_optimization(self) -> Dict[str, Any]:
        """æµ‹è¯•æ»šåŠ¨ä¼˜åŒ–"""
        try:
            self.logger.info("ğŸ”„ å¼€å§‹æ»šåŠ¨ä¼˜åŒ–æµ‹è¯•...")
            
            # å¯¼èˆªåˆ°æµ‹è¯•é¡µé¢
            try:
                await self.parser.navigate_to_profile('elonmusk')
            except Exception:
                await self.parser.page.goto('https://x.com/elonmusk', wait_until='domcontentloaded')
            
            await asyncio.sleep(3)
            
            # è·å–åˆå§‹æ¨æ–‡æ•°é‡
            initial_tweets = await self.get_tweet_count()
            
            # æ‰§è¡Œä¼˜åŒ–æ»šåŠ¨
            scroll_results = await self.optimized_scroll_test(target_tweets=15)
            
            # è·å–æœ€ç»ˆæ¨æ–‡æ•°é‡
            final_tweets = await self.get_tweet_count()
            
            return {
                'test_name': 'scroll_optimization',
                'initial_tweets': initial_tweets,
                'final_tweets': final_tweets,
                'scroll_attempts': scroll_results.get('attempts', 0),
                'efficiency': final_tweets / max(scroll_results.get('attempts', 1), 1),
                'success': final_tweets >= 10,
                'details': scroll_results
            }
            
        except Exception as e:
            self.logger.error(f"æ»šåŠ¨ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
            return {
                'test_name': 'scroll_optimization',
                'success': False,
                'error': str(e)
            }
    
    async def test_content_extraction_optimization(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å®¹æå–ä¼˜åŒ–"""
        try:
            self.logger.info("ğŸ” å¼€å§‹å†…å®¹æå–ä¼˜åŒ–æµ‹è¯•...")
            
            # è·å–æ¨æ–‡å…ƒç´ 
            tweet_elements = await self.get_tweet_elements()
            
            if not tweet_elements:
                return {
                    'test_name': 'content_extraction_optimization',
                    'success': False,
                    'error': 'æœªæ‰¾åˆ°æ¨æ–‡å…ƒç´ '
                }
            
            # æµ‹è¯•å‰5æ¡æ¨æ–‡
            test_tweets = tweet_elements[:5]
            original_results = []
            optimized_results = []
            
            for i, element in enumerate(test_tweets):
                self.logger.info(f"æµ‹è¯•æ¨æ–‡ {i+1}/{len(test_tweets)}")
                
                # åŸå§‹è§£æ
                try:
                    original_tweet = await self.parser.parse_tweet_element(element)
                    original_results.append(original_tweet)
                except Exception as e:
                    self.logger.debug(f"åŸå§‹è§£æå¤±è´¥: {e}")
                    original_results.append(None)
                
                # ä¼˜åŒ–è§£æ
                try:
                    optimized_tweet = await self.optimized_parse_tweet(element)
                    optimized_results.append(optimized_tweet)
                except Exception as e:
                    self.logger.debug(f"ä¼˜åŒ–è§£æå¤±è´¥: {e}")
                    optimized_results.append(None)
            
            # åˆ†æç»“æœ
            original_success = len([r for r in original_results if r])
            optimized_success = len([r for r in optimized_results if r])
            
            # å†…å®¹è´¨é‡åˆ†æ
            content_improvements = 0
            for orig, opt in zip(original_results, optimized_results):
                if orig and opt:
                    orig_content = orig.get('content', '')
                    opt_content = opt.get('content', '')
                    if len(opt_content) > 0 and opt_content != orig_content:
                        content_improvements += 1
            
            return {
                'test_name': 'content_extraction_optimization',
                'total_tested': len(test_tweets),
                'original_success': original_success,
                'optimized_success': optimized_success,
                'content_improvements': content_improvements,
                'improvement_rate': content_improvements / max(len(test_tweets), 1),
                'success': optimized_success >= original_success,
                'sample_results': {
                    'original': original_results[:2],
                    'optimized': optimized_results[:2]
                }
            }
            
        except Exception as e:
            self.logger.error(f"å†…å®¹æå–ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
            return {
                'test_name': 'content_extraction_optimization',
                'success': False,
                'error': str(e)
            }
    
    async def get_tweet_count(self) -> int:
        """è·å–å½“å‰é¡µé¢æ¨æ–‡æ•°é‡"""
        try:
            await self.parser.page.wait_for_selector('[data-testid="tweet"]', timeout=5000)
            elements = await self.parser.page.query_selector_all('[data-testid="tweet"]')
            return len(elements)
        except Exception:
            return 0
    
    async def get_tweet_elements(self):
        """è·å–æ¨æ–‡å…ƒç´ """
        try:
            await self.parser.page.wait_for_selector('[data-testid="tweet"]', timeout=10000)
            return await self.parser.page.query_selector_all('[data-testid="tweet"]')
        except Exception:
            return []
    
    async def optimized_scroll_test(self, target_tweets: int = 15) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„æ»šåŠ¨æµ‹è¯•"""
        attempts = 0
        max_attempts = 10
        last_count = 0
        stagnant_rounds = 0
        
        while attempts < max_attempts:
            current_count = await self.get_tweet_count()
            
            if current_count >= target_tweets:
                break
            
            if current_count == last_count:
                stagnant_rounds += 1
            else:
                stagnant_rounds = 0
            
            last_count = current_count
            
            # æ‰§è¡Œæ»šåŠ¨
            try:
                scroll_distance = 1000 if stagnant_rounds < 3 else 2000
                await self.parser.page.evaluate(f'window.scrollBy(0, {scroll_distance})')
                await asyncio.sleep(2)
                attempts += 1
            except Exception as e:
                self.logger.debug(f"æ»šåŠ¨å¤±è´¥: {e}")
                break
            
            # å¦‚æœè¿ç»­å¤šè½®æ— å˜åŒ–ï¼Œå°è¯•åˆ·æ–°
            if stagnant_rounds >= 5:
                try:
                    await self.parser.page.reload(wait_until='domcontentloaded')
                    await asyncio.sleep(3)
                    stagnant_rounds = 0
                except Exception:
                    break
        
        final_count = await self.get_tweet_count()
        
        return {
            'attempts': attempts,
            'final_count': final_count,
            'target_reached': final_count >= target_tweets,
            'efficiency': final_count / max(attempts, 1)
        }
    
    async def optimized_parse_tweet(self, element) -> Optional[Dict[str, Any]]:
        """ä¼˜åŒ–çš„æ¨æ–‡è§£æ"""
        try:
            # æå–ç”¨æˆ·å
            username = 'unknown'
            try:
                username_element = await element.query_selector('[data-testid="User-Name"] [dir="ltr"]')
                if username_element:
                    username_text = await username_element.text_content()
                    username = re.sub(r'^@', '', username_text.strip().split()[0])
            except Exception:
                pass
            
            # æå–å†…å®¹
            content = 'No content available'
            try:
                content_element = await element.query_selector('[data-testid="tweetText"]')
                if content_element:
                    raw_content = await content_element.text_content()
                    content = self.clean_tweet_content(raw_content)
            except Exception:
                pass
            
            # æå–é“¾æ¥
            link = ''
            try:
                link_element = await element.query_selector('a[href*="/status/"]')
                if link_element:
                    href = await link_element.get_attribute('href')
                    if href:
                        link = f'https://x.com{href}' if href.startswith('/') else href
            except Exception:
                pass
            
            # æå–æ—¶é—´
            publish_time = ''
            try:
                time_element = await element.query_selector('time')
                if time_element:
                    datetime_attr = await time_element.get_attribute('datetime')
                    if datetime_attr:
                        publish_time = datetime_attr
            except Exception:
                pass
            
            # æ„å»ºæ¨æ–‡æ•°æ®
            tweet_data = {
                'username': username,
                'content': content,
                'publish_time': publish_time,
                'link': link,
                'likes': 0,
                'comments': 0,
                'retweets': 0
            }
            
            # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
            if username != 'unknown' and content != 'No content available':
                return tweet_data
            
            return None
            
        except Exception as e:
            self.logger.debug(f"ä¼˜åŒ–è§£æå¤±è´¥: {e}")
            return None
    
    async def run_comprehensive_test(self) -> bool:
        """è¿è¡Œç»¼åˆä¼˜åŒ–æµ‹è¯•"""
        try:
            # åˆå§‹åŒ–æµè§ˆå™¨
            if not await self.initialize_browser():
                self.logger.error("âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
                return False
            
            # è¿è¡Œæ»šåŠ¨ä¼˜åŒ–æµ‹è¯•
            scroll_test = await self.test_scroll_optimization()
            self.test_results['optimization_tests'].append(scroll_test)
            
            # è¿è¡Œå†…å®¹æå–ä¼˜åŒ–æµ‹è¯•
            content_test = await self.test_content_extraction_optimization()
            self.test_results['optimization_tests'].append(content_test)
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            self.calculate_performance_metrics()
            
            # ç”Ÿæˆå»ºè®®
            self.generate_recommendations()
            
            # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
            report_file = f"optimization_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ ä¼˜åŒ–æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
            # è¾“å‡ºæµ‹è¯•ç»“æœ
            self.print_test_summary()
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç»¼åˆä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        finally:
            await self.cleanup_resources()
    
    def calculate_performance_metrics(self):
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        tests = self.test_results['optimization_tests']
        
        # æ»šåŠ¨æ•ˆç‡
        scroll_test = next((t for t in tests if t['test_name'] == 'scroll_optimization'), {})
        scroll_efficiency = scroll_test.get('efficiency', 0)
        
        # å†…å®¹æå–æ”¹è¿›ç‡
        content_test = next((t for t in tests if t['test_name'] == 'content_extraction_optimization'), {})
        improvement_rate = content_test.get('improvement_rate', 0)
        
        # æ€»ä½“æˆåŠŸç‡
        successful_tests = len([t for t in tests if t.get('success', False)])
        overall_success_rate = successful_tests / max(len(tests), 1)
        
        self.test_results['performance_metrics'] = {
            'scroll_efficiency': scroll_efficiency,
            'content_improvement_rate': improvement_rate,
            'overall_success_rate': overall_success_rate,
            'optimization_score': (scroll_efficiency + improvement_rate + overall_success_rate) / 3
        }
    
    def generate_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        metrics = self.test_results['performance_metrics']
        recommendations = []
        
        if metrics.get('scroll_efficiency', 0) < 0.8:
            recommendations.append("å»ºè®®ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥ï¼Œå¢åŠ æ™ºèƒ½åœæ»æ£€æµ‹å’Œè‡ªé€‚åº”æ»šåŠ¨è·ç¦»")
        
        if metrics.get('content_improvement_rate', 0) < 0.5:
            recommendations.append("å»ºè®®æ”¹è¿›å†…å®¹æå–ç®—æ³•ï¼Œå¢å¼ºæ–‡æœ¬æ¸…ç†å’Œå»é‡åŠŸèƒ½")
        
        if metrics.get('overall_success_rate', 0) < 0.8:
            recommendations.append("å»ºè®®å¢å¼ºé”™è¯¯å¤„ç†æœºåˆ¶ï¼Œæé«˜ç³»ç»Ÿç¨³å®šæ€§")
        
        if metrics.get('optimization_score', 0) >= 0.8:
            recommendations.append("ç³»ç»Ÿä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼Œå¯è€ƒè™‘éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")
        
        self.test_results['recommendations'] = recommendations
    
    def print_test_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        self.logger.info("\nğŸ“Š ä¼˜åŒ–æµ‹è¯•ç»“æœæ‘˜è¦:")
        
        for test in self.test_results['optimization_tests']:
            test_name = test.get('test_name', 'Unknown')
            success = test.get('success', False)
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            self.logger.info(f"  {test_name}: {status}")
        
        metrics = self.test_results['performance_metrics']
        self.logger.info(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        self.logger.info(f"  æ»šåŠ¨æ•ˆç‡: {metrics.get('scroll_efficiency', 0):.2f}")
        self.logger.info(f"  å†…å®¹æ”¹è¿›ç‡: {metrics.get('content_improvement_rate', 0):.1%}")
        self.logger.info(f"  æ€»ä½“æˆåŠŸç‡: {metrics.get('overall_success_rate', 0):.1%}")
        self.logger.info(f"  ä¼˜åŒ–åˆ†æ•°: {metrics.get('optimization_score', 0):.2f}")
        
        self.logger.info(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for rec in self.test_results['recommendations']:
            self.logger.info(f"  â€¢ {rec}")

async def main():
    """ä¸»å‡½æ•°"""
    tester = StableOptimizationTester()
    success = await tester.run_comprehensive_test()
    
    if success:
        logger.info("\nğŸŠ ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
        return True
    else:
        logger.error("\nğŸ’¥ ä¼˜åŒ–æµ‹è¯•å¤±è´¥")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)