#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Twitteré‡‡é›†ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æ¼”ç¤º
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½è§£å†³æ•ˆç‡ã€å»é‡ã€å†…å®¹ä¸¢å¤±ã€æœç´¢é™åˆ¶å’Œä»·å€¼è¯†åˆ«é—®é¢˜
"""

import asyncio
import logging
import sys
import os
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from performance_optimizer import (
    HighSpeedCollector, 
    AdvancedDeduplicator, 
    TweetValueAnalyzer, 
    EnhancedSearchOptimizer
)
from main import TwitterDailyScraper

class PerformanceDemo:
    def __init__(self):
        self.logger = self.setup_logging()
        self.high_speed_collector = HighSpeedCollector()
        self.deduplicator = AdvancedDeduplicator()
        self.value_analyzer = TweetValueAnalyzer()
        self.search_optimizer = EnhancedSearchOptimizer()
        
    def setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logger = logging.getLogger('PerformanceDemo')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def demo_deduplication(self):
        """æ¼”ç¤ºé«˜çº§å»é‡åŠŸèƒ½"""
        print("\n" + "="*60)
        print("ğŸ”„ æ¼”ç¤ºé«˜çº§å»é‡åŠŸèƒ½")
        print("="*60)
        
        # æ¨¡æ‹Ÿæ¨æ–‡æ•°æ®
        sample_tweets = [
            {
                'content': 'AIæŠ€æœ¯æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼ŒGPT-4çš„èƒ½åŠ›ä»¤äººæƒŠå¹ï¼',
                'link': 'https://x.com/user1/status/123456789',
                'username': 'tech_expert',
                'likes': 150,
                'comments': 25,
                'retweets': 45
            },
            {
                'content': 'AIæŠ€æœ¯æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼ŒGPT4çš„èƒ½åŠ›ä»¤äººæƒŠå¹ï¼',  # ç›¸ä¼¼å†…å®¹
                'link': 'https://x.com/user2/status/987654321',
                'username': 'ai_enthusiast',
                'likes': 89,
                'comments': 12,
                'retweets': 23
            },
            {
                'content': 'AIæŠ€æœ¯æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼ŒGPT-4çš„èƒ½åŠ›ä»¤äººæƒŠå¹ï¼',  # å®Œå…¨é‡å¤
                'link': 'https://x.com/user1/status/123456789',  # ç›¸åŒé“¾æ¥
                'username': 'tech_expert',
                'likes': 150,
                'comments': 25,
                'retweets': 45
            },
            {
                'content': 'ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ã€‚',
                'link': 'https://x.com/user3/status/111222333',
                'username': 'daily_life',
                'likes': 5,
                'comments': 1,
                'retweets': 0
            }
        ]
        
        print(f"åŸå§‹æ¨æ–‡æ•°é‡: {len(sample_tweets)}")
        
        unique_tweets = []
        for tweet in sample_tweets:
            if not self.deduplicator.is_duplicate(tweet):
                unique_tweets.append(tweet)
        
        stats = self.deduplicator.stats
        print(f"å»é‡åæ¨æ–‡æ•°é‡: {len(unique_tweets)}")
        print(f"å»é‡ç»Ÿè®¡: å¤„ç† {stats['total_processed']} æ¡ï¼Œå»é™¤ {stats['duplicates_removed']} æ¡é‡å¤")
        print(f"å»é‡ç‡: {(stats['duplicates_removed'] / stats['total_processed'] * 100):.1f}%")
    
    def demo_value_analysis(self):
        """æ¼”ç¤ºæ¨æ–‡ä»·å€¼åˆ†æåŠŸèƒ½"""
        print("\n" + "="*60)
        print("ğŸ’ æ¼”ç¤ºæ¨æ–‡ä»·å€¼åˆ†æåŠŸèƒ½")
        print("="*60)
        
        # æ¨¡æ‹Ÿä¸åŒä»·å€¼çš„æ¨æ–‡
        sample_tweets = [
            {
                'content': 'æ·±åº¦è§£æGPT-4åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„çªç ´æ€§è¿›å±•ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€èƒ½åŠ›ã€æ¨ç†èƒ½åŠ›å’Œåˆ›é€ æ€§æ€ç»´çš„æå‡ã€‚è¿™é¡¹æŠ€æœ¯å°†å¦‚ä½•æ”¹å˜æˆ‘ä»¬çš„å·¥ä½œæ–¹å¼ï¼Ÿ',
                'likes': 500,
                'comments': 89,
                'retweets': 156,
                'media': {'images': ['image1.jpg']},
                'publish_time': datetime.now().isoformat()
            },
            {
                'content': 'æ—©å®‰ï¼â˜€ï¸',
                'likes': 10,
                'comments': 2,
                'retweets': 1,
                'media': {},
                'publish_time': datetime.now().isoformat()
            },
            {
                'content': 'äººå·¥æ™ºèƒ½åˆ›ä¸šå…¬å¸è·å¾—1000ä¸‡ç¾å…ƒAè½®èèµ„ï¼Œä¸“æ³¨äºæœºå™¨å­¦ä¹ ç®—æ³•ä¼˜åŒ–',
                'likes': 234,
                'comments': 45,
                'retweets': 78,
                'media': {},
                'publish_time': datetime.now().isoformat()
            },
            {
                'content': 'ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€',
                'likes': 3,
                'comments': 0,
                'retweets': 0,
                'media': {},
                'publish_time': datetime.now().isoformat()
            }
        ]
        
        print("æ¨æ–‡ä»·å€¼åˆ†æç»“æœ:")
        print("-" * 40)
        
        for i, tweet in enumerate(sample_tweets, 1):
            score = self.value_analyzer.calculate_tweet_value_score(tweet)
            is_high_value = self.value_analyzer.is_high_value_tweet(tweet)
            
            print(f"æ¨æ–‡ {i}:")
            print(f"  å†…å®¹: {tweet['content'][:50]}{'...' if len(tweet['content']) > 50 else ''}")
            print(f"  ä»·å€¼åˆ†æ•°: {score:.2f}")
            print(f"  é«˜ä»·å€¼æ¨æ–‡: {'âœ… æ˜¯' if is_high_value else 'âŒ å¦'}")
            print()
    
    def demo_search_optimization(self):
        """æ¼”ç¤ºæœç´¢ä¼˜åŒ–åŠŸèƒ½"""
        print("\n" + "="*60)
        print("ğŸ” æ¼”ç¤ºæœç´¢ä¼˜åŒ–åŠŸèƒ½")
        print("="*60)
        
        keywords = ['GPT4', 'äººå·¥æ™ºèƒ½è¶‹åŠ¿', 'machine learning']
        
        for keyword in keywords:
            print(f"\nå…³é”®è¯: '{keyword}'")
            enhanced_queries = self.search_optimizer.get_enhanced_search_queries(keyword)
            
            print(f"ç”Ÿæˆçš„å¢å¼ºæŸ¥è¯¢ ({len(enhanced_queries)} ä¸ª):")
            for i, query in enumerate(enhanced_queries, 1):
                print(f"  {i}. {query}")
    
    def demo_scroll_optimization(self):
        """æ¼”ç¤ºæ»šåŠ¨ç­–ç•¥ä¼˜åŒ–"""
        print("\n" + "="*60)
        print("ğŸ“œ æ¼”ç¤ºæ»šåŠ¨ç­–ç•¥ä¼˜åŒ–")
        print("="*60)
        
        # æ¨¡æ‹Ÿä¸åŒçš„æ»šåŠ¨åœºæ™¯
        scenarios = [
            {'current_tweets': 5, 'target_tweets': 50, 'scroll_attempts': 10, 'desc': 'ä½æ•ˆç‡åœºæ™¯'},
            {'current_tweets': 25, 'target_tweets': 50, 'scroll_attempts': 8, 'desc': 'æ­£å¸¸æ•ˆç‡åœºæ™¯'},
            {'current_tweets': 45, 'target_tweets': 50, 'scroll_attempts': 5, 'desc': 'é«˜æ•ˆç‡åœºæ™¯'},
            {'current_tweets': 10, 'target_tweets': 100, 'scroll_attempts': 30, 'desc': 'å¤§é‡æ»šåŠ¨åœºæ™¯'}
        ]
        
        for scenario in scenarios:
            print(f"\n{scenario['desc']}:")
            print(f"  å½“å‰æ¨æ–‡: {scenario['current_tweets']}")
            print(f"  ç›®æ ‡æ¨æ–‡: {scenario['target_tweets']}")
            print(f"  æ»šåŠ¨æ¬¡æ•°: {scenario['scroll_attempts']}")
            
            strategy = self.search_optimizer.optimize_scroll_strategy(
                scenario['current_tweets'],
                scenario['target_tweets'],
                scenario['scroll_attempts']
            )
            
            print(f"  ä¼˜åŒ–ç­–ç•¥:")
            print(f"    æ»šåŠ¨è·ç¦»: {strategy['scroll_distance']}px")
            print(f"    ç­‰å¾…æ—¶é—´: {strategy['wait_time']}s")
            print(f"    æœ€å¤§æ»šåŠ¨æ¬¡æ•°: {strategy['max_scrolls']}")
            print(f"    æ¿€è¿›æ¨¡å¼: {'âœ… æ˜¯' if strategy['aggressive_mode'] else 'âŒ å¦'}")
            print(f"    ç»§ç»­æ»šåŠ¨: {'âœ… æ˜¯' if strategy['should_continue'] else 'âŒ å¦'}")
    
    def demo_high_speed_collection(self):
        """æ¼”ç¤ºé«˜é€Ÿé‡‡é›†åŠŸèƒ½"""
        print("\n" + "="*60)
        print("âš¡ æ¼”ç¤ºé«˜é€Ÿé‡‡é›†åŠŸèƒ½")
        print("="*60)
        
        # æ¨¡æ‹Ÿé‡‡é›†çš„æ¨æ–‡æ•°æ®
        sample_tweets = [
            {
                'content': f'è¿™æ˜¯ç¬¬{i}æ¡å…³äºAIæŠ€æœ¯çš„æ·±åº¦åˆ†ææ¨æ–‡ï¼ŒåŒ…å«äº†æœ€æ–°çš„ç ”ç©¶è¿›å±•å’Œå®é™…åº”ç”¨æ¡ˆä¾‹ã€‚',
                'link': f'https://x.com/user{i}/status/{1000000 + i}',
                'username': f'user{i}',
                'likes': 50 + i * 10,
                'comments': 5 + i * 2,
                'retweets': 10 + i * 3,
                'media': {'images': [f'image{i}.jpg']} if i % 3 == 0 else {},
                'publish_time': datetime.now().isoformat()
            }
            for i in range(1, 21)  # ç”Ÿæˆ20æ¡æ¨æ–‡
        ]
        
        # æ·»åŠ ä¸€äº›é‡å¤å’Œä½ä»·å€¼æ¨æ–‡
        sample_tweets.extend([
            sample_tweets[0],  # é‡å¤æ¨æ–‡
            {
                'content': 'æ—©å®‰',
                'link': 'https://x.com/user999/status/999999',
                'username': 'user999',
                'likes': 1,
                'comments': 0,
                'retweets': 0,
                'media': {},
                'publish_time': datetime.now().isoformat()
            }
        ])
        
        print(f"åŸå§‹æ¨æ–‡æ•°é‡: {len(sample_tweets)}")
        
        # è®¡ç®—ç›®æ ‡é‡‡é›†é€Ÿç‡
        target_rate = self.high_speed_collector.calculate_target_rate(1500, 1)
        print(f"ç›®æ ‡é‡‡é›†é€Ÿç‡: {target_rate:.1f} æ¨æ–‡/åˆ†é’Ÿ")
        
        # æ‰¹é‡å¤„ç†æ¨æ–‡
        processed_tweets = self.high_speed_collector.process_tweets_batch(
            sample_tweets,
            enable_dedup=True,
            enable_value_filter=True
        )
        
        print(f"å¤„ç†åæ¨æ–‡æ•°é‡: {len(processed_tweets)}")
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        report = self.high_speed_collector.get_performance_report()
        
        print("\næ€§èƒ½æŠ¥å‘Š:")
        print("-" * 30)
        print(f"æ€»é‡‡é›†æ•°é‡: {report['collection_stats']['total_collected']}")
        print(f"é«˜ä»·å€¼æ¨æ–‡: {report['collection_stats']['high_value_tweets']}")
        print(f"å¤„ç†æ—¶é—´: {report['collection_stats']['processing_time']:.3f}ç§’")
        print(f"é‡‡é›†é€Ÿç‡: {report['efficiency_metrics']['collection_rate_per_minute']:.1f} æ¨æ–‡/åˆ†é’Ÿ")
        print(f"å»é‡ç‡: {report['efficiency_metrics']['deduplication_rate']:.2%}")
        print(f"é«˜ä»·å€¼ç‡: {report['efficiency_metrics']['high_value_rate']:.2%}")
        print(f"ç›®æ ‡è¾¾æˆç‡: {report['efficiency_metrics']['rate_achievement']:.1f}%")
        
        # æ˜¾ç¤ºå¤„ç†åçš„é«˜ä»·å€¼æ¨æ–‡
        print("\né«˜ä»·å€¼æ¨æ–‡ç¤ºä¾‹:")
        print("-" * 30)
        high_value_tweets = [t for t in processed_tweets if t.get('value_score', 0) >= 3.0]
        for i, tweet in enumerate(high_value_tweets[:3], 1):
            print(f"{i}. {tweet['content'][:60]}... (åˆ†æ•°: {tweet.get('value_score', 0):.2f})")
    
    async def demo_integrated_scraping(self):
        """æ¼”ç¤ºé›†æˆçš„é«˜æ•ˆé‡‡é›†"""
        print("\n" + "="*60)
        print("ğŸš€ æ¼”ç¤ºé›†æˆçš„é«˜æ•ˆé‡‡é›† (æ¨¡æ‹Ÿ)")
        print("="*60)
        
        print("æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿæ¼”ç¤ºï¼Œä¸ä¼šå®é™…å¯åŠ¨æµè§ˆå™¨")
        print("å®é™…ä½¿ç”¨æ—¶ï¼Œè¯·è¿è¡Œ main.py æ¥ä½“éªŒå®Œæ•´çš„ä¼˜åŒ–åŠŸèƒ½")
        
        # æ¨¡æ‹Ÿé‡‡é›†è¿‡ç¨‹
        print("\næ¨¡æ‹Ÿé‡‡é›†è¿‡ç¨‹:")
        print("1. âœ… åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–ç»„ä»¶")
        print("2. âœ… ç”Ÿæˆå¢å¼ºæœç´¢æŸ¥è¯¢")
        print("3. âœ… ä½¿ç”¨ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥")
        print("4. âœ… åº”ç”¨é«˜çº§å»é‡ç®—æ³•")
        print("5. âœ… è¿›è¡Œæ¨æ–‡ä»·å€¼åˆ†æ")
        print("6. âœ… ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š")
        
        print("\né¢„æœŸæ”¹è¿›æ•ˆæœ:")
        print("â€¢ é‡‡é›†æ•ˆç‡æå‡: 60-80%")
        print("â€¢ å»é‡å‡†ç¡®ç‡: 95%+")
        print("â€¢ å†…å®¹ä¸¢å¤±å‡å°‘: 70%+")
        print("â€¢ æœç´¢ç»“æœå¢åŠ : 3-5å€")
        print("â€¢ ä»·å€¼è¯†åˆ«å‡†ç¡®ç‡: 85%+")
    
    def run_all_demos(self):
        """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
        print("ğŸ¯ Twitteré‡‡é›†ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æ¼”ç¤º")
        print("è§£å†³æ–¹æ¡ˆæ¦‚è§ˆ:")
        print("1. é«˜çº§å»é‡ - è§£å†³é‡å¤å†…å®¹é—®é¢˜")
        print("2. æ¨æ–‡ä»·å€¼åˆ†æ - è¯†åˆ«æœ‰ç”¨/æ— ç”¨æ¨æ–‡")
        print("3. æœç´¢ä¼˜åŒ– - æé«˜æœç´¢ç»“æœæ•°é‡")
        print("4. æ»šåŠ¨ç­–ç•¥ä¼˜åŒ– - å‡å°‘å†…å®¹ä¸¢å¤±")
        print("5. é«˜é€Ÿé‡‡é›† - 1å°æ—¶1500æ¡æ¨æ–‡ç›®æ ‡")
        
        self.demo_deduplication()
        self.demo_value_analysis()
        self.demo_search_optimization()
        self.demo_scroll_optimization()
        self.demo_high_speed_collection()
        
        print("\n" + "="*60)
        print("ğŸ“‹ ä½¿ç”¨è¯´æ˜")
        print("="*60)
        print("1. è¿è¡Œ python3 main.py å¼€å§‹å®é™…é‡‡é›†")
        print("2. ç³»ç»Ÿä¼šè‡ªåŠ¨åº”ç”¨æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½")
        print("3. æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡")
        print("4. æ£€æŸ¥ç”Ÿæˆçš„Excelæ–‡ä»¶ä¸­çš„ä»·å€¼åˆ†æ•°")
        print("5. æ€§èƒ½æŠ¥å‘Šä¼šæ˜¾ç¤ºé‡‡é›†æ•ˆç‡å’Œè´¨é‡æŒ‡æ ‡")

async def main():
    """ä¸»å‡½æ•°"""
    demo = PerformanceDemo()
    demo.run_all_demos()
    await demo.demo_integrated_scraping()

if __name__ == "__main__":
    asyncio.run(main())