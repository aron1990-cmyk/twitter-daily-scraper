#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤å¹¶è¡Œä»»åŠ¡å’ŒæŠ“å–é€»è¾‘é—®é¢˜
1. ç¡®ä¿å¤šä»»åŠ¡å¹¶è¡Œæ‰§è¡Œæ—¶èƒ½å¯åŠ¨å¤šä¸ªæµè§ˆå™¨å®ä¾‹
2. ä¿®å¤æ¨æ–‡æŠ“å–é€»è¾‘ä¸­çš„æ•°é‡åˆ¤æ–­é—®é¢˜
"""

import os
import sys
import json
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def fix_refactored_task_manager():
    """ä¿®å¤RefactoredTaskManagerçš„å¹¶è¡Œä»»åŠ¡ç®¡ç†"""
    logger.info("å¼€å§‹ä¿®å¤RefactoredTaskManagerçš„å¹¶è¡Œä»»åŠ¡ç®¡ç†...")
    
    file_path = "/Users/aron/twitter-daily-scraper/refactored_task_manager.py"
    
    # è¯»å–åŸæ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä¿®å¤1: ç¡®ä¿ç”¨æˆ·IDæ± æ­£ç¡®åˆå§‹åŒ–
    old_init = '''def __init__(self, max_concurrent_tasks=1, user_ids=None):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.user_id_pool = list(user_ids or ['default'])
        self.available_user_ids = list(user_ids or ['default'])  # å…¼å®¹åŸAPI'''
    
    new_init = '''def __init__(self, max_concurrent_tasks=1, user_ids=None):
        self.max_concurrent_tasks = max_concurrent_tasks
        # ç¡®ä¿ç”¨æˆ·IDæ± æ­£ç¡®è®¾ç½®
        if user_ids and isinstance(user_ids, list) and len(user_ids) > 0:
            self.user_id_pool = list(user_ids)
            self.available_user_ids = list(user_ids)  # å…¼å®¹åŸAPI
            logger.info(f"[RefactoredTaskManager] åˆå§‹åŒ–ç”¨æˆ·IDæ± : {user_ids}")
        else:
            self.user_id_pool = ['default']
            self.available_user_ids = ['default']
            logger.warning(f"[RefactoredTaskManager] ä½¿ç”¨é»˜è®¤ç”¨æˆ·ID: ['default']")
        
        logger.info(f"[RefactoredTaskManager] æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°: {max_concurrent_tasks}")
        logger.info(f"[RefactoredTaskManager] å¯ç”¨ç”¨æˆ·IDæ•°é‡: {len(self.user_id_pool)}")
        logger.info(f"[RefactoredTaskManager] ç”¨æˆ·IDåˆ—è¡¨: {self.user_id_pool}")'''
    
    if old_init in content:
        content = content.replace(old_init, new_init)
        logger.info("âœ… ä¿®å¤äº†ç”¨æˆ·IDæ± åˆå§‹åŒ–é€»è¾‘")
    
    # ä¿®å¤2: æ”¹è¿›ç”¨æˆ·IDåˆ†é…é€»è¾‘
    old_get_user_id = '''def _get_available_user_id(self) -> Optional[str]:
        """è·å–å¯ç”¨çš„ç”¨æˆ·ID"""
        with self._state_lock:
            if self.available_users:
                return self.available_users.pop()
            return None'''
    
    new_get_user_id = '''def _get_available_user_id(self) -> Optional[str]:
        """è·å–å¯ç”¨çš„ç”¨æˆ·ID"""
        with self._state_lock:
            if self.available_users:
                user_id = self.available_users.pop()
                logger.info(f"[RefactoredTaskManager] åˆ†é…ç”¨æˆ·ID: {user_id}ï¼Œå‰©ä½™å¯ç”¨: {len(self.available_users)}")
                return user_id
            logger.warning(f"[RefactoredTaskManager] æ²¡æœ‰å¯ç”¨çš„ç”¨æˆ·IDï¼Œå½“å‰æ´»è·ƒä»»åŠ¡: {len(self.active_slots)}")
            return None'''
    
    if old_get_user_id in content:
        content = content.replace(old_get_user_id, new_get_user_id)
        logger.info("âœ… ä¿®å¤äº†ç”¨æˆ·IDåˆ†é…é€»è¾‘")
    
    # ä¿®å¤3: æ”¹è¿›ç”¨æˆ·IDå½’è¿˜é€»è¾‘
    old_return_user_id = '''def _return_user_id(self, user_id: str):
        """å½’è¿˜ç”¨æˆ·ID"""
        with self._state_lock:
            self.available_users.add(user_id)'''
    
    new_return_user_id = '''def _return_user_id(self, user_id: str):
        """å½’è¿˜ç”¨æˆ·ID"""
        with self._state_lock:
            if user_id in self.user_id_pool:  # ç¡®ä¿åªå½’è¿˜æœ‰æ•ˆçš„ç”¨æˆ·ID
                self.available_users.add(user_id)
                logger.info(f"[RefactoredTaskManager] å½’è¿˜ç”¨æˆ·ID: {user_id}ï¼Œå½“å‰å¯ç”¨: {len(self.available_users)}")
            else:
                logger.warning(f"[RefactoredTaskManager] å°è¯•å½’è¿˜æ— æ•ˆç”¨æˆ·ID: {user_id}")'''
    
    if old_return_user_id in content:
        content = content.replace(old_return_user_id, new_return_user_id)
        logger.info("âœ… ä¿®å¤äº†ç”¨æˆ·IDå½’è¿˜é€»è¾‘")
    
    # ä¿®å¤4: æ”¹è¿›ä»»åŠ¡å¯åŠ¨æ—¥å¿—
    old_start_background = '''print(f"[RefactoredTaskManager] åå°ä»»åŠ¡ {task_id} å¯åŠ¨æˆåŠŸï¼ŒPID: {process.pid}")'''
    new_start_background = '''print(f"[RefactoredTaskManager] åå°ä»»åŠ¡ {task_id} å¯åŠ¨æˆåŠŸï¼ŒPID: {process.pid}ï¼Œç”¨æˆ·ID: {user_id}")
            logger.info(f"[RefactoredTaskManager] å½“å‰æ´»è·ƒä»»åŠ¡æ•°: {len(self.active_slots)}/{self.max_concurrent_tasks}")'''
    
    if old_start_background in content:
        content = content.replace(old_start_background, new_start_background)
        logger.info("âœ… ä¿®å¤äº†ä»»åŠ¡å¯åŠ¨æ—¥å¿—")
    
    # å†™å›æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    logger.info("âœ… RefactoredTaskManagerä¿®å¤å®Œæˆ")

def fix_twitter_parser_scraping():
    """ä¿®å¤TwitterParserçš„æ¨æ–‡æŠ“å–é€»è¾‘"""
    logger.info("å¼€å§‹ä¿®å¤TwitterParserçš„æ¨æ–‡æŠ“å–é€»è¾‘...")
    
    file_path = "/Users/aron/twitter-daily-scraper/twitter_parser.py"
    
    # è¯»å–åŸæ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä¿®å¤1: æ”¹è¿›scrape_tweetsæ–¹æ³•çš„æ»šåŠ¨å’ŒæŠ“å–é€»è¾‘
    old_scrape_tweets = '''async def scrape_tweets(self, max_tweets: int = 10, enable_enhanced: bool = False) -> List[Dict[str, Any]]:
        """
        æŠ“å–å½“å‰é¡µé¢çš„æ¨æ–‡æ•°æ®
        
        Args:
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        if enable_enhanced:
            return await self.enhanced_tweet_scraping(max_tweets, enable_details=True)
        
        tweets_data = []
        
        try:
            # ç­‰å¾…æ¨æ–‡åŠ è½½
            await self.page.wait_for_selector('[data-testid="tweet"]', timeout=10000)
            
            # è·å–å½“å‰é¡µé¢çš„æ¨æ–‡å…ƒç´ 
            tweet_elements = await self.page.query_selector_all('[data-testid="tweet"]')
            available_tweets = len(tweet_elements)
            
            self.logger.info(f"å½“å‰é¡µé¢æ‰¾åˆ° {available_tweets} æ¡æ¨æ–‡ï¼Œç›®æ ‡æŠ“å– {max_tweets} æ¡")
            
            # ç¡®å®šå®é™…æŠ“å–æ•°é‡
            actual_max = min(max_tweets, available_tweets)
            
            for i, tweet_element in enumerate(tweet_elements[:actual_max]):
                # æ¯éš”å‡ æ¡æ¨æ–‡æ£€æŸ¥é¡µé¢ç„¦ç‚¹
                if i % 5 == 0:
                    await self.ensure_page_focus()
                
                # æ¨¡æ‹Ÿäººå·¥æŸ¥çœ‹æ¨æ–‡çš„è¡Œä¸º
                if self.behavior_simulator and i % 3 == 0:  # æ¯3æ¡æ¨æ–‡æ¨¡æ‹Ÿä¸€æ¬¡äº¤äº’
                    await self.behavior_simulator.simulate_tweet_interaction(tweet_element)
                
                self.logger.debug(f"å¼€å§‹è§£æç¬¬ {i+1} ä¸ªæ¨æ–‡å…ƒç´ ")
                tweet_data = await self.parse_tweet_element(tweet_element)
                if tweet_data:
                    tweets_data.append(tweet_data)
                    self.logger.info(f"æˆåŠŸè§£æç¬¬ {i+1} æ¡æ¨æ–‡: @{tweet_data['username']}")
                else:
                    self.logger.debug(f"ç¬¬ {i+1} ä¸ªæ¨æ–‡å…ƒç´ è§£æå¤±è´¥æˆ–è¿”å›None")
                
                # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººå·¥é˜…è¯»
                await asyncio.sleep(0.1)
            
            self.logger.info(f"æ¨æ–‡æŠ“å–å®Œæˆï¼ŒæˆåŠŸè·å– {len(tweets_data)} æ¡æ¨æ–‡")
            return tweets_data
            
        except Exception as e:
            self.logger.error(f"æŠ“å–æ¨æ–‡å¤±è´¥: {e}")
            return []'''
    
    new_scrape_tweets = '''async def scrape_tweets(self, max_tweets: int = 10, enable_enhanced: bool = False) -> List[Dict[str, Any]]:
        """
        æŠ“å–å½“å‰é¡µé¢çš„æ¨æ–‡æ•°æ®
        
        Args:
            max_tweets: æœ€å¤§æŠ“å–æ¨æ–‡æ•°é‡
            enable_enhanced: æ˜¯å¦å¯ç”¨å¢å¼ºæŠ“å–ï¼ˆåŒ…å«è¯¦æƒ…é¡µå†…å®¹ï¼‰
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        if enable_enhanced:
            return await self.enhanced_tweet_scraping(max_tweets, enable_details=True)
        
        tweets_data = []
        max_scroll_attempts = 20  # æœ€å¤§æ»šåŠ¨æ¬¡æ•°
        scroll_attempts = 0
        last_tweet_count = 0
        no_new_tweets_count = 0
        
        try:
            self.logger.info(f"å¼€å§‹æŠ“å–æ¨æ–‡ï¼Œç›®æ ‡æ•°é‡: {max_tweets}")
            
            while len(tweets_data) < max_tweets and scroll_attempts < max_scroll_attempts:
                # ç­‰å¾…æ¨æ–‡åŠ è½½
                try:
                    await self.page.wait_for_selector('[data-testid="tweet"]', timeout=5000)
                except Exception:
                    self.logger.warning(f"ç­‰å¾…æ¨æ–‡å…ƒç´ è¶…æ—¶ï¼Œå½“å‰æ»šåŠ¨æ¬¡æ•°: {scroll_attempts}")
                    if scroll_attempts == 0:  # ç¬¬ä¸€æ¬¡å°±æ²¡æ‰¾åˆ°æ¨æ–‡ï¼Œå¯èƒ½é¡µé¢æœ‰é—®é¢˜
                        break
                
                # è·å–å½“å‰é¡µé¢çš„æ¨æ–‡å…ƒç´ 
                tweet_elements = await self.page.query_selector_all('[data-testid="tweet"]')
                current_tweet_count = len(tweet_elements)
                
                self.logger.info(f"æ»šåŠ¨ç¬¬ {scroll_attempts + 1} æ¬¡ï¼Œé¡µé¢æ¨æ–‡æ•°: {current_tweet_count}ï¼Œå·²æŠ“å–: {len(tweets_data)}")
                
                # è§£ææ–°çš„æ¨æ–‡
                new_tweets_parsed = 0
                for i, tweet_element in enumerate(tweet_elements):
                    if len(tweets_data) >= max_tweets:
                        break
                    
                    # æ¯éš”å‡ æ¡æ¨æ–‡æ£€æŸ¥é¡µé¢ç„¦ç‚¹
                    if i % 5 == 0:
                        await self.ensure_page_focus()
                    
                    tweet_data = await self.parse_tweet_element(tweet_element)
                    if tweet_data:
                        # æ£€æŸ¥æ˜¯å¦å·²ç»æŠ“å–è¿‡è¿™æ¡æ¨æ–‡ï¼ˆå»é‡ï¼‰
                        tweet_id = tweet_data.get('link', '') or tweet_data.get('content', '')[:50]
                        if tweet_id not in self.seen_tweet_ids:
                            self.seen_tweet_ids.add(tweet_id)
                            tweets_data.append(tweet_data)
                            new_tweets_parsed += 1
                            self.logger.debug(f"æ–°æŠ“å–æ¨æ–‡: @{tweet_data.get('username', 'unknown')}")
                
                self.logger.info(f"æœ¬æ¬¡æ»šåŠ¨æ–°è§£ææ¨æ–‡: {new_tweets_parsed}ï¼Œç´¯è®¡: {len(tweets_data)}/{max_tweets}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡
                if current_tweet_count <= last_tweet_count:
                    no_new_tweets_count += 1
                    self.logger.info(f"é¡µé¢æ¨æ–‡æ•°é‡æœªå¢åŠ ï¼Œè¿ç»­æ¬¡æ•°: {no_new_tweets_count}")
                    if no_new_tweets_count >= 3:  # è¿ç»­3æ¬¡æ²¡æœ‰æ–°æ¨æ–‡ï¼Œå¯èƒ½åˆ°åº•äº†
                        self.logger.info("è¿ç»­å¤šæ¬¡æ»šåŠ¨æ— æ–°æ¨æ–‡ï¼Œå¯èƒ½å·²åˆ°é¡µé¢åº•éƒ¨")
                        break
                else:
                    no_new_tweets_count = 0
                
                last_tweet_count = current_tweet_count
                
                # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œåœæ­¢æ»šåŠ¨
                if len(tweets_data) >= max_tweets:
                    self.logger.info(f"å·²è¾¾åˆ°ç›®æ ‡æ¨æ–‡æ•°é‡: {len(tweets_data)}/{max_tweets}")
                    break
                
                # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šæ¨æ–‡
                scroll_attempts += 1
                if scroll_attempts < max_scroll_attempts:
                    await self.page.evaluate('window.scrollBy(0, 800)')
                    await asyncio.sleep(1)  # ç­‰å¾…åŠ è½½
                    
                    # å¤„ç†å¯èƒ½çš„å¼¹çª—
                    try:
                        await self.dismiss_translate_popup()
                    except Exception:
                        pass
            
            # åªè¿”å›ç›®æ ‡æ•°é‡çš„æ¨æ–‡
            final_tweets = tweets_data[:max_tweets]
            self.logger.info(f"æ¨æ–‡æŠ“å–å®Œæˆï¼Œç›®æ ‡: {max_tweets}ï¼Œå®é™…è·å–: {len(final_tweets)}ï¼Œæ»šåŠ¨æ¬¡æ•°: {scroll_attempts}")
            
            if len(final_tweets) < max_tweets:
                shortage = max_tweets - len(final_tweets)
                self.logger.warning(f"æ¨æ–‡æ•°é‡ä¸è¶³ï¼Œç¼ºå°‘ {shortage} æ¡æ¨æ–‡")
            
            return final_tweets
            
        except Exception as e:
            self.logger.error(f"æŠ“å–æ¨æ–‡å¤±è´¥: {e}")
            return tweets_data[:max_tweets] if tweets_data else []'''
    
    if old_scrape_tweets in content:
        content = content.replace(old_scrape_tweets, new_scrape_tweets)
        logger.info("âœ… ä¿®å¤äº†scrape_tweetsæ–¹æ³•çš„æ»šåŠ¨å’ŒæŠ“å–é€»è¾‘")
    
    # å†™å›æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    logger.info("âœ… TwitterParseræ¨æ–‡æŠ“å–é€»è¾‘ä¿®å¤å®Œæˆ")

def fix_web_app_config():
    """ä¿®å¤web_app.pyä¸­çš„é…ç½®åˆå§‹åŒ–"""
    logger.info("å¼€å§‹ä¿®å¤web_app.pyä¸­çš„é…ç½®åˆå§‹åŒ–...")
    
    file_path = "/Users/aron/twitter-daily-scraper/web_app.py"
    
    # è¯»å–åŸæ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä¿®å¤init_task_managerå‡½æ•°
    old_init_task_manager = '''def init_task_manager():
    """åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨"""
    global task_manager, optimized_scraper
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
    if task_manager is not None:
        print("âš ï¸ TaskManagerå·²ç»åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
        return
    
    max_concurrent = ADS_POWER_CONFIG.get('max_concurrent_tasks', 2)
    user_ids = ADS_POWER_CONFIG.get('user_ids', [ADS_POWER_CONFIG.get('user_id', 'default')])
    task_manager = RefactoredTaskManager(max_concurrent_tasks=max_concurrent, user_ids=user_ids)
    
    print(f"[RefactoredTaskManager] åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å¹¶å‘: {max_concurrent}")
    
    # åˆå§‹åŒ–ä¼˜åŒ–æŠ“å–å™¨
    # optimized_scraper = MultiWindowEnhancedScraper(max_workers=max_concurrent)
    
    print(f"âœ… TaskManagerå·²åˆå§‹åŒ–ï¼Œæœ€å¤§å¹¶å‘ä»»åŠ¡æ•°: {max_concurrent}")
    print(f"âœ… OptimizedScraperå·²åˆå§‹åŒ–ï¼Œæ”¯æŒå¤šçª—å£å¹¶å‘æŠ“å–")'''
    
    new_init_task_manager = '''def init_task_manager():
    """åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨"""
    global task_manager, optimized_scraper
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
    if task_manager is not None:
        print("âš ï¸ TaskManagerå·²ç»åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
        return
    
    max_concurrent = ADS_POWER_CONFIG.get('max_concurrent_tasks', 2)
    
    # è·å–ç”¨æˆ·IDåˆ—è¡¨ï¼Œä¼˜å…ˆä½¿ç”¨user_idsï¼Œç„¶åæ˜¯multi_user_idsï¼Œæœ€åæ˜¯å•ä¸ªuser_id
    user_ids = ADS_POWER_CONFIG.get('user_ids')
    if not user_ids:
        user_ids = ADS_POWER_CONFIG.get('multi_user_ids')
    if not user_ids:
        user_ids = [ADS_POWER_CONFIG.get('user_id', 'default')]
    
    print(f"[TaskManager] é…ç½®ä¿¡æ¯:")
    print(f"  - æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°: {max_concurrent}")
    print(f"  - ç”¨æˆ·IDåˆ—è¡¨: {user_ids}")
    print(f"  - ç”¨æˆ·IDæ•°é‡: {len(user_ids)}")
    
    # ç¡®ä¿ç”¨æˆ·IDæ•°é‡è¶³å¤Ÿæ”¯æŒå¹¶å‘ä»»åŠ¡
    if len(user_ids) < max_concurrent:
        print(f"âš ï¸ è­¦å‘Š: ç”¨æˆ·IDæ•°é‡({len(user_ids)})å°‘äºæœ€å¤§å¹¶å‘ä»»åŠ¡æ•°({max_concurrent})")
        print(f"âš ï¸ å»ºè®®é…ç½®è‡³å°‘ {max_concurrent} ä¸ªç”¨æˆ·IDä»¥æ”¯æŒå®Œå…¨å¹¶è¡Œ")
    
    task_manager = RefactoredTaskManager(max_concurrent_tasks=max_concurrent, user_ids=user_ids)
    
    print(f"[RefactoredTaskManager] åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å¹¶å‘: {max_concurrent}")
    
    # åˆå§‹åŒ–ä¼˜åŒ–æŠ“å–å™¨
    # optimized_scraper = MultiWindowEnhancedScraper(max_workers=max_concurrent)
    
    print(f"âœ… TaskManagerå·²åˆå§‹åŒ–ï¼Œæœ€å¤§å¹¶å‘ä»»åŠ¡æ•°: {max_concurrent}")
    print(f"âœ… ç”¨æˆ·IDæ± å¤§å°: {len(user_ids)}")
    print(f"âœ… OptimizedScraperå·²åˆå§‹åŒ–ï¼Œæ”¯æŒå¤šçª—å£å¹¶å‘æŠ“å–")'''
    
    if old_init_task_manager in content:
        content = content.replace(old_init_task_manager, new_init_task_manager)
        logger.info("âœ… ä¿®å¤äº†init_task_managerå‡½æ•°")
    
    # å†™å›æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    logger.info("âœ… web_app.pyé…ç½®åˆå§‹åŒ–ä¿®å¤å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹ä¿®å¤å¹¶è¡Œä»»åŠ¡å’ŒæŠ“å–é€»è¾‘é—®é¢˜...")
    
    try:
        # ä¿®å¤1: RefactoredTaskManagerçš„å¹¶è¡Œä»»åŠ¡ç®¡ç†
        fix_refactored_task_manager()
        
        # ä¿®å¤2: TwitterParserçš„æ¨æ–‡æŠ“å–é€»è¾‘
        fix_twitter_parser_scraping()
        
        # ä¿®å¤3: web_app.pyçš„é…ç½®åˆå§‹åŒ–
        fix_web_app_config()
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ æ‰€æœ‰ä¿®å¤å®Œæˆï¼")
        logger.info("="*60)
        logger.info("ä¿®å¤å†…å®¹æ€»ç»“:")
        logger.info("1. âœ… ä¿®å¤äº†RefactoredTaskManagerçš„ç”¨æˆ·IDæ± ç®¡ç†")
        logger.info("2. âœ… æ”¹è¿›äº†ç”¨æˆ·IDåˆ†é…å’Œå½’è¿˜é€»è¾‘")
        logger.info("3. âœ… ä¿®å¤äº†TwitterParserçš„æ¨æ–‡æŠ“å–å’Œæ»šåŠ¨é€»è¾‘")
        logger.info("4. âœ… æ”¹è¿›äº†æ¨æ–‡æ•°é‡åˆ¤æ–­å’Œå»é‡æœºåˆ¶")
        logger.info("5. âœ… ä¿®å¤äº†web_app.pyçš„ä»»åŠ¡ç®¡ç†å™¨åˆå§‹åŒ–")
        logger.info("="*60)
        logger.info("\nç°åœ¨ç³»ç»Ÿåº”è¯¥èƒ½å¤Ÿ:")
        logger.info("â€¢ åŒæ—¶å¯åŠ¨å¤šä¸ªä»»åŠ¡æ—¶æ­£ç¡®åˆ†é…ä¸åŒçš„AdsPowerç”¨æˆ·ID")
        logger.info("â€¢ å¯åŠ¨å¤šä¸ªæµè§ˆå™¨å®ä¾‹å¹¶è¡Œæ‰§è¡ŒæŠ“å–ä»»åŠ¡")
        logger.info("â€¢ æ­£ç¡®åˆ¤æ–­æ¨æ–‡æ•°é‡å¹¶è¿›è¡Œå……åˆ†çš„æ»šåŠ¨æŠ“å–")
        logger.info("â€¢ é¿å…Mike_Stelznerç­‰å¤§Vè´¦å·çš„å†…å®¹ä¸è¶³è¯¯æŠ¥")
        logger.info("\nè¯·é‡å¯åº”ç”¨ä»¥ä½¿ä¿®å¤ç”Ÿæ•ˆã€‚")
        
    except Exception as e:
        logger.error(f"ä¿®å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()