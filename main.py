#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Twitter æ—¥æŠ¥é‡‡é›†ç³»ç»Ÿ - ä¸»ç¨‹åºå…¥å£
åŸºäº AdsPower è™šæ‹Ÿæµè§ˆå™¨çš„è‡ªåŠ¨åŒ– Twitter ä¿¡æ¯é‡‡é›†ç³»ç»Ÿ
"""

import asyncio
import logging
import sys
import os
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import (
    ADS_POWER_CONFIG, TWITTER_TARGETS, FILTER_CONFIG, 
    OUTPUT_CONFIG, BROWSER_CONFIG, LOG_CONFIG
)
from ads_browser_launcher import AdsPowerLauncher
from twitter_parser import TwitterParser
from tweet_filter import TweetFilter
from excel_writer import ExcelWriter

class TwitterDailyScraper:
    def __init__(self):
        self.launcher = AdsPowerLauncher()
        self.parser = None
        self.filter_engine = TweetFilter()
        self.excel_writer = ExcelWriter()
        self.logger = self.setup_logging()
        
    def setup_logging(self) -> logging.Logger:
        """
        è®¾ç½®æ—¥å¿—é…ç½®
        
        Returns:
            é…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨
        """
        # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
        logger = logging.getLogger('TwitterScraper')
        logger.setLevel(getattr(logging, LOG_CONFIG['level']))
        
        # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
        if logger.handlers:
            return logger
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(LOG_CONFIG['format'])
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        try:
            file_handler = logging.FileHandler(LOG_CONFIG['filename'], encoding='utf-8')
            file_handler.setLevel(logging.DEBUG)
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        except Exception as e:
            logger.warning(f"æ— æ³•åˆ›å»ºæ—¥å¿—æ–‡ä»¶: {e}")
        
        return logger
    
    async def initialize_browser(self, user_id: str = None) -> bool:
        """
        åˆå§‹åŒ–æµè§ˆå™¨ç¯å¢ƒ
        
        Args:
            user_id: AdsPower ç”¨æˆ·ID
            
        Returns:
            æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        try:
            self.logger.info("å¼€å§‹åˆå§‹åŒ– AdsPower æµè§ˆå™¨...")
            
            # å¯åŠ¨ AdsPower æµè§ˆå™¨
            browser_info = self.launcher.start_browser(user_id)
            
            # ç­‰å¾…æµè§ˆå™¨å‡†å¤‡å°±ç»ª
            if not self.launcher.wait_for_browser_ready():
                raise Exception("æµè§ˆå™¨å¯åŠ¨è¶…æ—¶")
            
            # è·å–è°ƒè¯•ç«¯å£
            debug_port = self.launcher.get_debug_port()
            if not debug_port:
                raise Exception("æ— æ³•è·å–æµè§ˆå™¨è°ƒè¯•ç«¯å£")
            
            self.logger.info(f"æµè§ˆå™¨è°ƒè¯•ç«¯å£: {debug_port}")
            
            # åˆ›å»º Twitter è§£æå™¨å¹¶è¿æ¥æµè§ˆå™¨
            self.parser = TwitterParser(debug_port)
            await self.parser.connect_browser()
            
            # å¯¼èˆªåˆ° Twitter
            await self.parser.navigate_to_twitter()
            
            self.logger.info("æµè§ˆå™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def scrape_user_tweets(self, usernames: List[str], max_tweets_per_user: int = 10) -> List[Dict[str, Any]]:
        """
        æŠ“å–æŒ‡å®šç”¨æˆ·çš„æ¨æ–‡
        
        Args:
            usernames: ç”¨æˆ·ååˆ—è¡¨
            max_tweets_per_user: æ¯ä¸ªç”¨æˆ·æœ€å¤§æŠ“å–æ¨æ–‡æ•°
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        all_tweets = []
        
        for username in usernames:
            try:
                self.logger.info(f"å¼€å§‹æŠ“å–ç”¨æˆ· @{username} çš„æ¨æ–‡...")
                
                tweets = await self.parser.scrape_user_tweets(username, max_tweets_per_user)
                all_tweets.extend(tweets)
                
                self.logger.info(f"ç”¨æˆ· @{username} æŠ“å–å®Œæˆï¼Œè·å¾— {len(tweets)} æ¡æ¨æ–‡")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
                await asyncio.sleep(BROWSER_CONFIG['wait_time'])
                
            except Exception as e:
                self.logger.error(f"æŠ“å–ç”¨æˆ· @{username} å¤±è´¥: {e}")
                continue
        
        return all_tweets
    
    async def scrape_keyword_tweets(self, keywords: List[str], max_tweets_per_keyword: int = 10) -> List[Dict[str, Any]]:
        """
        æŠ“å–åŒ…å«æŒ‡å®šå…³é”®è¯çš„æ¨æ–‡
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            max_tweets_per_keyword: æ¯ä¸ªå…³é”®è¯æœ€å¤§æŠ“å–æ¨æ–‡æ•°
            
        Returns:
            æ¨æ–‡æ•°æ®åˆ—è¡¨
        """
        all_tweets = []
        
        for keyword in keywords:
            try:
                self.logger.info(f"å¼€å§‹æœç´¢å…³é”®è¯ '{keyword}' çš„æ¨æ–‡...")
                
                tweets = await self.parser.scrape_keyword_tweets(keyword, max_tweets_per_keyword)
                all_tweets.extend(tweets)
                
                self.logger.info(f"å…³é”®è¯ '{keyword}' æœç´¢å®Œæˆï¼Œè·å¾— {len(tweets)} æ¡æ¨æ–‡")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
                await asyncio.sleep(BROWSER_CONFIG['wait_time'])
                
            except Exception as e:
                self.logger.error(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                continue
        
        return all_tweets
    
    def remove_duplicates(self, tweets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å»é™¤é‡å¤çš„æ¨æ–‡
        
        Args:
            tweets: æ¨æ–‡æ•°æ®åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ¨æ–‡åˆ—è¡¨
        """
        seen_links = set()
        unique_tweets = []
        
        for tweet in tweets:
            link = tweet.get('link', '')
            content = tweet.get('content', '')
            
            # ä½¿ç”¨é“¾æ¥æˆ–å†…å®¹ä½œä¸ºå»é‡æ ‡è¯†
            identifier = link if link else content
            
            if identifier and identifier not in seen_links:
                seen_links.add(identifier)
                unique_tweets.append(tweet)
        
        removed_count = len(tweets) - len(unique_tweets)
        if removed_count > 0:
            self.logger.info(f"å»é™¤äº† {removed_count} æ¡é‡å¤æ¨æ–‡")
        
        return unique_tweets
    
    async def run_scraping_task(self, user_id: str = None) -> str:
        """
        æ‰§è¡Œå®Œæ•´çš„æŠ“å–ä»»åŠ¡
        
        Args:
            user_id: AdsPower ç”¨æˆ·ID
            
        Returns:
            ç”Ÿæˆçš„ Excel æ–‡ä»¶è·¯å¾„
        """
        try:
            self.logger.info("="*50)
            self.logger.info("Twitter æ—¥æŠ¥é‡‡é›†ä»»åŠ¡å¼€å§‹")
            self.logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            self.logger.info("="*50)
            
            # åˆå§‹åŒ–æµè§ˆå™¨
            if not await self.initialize_browser(user_id):
                raise Exception("æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
            
            all_tweets = []
            
            # æŠ“å–ç”¨æˆ·æ¨æ–‡
            if TWITTER_TARGETS['accounts']:
                self.logger.info(f"å¼€å§‹æŠ“å– {len(TWITTER_TARGETS['accounts'])} ä¸ªç”¨æˆ·çš„æ¨æ–‡...")
                user_tweets = await self.scrape_user_tweets(
                    TWITTER_TARGETS['accounts'], 
                    FILTER_CONFIG['max_tweets_per_target']
                )
                all_tweets.extend(user_tweets)
                self.logger.info(f"ç”¨æˆ·æ¨æ–‡æŠ“å–å®Œæˆï¼Œå…±è·å¾— {len(user_tweets)} æ¡æ¨æ–‡")
            
            # æŠ“å–å…³é”®è¯æ¨æ–‡
            if TWITTER_TARGETS['keywords']:
                self.logger.info(f"å¼€å§‹æœç´¢ {len(TWITTER_TARGETS['keywords'])} ä¸ªå…³é”®è¯çš„æ¨æ–‡...")
                keyword_tweets = await self.scrape_keyword_tweets(
                    TWITTER_TARGETS['keywords'], 
                    FILTER_CONFIG['max_tweets_per_target']
                )
                all_tweets.extend(keyword_tweets)
                self.logger.info(f"å…³é”®è¯æ¨æ–‡æœç´¢å®Œæˆï¼Œå…±è·å¾— {len(keyword_tweets)} æ¡æ¨æ–‡")
            
            if not all_tweets:
                self.logger.warning("æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ¨æ–‡æ•°æ®")
                return ''
            
            # å»é™¤é‡å¤æ¨æ–‡
            unique_tweets = self.remove_duplicates(all_tweets)
            self.logger.info(f"å»é‡åå…±æœ‰ {len(unique_tweets)} æ¡æ¨æ–‡")
            
            # ç­›é€‰æ¨æ–‡
            self.logger.info("å¼€å§‹ç­›é€‰æ¨æ–‡...")
            filtered_tweets = self.filter_engine.filter_tweets(unique_tweets)
            passed_tweets = self.filter_engine.get_passed_tweets(filtered_tweets)
            
            self.logger.info(f"ç­›é€‰å®Œæˆï¼Œ{len(passed_tweets)}/{len(unique_tweets)} æ¡æ¨æ–‡é€šè¿‡ç­›é€‰")
            
            if not passed_tweets:
                self.logger.warning("æ²¡æœ‰æ¨æ–‡é€šè¿‡ç­›é€‰æ¡ä»¶")
                # ä»ç„¶ç”Ÿæˆæ–‡ä»¶ï¼Œä½†åªåŒ…å«ç»Ÿè®¡ä¿¡æ¯
                passed_tweets = []
            
            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            statistics = self.filter_engine.get_filter_statistics(filtered_tweets)
            
            # å¯¼å‡ºåˆ° Excel
            self.logger.info("å¼€å§‹å¯¼å‡ºæ•°æ®åˆ° Excel...")
            output_file = self.excel_writer.write_tweets_with_summary(passed_tweets, statistics)
            
            self.logger.info("="*50)
            self.logger.info("Twitter æ—¥æŠ¥é‡‡é›†ä»»åŠ¡å®Œæˆ")
            self.logger.info(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            self.logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
            self.logger.info(f"æ€»æ¨æ–‡æ•°: {statistics['total_tweets']}")
            self.logger.info(f"é€šè¿‡ç­›é€‰: {statistics['passed_tweets']}")
            self.logger.info(f"é€šè¿‡ç‡: {statistics['pass_rate']:.2%}")
            self.logger.info("="*50)
            
            return output_file
            
        except Exception as e:
            self.logger.error(f"æŠ“å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            raise
        finally:
            await self.cleanup()
    
    async def cleanup(self):
        """
        æ¸…ç†èµ„æº
        """
        try:
            # å…³é—­æµè§ˆå™¨è§£æå™¨
            if self.parser:
                await self.parser.close()
            
            # åœæ­¢ AdsPower æµè§ˆå™¨
            self.launcher.stop_browser()
            
            self.logger.info("èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"èµ„æºæ¸…ç†å¤±è´¥: {e}")
    
    def validate_config(self) -> bool:
        """
        éªŒè¯é…ç½®æ–‡ä»¶
        
        Returns:
            é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        errors = []
        
        # æ£€æŸ¥ AdsPower é…ç½®
        if not ADS_POWER_CONFIG.get('user_id'):
            errors.append("AdsPower ç”¨æˆ·IDæœªé…ç½®")
        
        # æ£€æŸ¥ç›®æ ‡é…ç½®
        if not TWITTER_TARGETS['accounts'] and not TWITTER_TARGETS['keywords']:
            errors.append("æœªé…ç½®ä»»ä½•æŠ“å–ç›®æ ‡ï¼ˆç”¨æˆ·æˆ–å…³é”®è¯ï¼‰")
        
        # æ£€æŸ¥ç­›é€‰é…ç½®
        if (FILTER_CONFIG['min_likes'] <= 0 and 
            FILTER_CONFIG['min_comments'] <= 0 and 
            FILTER_CONFIG['min_retweets'] <= 0 and 
            not FILTER_CONFIG['keywords_filter']):
            errors.append("ç­›é€‰æ¡ä»¶é…ç½®æ— æ•ˆ")
        
        if errors:
            for error in errors:
                self.logger.error(f"é…ç½®é”™è¯¯: {error}")
            return False
        
        return True

async def main():
    """
    ä¸»å‡½æ•°
    """
    scraper = TwitterDailyScraper()
    
    try:
        # éªŒè¯é…ç½®
        if not scraper.validate_config():
            print("é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ config.py æ–‡ä»¶")
            return
        
        # æ‰§è¡ŒæŠ“å–ä»»åŠ¡
        output_file = await scraper.run_scraping_task()
        
        if output_file:
            print(f"\nâœ… ä»»åŠ¡å®Œæˆï¼")
            print(f"ğŸ“Š Excel æŠ¥è¡¨å·²ç”Ÿæˆ: {output_file}")
            print(f"ğŸ“ æ•°æ®ç›®å½•: {OUTPUT_CONFIG['data_dir']}")
        else:
            print("\nâŒ ä»»åŠ¡å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦æƒ…")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
        scraper.logger.info("ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        scraper.logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
    finally:
        await scraper.cleanup()

if __name__ == "__main__":
    # æ£€æŸ¥ Python ç‰ˆæœ¬
    if sys.version_info < (3, 7):
        print("é”™è¯¯: éœ€è¦ Python 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬")
        sys.exit(1)
    
    # è¿è¡Œä¸»ç¨‹åº
    try:
        asyncio.run(main())
    except Exception as e:
        print(f"ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)