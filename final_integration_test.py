#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆé›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•å®Œæ•´çš„TwitteræŠ“å–æµç¨‹
"""

import asyncio
import logging
import sys
import os
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ads_browser_launcher import AdsPowerLauncher
from twitter_parser import TwitterParser
from data_extractor import DataExtractor
from tweet_filter import TweetFilter

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# AdsPoweré…ç½®
ADS_POWER_CONFIG = {
    'user_id': 'k11p9ypc',
    'open_tabs': 1,
    'launch_args': [],
    'headless': False,
    'disable_password_filling': False,
    'clear_cache_after_closing': False,
    'enable_password_saving': False
}

async def test_complete_scraping_workflow():
    """æµ‹è¯•å®Œæ•´çš„æŠ“å–å·¥ä½œæµç¨‹"""
    launcher = None
    parser = None
    
    try:
        # å¯åŠ¨æµè§ˆå™¨
        logger.info("ğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        launcher = AdsPowerLauncher(ADS_POWER_CONFIG)
        browser_info = launcher.start_browser()
        launcher.wait_for_browser_ready()
        debug_port = launcher.get_debug_port()
        logger.info(f"âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼Œè°ƒè¯•ç«¯å£: {debug_port}")
        
        # åˆå§‹åŒ–TwitterParser
        logger.info("ğŸ”§ åˆå§‹åŒ–TwitterParser...")
        parser = TwitterParser(debug_port=debug_port)
        await parser.initialize()
        logger.info("âœ… TwitterParseråˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–æ•°æ®æå–å™¨å’Œè¿‡æ»¤å™¨
        data_extractor = DataExtractor()
        tweet_filter = TweetFilter()
        logger.info("âœ… æ•°æ®æå–å™¨å’Œè¿‡æ»¤å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # æµ‹è¯•åœºæ™¯1: ç”¨æˆ·æ¨æ–‡æŠ“å–
        logger.info("\nğŸ“‹ æµ‹è¯•åœºæ™¯1: ç”¨æˆ·æ¨æ–‡æŠ“å–")
        logger.info("ğŸ”„ å¯¼èˆªåˆ°@elonmuské¡µé¢...")
        
        try:
            await parser.navigate_to_profile('elonmusk')
            logger.info("âœ… æˆåŠŸå¯¼èˆªåˆ°ç”¨æˆ·é¡µé¢")
        except Exception as nav_error:
            logger.warning(f"å¯¼èˆªå¤±è´¥: {nav_error}ï¼Œå°è¯•ç›´æ¥è®¿é—®")
            await parser.page.goto('https://x.com/elonmusk', wait_until='domcontentloaded')
            await asyncio.sleep(5)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        logger.info("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
        await asyncio.sleep(10)
        
        # æ£€æŸ¥åˆå§‹æ¨æ–‡æ•°é‡
        initial_tweets = await parser.page.query_selector_all('[data-testid="tweet"]')
        logger.info(f"ğŸ“Š åˆå§‹æ¨æ–‡æ•°é‡: {len(initial_tweets)}")
        
        # æ‰§è¡Œæ»šåŠ¨åŠ è½½
        target_tweets = 15
        logger.info(f"ğŸ¯ å¼€å§‹æ»šåŠ¨åŠ è½½ï¼Œç›®æ ‡: {target_tweets} æ¡æ¨æ–‡")
        
        scroll_attempts = 0
        max_scroll_attempts = 8
        last_tweet_count = len(initial_tweets)
        stagnant_count = 0
        
        while scroll_attempts < max_scroll_attempts:
            # è·å–å½“å‰æ¨æ–‡æ•°é‡
            current_tweets = await parser.page.query_selector_all('[data-testid="tweet"]')
            current_count = len(current_tweets)
            
            logger.info(f"ğŸ“ˆ æ»šåŠ¨å°è¯• {scroll_attempts + 1}/{max_scroll_attempts}ï¼Œå½“å‰æ¨æ–‡: {current_count}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if current_count >= target_tweets:
                logger.info(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡æ¨æ–‡æ•°é‡: {current_count}/{target_tweets}")
                break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡åŠ è½½
            if current_count == last_tweet_count:
                stagnant_count += 1
            else:
                stagnant_count = 0
            
            last_tweet_count = current_count
            
            # å¦‚æœè¿ç»­åœæ»å¤ªå¤šæ¬¡ï¼Œåœæ­¢æ»šåŠ¨
            if stagnant_count >= 4:
                logger.warning("âš ï¸ è¿ç»­å¤šæ¬¡æ— æ–°æ¨æ–‡ï¼Œåœæ­¢æ»šåŠ¨")
                break
            
            # æ‰§è¡Œæ»šåŠ¨
            scroll_distance = 1000 if stagnant_count < 2 else 2000
            await parser.page.evaluate(f'window.scrollBy(0, {scroll_distance})')
            await asyncio.sleep(3)
            
            scroll_attempts += 1
        
        # è·å–æœ€ç»ˆæ¨æ–‡åˆ—è¡¨
        final_tweets = await parser.page.query_selector_all('[data-testid="tweet"]')
        final_count = len(final_tweets)
        logger.info(f"ğŸ“Š æœ€ç»ˆæ¨æ–‡æ•°é‡: {final_count}")
        
        # è§£ææ¨æ–‡æ•°æ®
        logger.info("ğŸ” å¼€å§‹è§£ææ¨æ–‡æ•°æ®...")
        parsed_tweets = []
        successful_parses = 0
        failed_parses = 0
        
        for i, tweet_element in enumerate(final_tweets[:target_tweets]):
            try:
                tweet_data = await parser.parse_tweet_element(tweet_element)
                if tweet_data:
                    # éªŒè¯å¿…è¦å­—æ®µ
                    if tweet_data.get('content') or tweet_data.get('username') != 'unknown':
                        parsed_tweets.append(tweet_data)
                        successful_parses += 1
                        logger.info(f"  âœ… æ¨æ–‡ {i+1}: {tweet_data.get('username', 'unknown')} - {tweet_data.get('content', 'No content')[:50]}...")
                    else:
                        failed_parses += 1
                        logger.warning(f"  âš ï¸ æ¨æ–‡ {i+1}: æ•°æ®ä¸å®Œæ•´")
                else:
                    failed_parses += 1
                    logger.warning(f"  âŒ æ¨æ–‡ {i+1}: è§£æå¤±è´¥")
            except Exception as e:
                failed_parses += 1
                logger.warning(f"  âŒ æ¨æ–‡ {i+1}: è§£æå¼‚å¸¸ - {e}")
        
        logger.info(f"ğŸ“Š è§£æç»“æœ: æˆåŠŸ {successful_parses}ï¼Œå¤±è´¥ {failed_parses}")
        
        # åº”ç”¨è¿‡æ»¤å™¨
        logger.info("ğŸ” åº”ç”¨æ¨æ–‡è¿‡æ»¤å™¨...")
        filtered_tweets = []
        for tweet in parsed_tweets:
            try:
                if tweet_filter.should_include_tweet(tweet):
                    filtered_tweets.append(tweet)
            except Exception as e:
                logger.warning(f"è¿‡æ»¤æ¨æ–‡æ—¶å‡ºé”™: {e}")
        
        logger.info(f"ğŸ“Š è¿‡æ»¤ç»“æœ: {len(filtered_tweets)}/{len(parsed_tweets)} æ¡æ¨æ–‡é€šè¿‡è¿‡æ»¤")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        test_results = {
            'timestamp': datetime.now().isoformat(),
            'test_scenario': 'user_profile_scraping',
            'target_user': 'elonmusk',
            'target_tweets': target_tweets,
            'final_tweet_count': final_count,
            'parsed_tweets': successful_parses,
            'failed_parses': failed_parses,
            'filtered_tweets': len(filtered_tweets),
            'scroll_attempts': scroll_attempts,
            'success_rate': successful_parses / max(final_count, 1) * 100,
            'sample_tweets': filtered_tweets[:3]  # ä¿å­˜å‰3æ¡ä½œä¸ºæ ·æœ¬
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æµ‹è¯•åœºæ™¯2: æœç´¢åŠŸèƒ½æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
        if successful_parses >= target_tweets * 0.6:  # å¦‚æœç”¨æˆ·æŠ“å–æˆåŠŸç‡è¶…è¿‡60%
            logger.info("\nğŸ“‹ æµ‹è¯•åœºæ™¯2: æœç´¢åŠŸèƒ½æµ‹è¯•")
            try:
                search_keyword = "AI"
                logger.info(f"ğŸ” æœç´¢å…³é”®è¯: {search_keyword}")
                
                await parser.search_tweets(search_keyword)
                await asyncio.sleep(5)
                
                # è·å–æœç´¢ç»“æœ
                search_tweets = await parser.page.query_selector_all('[data-testid="tweet"]')
                logger.info(f"ğŸ“Š æœç´¢ç»“æœ: {len(search_tweets)} æ¡æ¨æ–‡")
                
                if len(search_tweets) > 0:
                    logger.info("âœ… æœç´¢åŠŸèƒ½æ­£å¸¸")
                    test_results['search_test'] = {
                        'keyword': search_keyword,
                        'results_count': len(search_tweets),
                        'status': 'success'
                    }
                else:
                    logger.warning("âš ï¸ æœç´¢åŠŸèƒ½å¯èƒ½æœ‰é—®é¢˜")
                    test_results['search_test'] = {
                        'keyword': search_keyword,
                        'results_count': 0,
                        'status': 'no_results'
                    }
                    
            except Exception as search_error:
                logger.warning(f"âŒ æœç´¢åŠŸèƒ½æµ‹è¯•å¤±è´¥: {search_error}")
                test_results['search_test'] = {
                    'keyword': search_keyword,
                    'status': 'failed',
                    'error': str(search_error)
                }
        
        # æ›´æ–°æµ‹è¯•ç»“æœæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        logger.info("\nğŸ“Š æµ‹è¯•æŠ¥å‘Š:")
        logger.info(f"  ç›®æ ‡æ¨æ–‡æ•°é‡: {target_tweets}")
        logger.info(f"  å®é™…è·å–æ¨æ–‡: {final_count}")
        logger.info(f"  æˆåŠŸè§£ææ¨æ–‡: {successful_parses}")
        logger.info(f"  è§£ææˆåŠŸç‡: {successful_parses / max(final_count, 1) * 100:.1f}%")
        logger.info(f"  è¿‡æ»¤åæ¨æ–‡: {len(filtered_tweets)}")
        logger.info(f"  æ»šåŠ¨æ¬¡æ•°: {scroll_attempts}")
        
        # åˆ¤æ–­æµ‹è¯•ç»“æœ
        if successful_parses >= target_tweets * 0.7:
            logger.info("ğŸ‰ é›†æˆæµ‹è¯•æˆåŠŸï¼æŠ“å–ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
            return True
        elif successful_parses >= target_tweets * 0.4:
            logger.warning("âš ï¸ é›†æˆæµ‹è¯•éƒ¨åˆ†æˆåŠŸï¼Œç³»ç»ŸåŸºæœ¬å¯ç”¨ä½†éœ€è¦ä¼˜åŒ–")
            return True
        else:
            logger.error("âŒ é›†æˆæµ‹è¯•å¤±è´¥ï¼Œç³»ç»Ÿå­˜åœ¨ä¸¥é‡é—®é¢˜")
            return False
            
    except Exception as e:
        logger.error(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False
        
    finally:
        # æ¸…ç†èµ„æº
        if parser:
            try:
                await parser.close()
                logger.info("âœ… TwitterParserå·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­TwitterParseræ—¶å‡ºé”™: {e}")
        
        if launcher:
            try:
                launcher.stop_browser()
                logger.info("âœ… æµè§ˆå™¨å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    success = asyncio.run(test_complete_scraping_workflow())
    if success:
        logger.info("\nğŸŠ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼Œç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼")
    else:
        logger.error("\nğŸ’¥ æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        sys.exit(1)