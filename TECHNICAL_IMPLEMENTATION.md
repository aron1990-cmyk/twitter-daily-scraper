# Twitteré‡‡é›†ç³»ç»ŸæŠ€æœ¯å®ç°è¯¦è§£

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯å®ç°

### 1. é«˜é€Ÿé‡‡é›†å™¨å®ç° (HighSpeedCollector)

#### 1.1 æ ¸å¿ƒç®—æ³•
```python
class HighSpeedCollector:
    def __init__(self, target_rate=25, batch_size=50):
        self.target_rate = target_rate  # ç›®æ ‡é€Ÿç‡: 25æ¨æ–‡/åˆ†é’Ÿ
        self.batch_size = batch_size    # æ‰¹å¤„ç†å¤§å°
        self.start_time = time.time()
        self.total_collected = 0
        self.performance_metrics = {
            'collection_rate': 0,
            'processing_time': 0,
            'efficiency_score': 0
        }
    
    def calculate_target_rate(self, target_tweets, time_hours):
        """è®¡ç®—ç›®æ ‡é‡‡é›†é€Ÿç‡"""
        return target_tweets / (time_hours * 60)  # æ¨æ–‡/åˆ†é’Ÿ
    
    def process_tweets_batch(self, tweets, enable_dedup=True, enable_value_filter=True):
        """æ‰¹é‡å¤„ç†æ¨æ–‡"""
        start_time = time.time()
        processed_tweets = []
        
        # æ‰¹é‡å»é‡
        if enable_dedup:
            tweets = self._batch_deduplicate(tweets)
        
        # æ‰¹é‡ä»·å€¼ç­›é€‰
        if enable_value_filter:
            tweets = self._batch_value_filter(tweets)
        
        processing_time = time.time() - start_time
        self._update_performance_metrics(len(tweets), processing_time)
        
        return tweets
```

#### 1.2 æ€§èƒ½ç›‘æ§å®ç°
```python
def _update_performance_metrics(self, processed_count, processing_time):
    """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
    self.total_collected += processed_count
    elapsed_time = time.time() - self.start_time
    
    # è®¡ç®—å®æ—¶é‡‡é›†é€Ÿç‡
    current_rate = (self.total_collected / elapsed_time) * 60  # æ¨æ–‡/åˆ†é’Ÿ
    
    # è®¡ç®—æ•ˆç‡åˆ†æ•°
    efficiency = (current_rate / self.target_rate) * 100
    
    self.performance_metrics.update({
        'collection_rate': current_rate,
        'processing_time': processing_time,
        'efficiency_score': efficiency,
        'target_achievement': min(efficiency, 100)
    })
```

### 2. é«˜çº§å»é‡ç®—æ³•å®ç° (AdvancedDeduplicator)

#### 2.1 å¤šå±‚æ¬¡å»é‡ç­–ç•¥
```python
class AdvancedDeduplicator:
    def __init__(self, similarity_threshold=0.85):
        self.similarity_threshold = similarity_threshold
        self.seen_links = set()           # é“¾æ¥å»é‡
        self.seen_hashes = set()          # å“ˆå¸Œå»é‡
        self.content_cache = []           # å†…å®¹ç›¸ä¼¼åº¦ç¼“å­˜
        self.user_time_cache = {}         # ç”¨æˆ·æ—¶é—´ç¼“å­˜
        
    def is_duplicate(self, tweet):
        """å¤šå±‚æ¬¡é‡å¤æ£€æµ‹"""
        # ç¬¬ä¸€å±‚: é“¾æ¥å»é‡ (æœ€å¿«)
        if self._is_link_duplicate(tweet):
            return True
            
        # ç¬¬äºŒå±‚: å“ˆå¸Œå»é‡ (å¿«é€Ÿ)
        if self._is_hash_duplicate(tweet):
            return True
            
        # ç¬¬ä¸‰å±‚: ç”¨æˆ·æ—¶é—´å»é‡ (ä¸­ç­‰)
        if self._is_user_time_duplicate(tweet):
            return True
            
        # ç¬¬å››å±‚: å†…å®¹ç›¸ä¼¼åº¦å»é‡ (è¾ƒæ…¢ä½†å‡†ç¡®)
        if self._is_content_similar(tweet):
            return True
            
        # ä¸æ˜¯é‡å¤ï¼Œæ·»åŠ åˆ°ç¼“å­˜
        self._add_to_cache(tweet)
        return False
```

#### 2.2 å†…å®¹ç›¸ä¼¼åº¦ç®—æ³•
```python
def _calculate_similarity(self, text1, text2):
    """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ (ç¼–è¾‘è·ç¦»ç®—æ³•)"""
    # é¢„å¤„ç†æ–‡æœ¬
    text1 = self._preprocess_text(text1)
    text2 = self._preprocess_text(text2)
    
    # è®¡ç®—ç¼–è¾‘è·ç¦»
    distance = self._edit_distance(text1, text2)
    max_len = max(len(text1), len(text2))
    
    # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    similarity = 1 - (distance / max_len) if max_len > 0 else 1
    return similarity

def _edit_distance(self, s1, s2):
    """è®¡ç®—ç¼–è¾‘è·ç¦» (åŠ¨æ€è§„åˆ’)"""
    m, n = len(s1), len(s2)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    
    # åˆå§‹åŒ–
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j
    
    # åŠ¨æ€è§„åˆ’è®¡ç®—
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if s1[i-1] == s2[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(
                    dp[i-1][j] + 1,    # åˆ é™¤
                    dp[i][j-1] + 1,    # æ’å…¥
                    dp[i-1][j-1] + 1   # æ›¿æ¢
                )
    
    return dp[m][n]
```

### 3. æ¨æ–‡ä»·å€¼åˆ†æå®ç° (TweetValueAnalyzer)

#### 3.1 å¤šç»´åº¦è¯„åˆ†ç®—æ³•
```python
class TweetValueAnalyzer:
    def __init__(self, content_weight=0.4, engagement_weight=0.4, media_weight=0.2):
        self.content_weight = content_weight
        self.engagement_weight = engagement_weight
        self.media_weight = media_weight
        self.high_value_threshold = 3.0
        
    def calculate_tweet_value_score(self, tweet):
        """è®¡ç®—æ¨æ–‡ä»·å€¼åˆ†æ•° (0-10åˆ†)"""
        content_score = self._analyze_content_quality(tweet.get('content', ''))
        engagement_score = self._analyze_engagement_data(tweet)
        media_score = self._analyze_media_richness(tweet.get('media', {}))
        
        # åŠ æƒè®¡ç®—æ€»åˆ†
        total_score = (
            content_score * self.content_weight +
            engagement_score * self.engagement_weight +
            media_score * self.media_weight
        )
        
        return min(total_score, 10.0)  # é™åˆ¶æœ€é«˜åˆ†ä¸º10åˆ†
```

#### 3.2 å†…å®¹è´¨é‡åˆ†æ
```python
def _analyze_content_quality(self, content):
    """åˆ†æå†…å®¹è´¨é‡ (0-10åˆ†)"""
    if not content:
        return 0
    
    score = 0
    
    # é•¿åº¦åˆ†æ (0-2åˆ†)
    length_score = min(len(content) / 50, 2.0)  # 50å­—ç¬¦å¾—1åˆ†ï¼Œ100å­—ç¬¦å¾—2åˆ†
    score += length_score
    
    # ä¿¡æ¯å¯†åº¦åˆ†æ (0-3åˆ†)
    info_density = self._calculate_info_density(content)
    score += info_density
    
    # å…³é”®è¯ç›¸å…³æ€§ (0-3åˆ†)
    keyword_relevance = self._calculate_keyword_relevance(content)
    score += keyword_relevance
    
    # è¯­è¨€è´¨é‡ (0-2åˆ†)
    language_quality = self._assess_language_quality(content)
    score += language_quality
    
    return score

def _calculate_info_density(self, content):
    """è®¡ç®—ä¿¡æ¯å¯†åº¦"""
    # ç»Ÿè®¡æœ‰æ„ä¹‰çš„è¯æ±‡
    meaningful_words = re.findall(r'\b[a-zA-Z\u4e00-\u9fff]{3,}\b', content)
    
    # ç»Ÿè®¡æ•°å­—ã€é“¾æ¥ã€æ ‡ç­¾ç­‰ä¿¡æ¯å…ƒç´ 
    numbers = re.findall(r'\d+', content)
    links = re.findall(r'http[s]?://\S+', content)
    hashtags = re.findall(r'#\w+', content)
    mentions = re.findall(r'@\w+', content)
    
    # è®¡ç®—ä¿¡æ¯å¯†åº¦åˆ†æ•°
    info_elements = len(meaningful_words) + len(numbers) + len(links) + len(hashtags) + len(mentions)
    total_words = len(content.split())
    
    if total_words == 0:
        return 0
    
    density = info_elements / total_words
    return min(density * 6, 3.0)  # æœ€é«˜3åˆ†
```

#### 3.3 äº’åŠ¨æ•°æ®åˆ†æ
```python
def _analyze_engagement_data(self, tweet):
    """åˆ†æäº’åŠ¨æ•°æ® (0-10åˆ†)"""
    likes = tweet.get('likes', 0)
    comments = tweet.get('comments', 0)
    retweets = tweet.get('retweets', 0)
    
    # åŠ æƒè®¡ç®—äº’åŠ¨åˆ†æ•°
    engagement_score = (
        likes * 0.5 +      # ç‚¹èµæƒé‡0.5
        comments * 2.0 +   # è¯„è®ºæƒé‡2.0 (æ›´æœ‰ä»·å€¼)
        retweets * 1.5     # è½¬å‘æƒé‡1.5
    )
    
    # å¯¹æ•°ç¼©æ”¾ï¼Œé¿å…æå€¼å½±å“
    if engagement_score > 0:
        log_score = math.log10(engagement_score + 1) * 2
        return min(log_score, 10.0)
    
    return 0
```

### 4. å¢å¼ºæœç´¢ä¼˜åŒ–å®ç° (EnhancedSearchOptimizer)

#### 4.1 æŸ¥è¯¢æ‰©å±•ç®—æ³•
```python
class EnhancedSearchOptimizer:
    def __init__(self):
        self.synonym_dict = self._load_synonym_dict()
        self.related_terms = self._load_related_terms()
        
    def get_enhanced_search_queries(self, keyword, max_queries=5):
        """ç”Ÿæˆå¢å¼ºæœç´¢æŸ¥è¯¢"""
        queries = [keyword]  # åŸå§‹æŸ¥è¯¢
        
        # æ·»åŠ åŒä¹‰è¯å˜ä½“
        synonyms = self._get_synonyms(keyword)
        queries.extend(synonyms[:2])  # æœ€å¤š2ä¸ªåŒä¹‰è¯
        
        # æ·»åŠ ç›¸å…³æœ¯è¯­ç»„åˆ
        related = self._get_related_terms(keyword)
        for term in related[:2]:  # æœ€å¤š2ä¸ªç›¸å…³æœ¯è¯­
            queries.append(f"{keyword} {term}")
        
        # æ·»åŠ æ ¼å¼å˜ä½“
        format_variants = self._generate_format_variants(keyword)
        queries.extend(format_variants[:2])
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_queries = list(dict.fromkeys(queries))  # ä¿æŒé¡ºåºçš„å»é‡
        return unique_queries[:max_queries]
```

#### 4.2 æ™ºèƒ½æ»šåŠ¨ç­–ç•¥
```python
def optimize_scroll_strategy(self, current_tweets, target_tweets, scroll_attempts):
    """ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥"""
    progress_rate = current_tweets / target_tweets if target_tweets > 0 else 0
    efficiency = current_tweets / scroll_attempts if scroll_attempts > 0 else 0
    
    # åŸºç¡€ç­–ç•¥
    strategy = {
        'scroll_distance': 800,
        'wait_time': 2.0,
        'max_scrolls': 50,
        'aggressive_mode': False,
        'should_continue': True
    }
    
    # æ ¹æ®æ•ˆç‡è°ƒæ•´ç­–ç•¥
    if efficiency < 1.0:  # ä½æ•ˆç‡
        strategy.update({
            'scroll_distance': 1200,  # å¢åŠ æ»šåŠ¨è·ç¦»
            'wait_time': 3.0,         # å¢åŠ ç­‰å¾…æ—¶é—´
            'aggressive_mode': True   # å¯ç”¨æ¿€è¿›æ¨¡å¼
        })
    elif efficiency > 3.0:  # é«˜æ•ˆç‡
        strategy.update({
            'scroll_distance': 600,   # å‡å°‘æ»šåŠ¨è·ç¦»
            'wait_time': 1.5,        # å‡å°‘ç­‰å¾…æ—¶é—´
        })
    
    # æ ¹æ®è¿›åº¦è°ƒæ•´
    if progress_rate > 0.9:  # æ¥è¿‘ç›®æ ‡
        strategy['should_continue'] = False
    elif progress_rate < 0.1 and scroll_attempts > 20:  # è¿›åº¦ç¼“æ…¢
        strategy['aggressive_mode'] = True
    
    return strategy
```

### 5. æ™ºèƒ½æ»šåŠ¨å®ç° (twitter_parser.py)

#### 5.1 ä¼˜åŒ–æ»šåŠ¨å‡½æ•°
```python
async def scroll_and_load_tweets(self, page, target_count=50, max_scrolls=50):
    """ä¼˜åŒ–çš„æ»šåŠ¨åŠ è½½æ¨æ–‡"""
    current_tweets = 0
    scroll_attempts = 0
    consecutive_no_new = 0
    
    while current_tweets < target_count and scroll_attempts < max_scrolls:
        # è·å–å½“å‰æ¨æ–‡æ•°é‡
        tweets_before = await self._count_tweets(page)
        
        # è·å–ä¼˜åŒ–æ»šåŠ¨ç­–ç•¥
        strategy = self.search_optimizer.optimize_scroll_strategy(
            current_tweets, target_count, scroll_attempts
        )
        
        if not strategy['should_continue']:
            break
        
        # æ‰§è¡Œæ»šåŠ¨
        if strategy['aggressive_mode']:
            await self._aggressive_scroll(page, strategy)
        else:
            await self._normal_scroll(page, strategy)
        
        # ç­‰å¾…åŠ è½½
        await asyncio.sleep(strategy['wait_time'])
        
        # æ£€æŸ¥æ–°æ¨æ–‡
        tweets_after = await self._count_tweets(page)
        new_tweets = tweets_after - tweets_before
        
        if new_tweets > 0:
            current_tweets = tweets_after
            consecutive_no_new = 0
        else:
            consecutive_no_new += 1
        
        scroll_attempts += 1
        
        # å¦‚æœè¿ç»­å¤šæ¬¡æ— æ–°å†…å®¹ï¼Œå¯ç”¨äººå·¥è¡Œä¸ºæ¨¡æ‹Ÿå™¨
        if consecutive_no_new >= 3 and self.behavior_simulator:
            await self._fallback_to_behavior_simulator(page)
            consecutive_no_new = 0
    
    return current_tweets
```

#### 5.2 æ¿€è¿›æ»šåŠ¨æ¨¡å¼
```python
async def _aggressive_scroll(self, page, strategy):
    """æ¿€è¿›æ»šåŠ¨æ¨¡å¼ - å¤„ç†å†…å®¹åœæ»"""
    # å¤šæ¬¡å¿«é€Ÿæ»šåŠ¨
    for _ in range(3):
        await page.evaluate(f"window.scrollBy(0, {strategy['scroll_distance']})")
        await asyncio.sleep(0.5)
    
    # å°è¯•ç‚¹å‡»"æ˜¾ç¤ºæ›´å¤š"æŒ‰é’®
    try:
        show_more_button = await page.query_selector('[data-testid="showMoreButton"]')
        if show_more_button:
            await show_more_button.click()
            await asyncio.sleep(1)
    except:
        pass
    
    # éšæœºæ»šåŠ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸º
    import random
    random_scroll = random.randint(200, 400)
    await page.evaluate(f"window.scrollBy(0, {random_scroll})")
```

### 6. æ‰¹é‡å¤„ç†ä¼˜åŒ–å®ç°

#### 6.1 å¹¶å‘å¤„ç†æ¶æ„
```python
class BatchProcessor:
    def __init__(self, max_workers=3):
        self.max_workers = max_workers
        self.semaphore = asyncio.Semaphore(max_workers)
        
    async def process_batch(self, tasks):
        """å¹¶å‘æ‰¹é‡å¤„ç†"""
        async def process_single_task(task):
            async with self.semaphore:
                return await self._execute_task(task)
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        concurrent_tasks = [process_single_task(task) for task in tasks]
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*concurrent_tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœå’Œå¼‚å¸¸
        successful_results = []
        failed_tasks = []
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed_tasks.append((tasks[i], result))
            else:
                successful_results.append(result)
        
        return successful_results, failed_tasks
```

#### 6.2 å†…å­˜ä¼˜åŒ–å¤„ç†
```python
def process_large_dataset(self, data, chunk_size=1000):
    """å¤§æ•°æ®é›†åˆ†å—å¤„ç†"""
    results = []
    
    for i in range(0, len(data), chunk_size):
        chunk = data[i:i + chunk_size]
        
        # å¤„ç†å½“å‰å—
        chunk_results = self._process_chunk(chunk)
        results.extend(chunk_results)
        
        # å†…å­˜æ¸…ç†
        if i % (chunk_size * 10) == 0:  # æ¯10ä¸ªå—æ¸…ç†ä¸€æ¬¡
            gc.collect()
    
    return results
```

### 7. æ€§èƒ½ç›‘æ§å®ç°

#### 7.1 å®æ—¶æ€§èƒ½ç›‘æ§
```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'start_time': time.time(),
            'total_processed': 0,
            'success_count': 0,
            'error_count': 0,
            'processing_times': [],
            'memory_usage': [],
            'cpu_usage': []
        }
        
    def record_operation(self, operation_time, success=True):
        """è®°å½•æ“ä½œæ€§èƒ½"""
        self.metrics['total_processed'] += 1
        self.metrics['processing_times'].append(operation_time)
        
        if success:
            self.metrics['success_count'] += 1
        else:
            self.metrics['error_count'] += 1
        
        # è®°å½•ç³»ç»Ÿèµ„æºä½¿ç”¨
        self._record_system_metrics()
    
    def get_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        elapsed_time = time.time() - self.metrics['start_time']
        
        return {
            'processing_rate': self.metrics['total_processed'] / elapsed_time * 60,  # æ¯åˆ†é’Ÿå¤„ç†æ•°
            'success_rate': self.metrics['success_count'] / self.metrics['total_processed'] * 100,
            'average_processing_time': sum(self.metrics['processing_times']) / len(self.metrics['processing_times']),
            'memory_usage_avg': sum(self.metrics['memory_usage']) / len(self.metrics['memory_usage']),
            'cpu_usage_avg': sum(self.metrics['cpu_usage']) / len(self.metrics['cpu_usage'])
        }
```

### 8. é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

#### 8.1 æ™ºèƒ½é‡è¯•ç­–ç•¥
```python
class SmartRetryHandler:
    def __init__(self, max_retries=3, base_delay=1.0, max_delay=60.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
        
    async def execute_with_retry(self, func, *args, **kwargs):
        """å¸¦é‡è¯•çš„å‡½æ•°æ‰§è¡Œ"""
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                
                if attempt == self.max_retries:
                    break
                
                # è®¡ç®—å»¶è¿Ÿæ—¶é—´ (æŒ‡æ•°é€€é¿)
                delay = min(self.base_delay * (2 ** attempt), self.max_delay)
                
                # æ ¹æ®å¼‚å¸¸ç±»å‹è°ƒæ•´ç­–ç•¥
                if self._is_rate_limit_error(e):
                    delay *= 2  # é€Ÿç‡é™åˆ¶é”™è¯¯å»¶é•¿ç­‰å¾…
                elif self._is_network_error(e):
                    delay *= 1.5  # ç½‘ç»œé”™è¯¯é€‚å½“å»¶é•¿
                
                await asyncio.sleep(delay)
        
        raise last_exception
```

### 9. æ•°æ®æŒä¹…åŒ–ä¼˜åŒ–

#### 9.1 æ‰¹é‡æ•°æ®åº“æ“ä½œ
```python
class OptimizedDatabase:
    def __init__(self, db_path):
        self.db_path = db_path
        self.batch_size = 100
        self.pending_operations = []
        
    async def batch_insert(self, table, data_list):
        """æ‰¹é‡æ’å…¥æ•°æ®"""
        if not data_list:
            return
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(data_list), self.batch_size):
            batch = data_list[i:i + self.batch_size]
            
            # æ„å»ºæ‰¹é‡æ’å…¥SQL
            placeholders = ','.join(['?' * len(batch[0])] * len(batch))
            columns = ','.join(batch[0].keys())
            sql = f"INSERT OR REPLACE INTO {table} ({columns}) VALUES {placeholders}"
            
            # å±•å¹³æ•°æ®
            flat_data = []
            for item in batch:
                flat_data.extend(item.values())
            
            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            await self._execute_sql(sql, flat_data)
```

### 10. é…ç½®ç®¡ç†ä¼˜åŒ–

#### 10.1 åŠ¨æ€é…ç½®åŠ è½½
```python
class DynamicConfigManager:
    def __init__(self, config_file):
        self.config_file = config_file
        self.config = {}
        self.last_modified = 0
        self.watchers = []
        
    def get_config(self, key, default=None):
        """è·å–é…ç½®å€¼ï¼Œæ”¯æŒçƒ­é‡è½½"""
        self._check_and_reload()
        return self.config.get(key, default)
    
    def _check_and_reload(self):
        """æ£€æŸ¥æ–‡ä»¶å˜åŒ–å¹¶é‡æ–°åŠ è½½"""
        try:
            current_modified = os.path.getmtime(self.config_file)
            if current_modified > self.last_modified:
                self._reload_config()
                self.last_modified = current_modified
                self._notify_watchers()
        except OSError:
            pass
    
    def _reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(self.config_file, 'r', encoding='utf-8') as f:
            if self.config_file.endswith('.yaml'):
                import yaml
                self.config = yaml.safe_load(f)
            else:
                import json
                self.config = json.load(f)
```

## ğŸ” è°ƒè¯•å’Œè¯Šæ–­å·¥å…·

### 1. æ€§èƒ½åˆ†æå™¨
```python
class PerformanceProfiler:
    def __init__(self):
        self.function_times = {}
        self.call_counts = {}
        
    def profile(self, func):
        """å‡½æ•°æ€§èƒ½åˆ†æè£…é¥°å™¨"""
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                end_time = time.time()
                execution_time = end_time - start_time
                
                func_name = func.__name__
                if func_name not in self.function_times:
                    self.function_times[func_name] = []
                    self.call_counts[func_name] = 0
                
                self.function_times[func_name].append(execution_time)
                self.call_counts[func_name] += 1
        
        return wrapper
    
    def get_performance_summary(self):
        """è·å–æ€§èƒ½æ‘˜è¦"""
        summary = {}
        for func_name, times in self.function_times.items():
            summary[func_name] = {
                'total_time': sum(times),
                'average_time': sum(times) / len(times),
                'call_count': self.call_counts[func_name],
                'min_time': min(times),
                'max_time': max(times)
            }
        return summary
```

### 2. å†…å­˜ç›‘æ§å™¨
```python
class MemoryMonitor:
    def __init__(self):
        self.snapshots = []
        
    def take_snapshot(self, label=""):
        """è·å–å†…å­˜å¿«ç…§"""
        import psutil
        process = psutil.Process()
        memory_info = process.memory_info()
        
        snapshot = {
            'timestamp': time.time(),
            'label': label,
            'rss': memory_info.rss,  # ç‰©ç†å†…å­˜
            'vms': memory_info.vms,  # è™šæ‹Ÿå†…å­˜
            'percent': process.memory_percent()
        }
        
        self.snapshots.append(snapshot)
        return snapshot
    
    def detect_memory_leaks(self):
        """æ£€æµ‹å†…å­˜æ³„æ¼"""
        if len(self.snapshots) < 2:
            return None
        
        # è®¡ç®—å†…å­˜å¢é•¿è¶‹åŠ¿
        memory_growth = []
        for i in range(1, len(self.snapshots)):
            growth = self.snapshots[i]['rss'] - self.snapshots[i-1]['rss']
            memory_growth.append(growth)
        
        # æ£€æµ‹æŒç»­å¢é•¿
        if len(memory_growth) >= 5:
            recent_growth = memory_growth[-5:]
            if all(g > 0 for g in recent_growth):
                return {
                    'leak_detected': True,
                    'average_growth': sum(recent_growth) / len(recent_growth),
                    'total_growth': sum(memory_growth)
                }
        
        return {'leak_detected': False}
```

---

**æ€»ç»“**: æœ¬æŠ€æœ¯å®ç°æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†Twitteré‡‡é›†ç³»ç»Ÿå„ä¸ªæ ¸å¿ƒæ¨¡å—çš„å…·ä½“å®ç°æ–¹æ³•ï¼ŒåŒ…æ‹¬ç®—æ³•è®¾è®¡ã€ä»£ç å®ç°ã€æ€§èƒ½ä¼˜åŒ–å’Œè°ƒè¯•å·¥å…·ã€‚æ‰€æœ‰å®ç°éƒ½ç»è¿‡ä¼˜åŒ–ï¼Œç¡®ä¿é«˜æ€§èƒ½ã€é«˜å¯é æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚