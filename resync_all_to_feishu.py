#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sqlite3
import json
import logging
from enhanced_tweet_scraper import EnhancedTwitterScraper, load_feishu_config_from_file

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def resync_all_to_feishu():
    """é‡æ–°åŒæ­¥æ‰€æœ‰æ¨æ–‡åˆ°é£ä¹¦"""
    print("ğŸ”„ å¼€å§‹é‡æ–°åŒæ­¥æ‰€æœ‰æ¨æ–‡åˆ°é£ä¹¦...")
    
    # åŠ è½½é£ä¹¦é…ç½®
    feishu_config = load_feishu_config_from_file()
    if not feishu_config:
        print("âŒ æ— æ³•åŠ è½½é£ä¹¦é…ç½®")
        return
    
    if not feishu_config.get('enabled'):
        print("âŒ é£ä¹¦åŒæ­¥æœªå¯ç”¨ï¼Œè¯·å…ˆåœ¨é…ç½®ä¸­å¯ç”¨")
        return
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå ä½ç¬¦é…ç½®
    is_placeholder = (
        feishu_config.get('app_id') == 'your_feishu_app_id' or
        feishu_config.get('spreadsheet_token') == 'your_spreadsheet_token'
    )
    
    if is_placeholder:
        print("âŒ æ£€æµ‹åˆ°å ä½ç¬¦é…ç½®ï¼Œè¯·å…ˆé…ç½®æœ‰æ•ˆçš„é£ä¹¦å‚æ•°")
        return
    
    print(f"âœ… é£ä¹¦é…ç½®åŠ è½½æˆåŠŸï¼Œå¯ç”¨çŠ¶æ€: {feishu_config.get('enabled')}")
    
    # ç»Ÿè®¡æ•°æ®åº“çŠ¶æ€
    conn = sqlite3.connect('twitter_scraper.db')
    cursor = conn.cursor()
    
    cursor.execute("SELECT COUNT(*) FROM tweet_data")
    total_count = cursor.fetchone()[0]
    
    cursor.execute("""
        SELECT 
            COUNT(CASE WHEN publish_time IS NOT NULL AND publish_time != '' THEN 1 END) as has_publish_time,
            COUNT(CASE WHEN scraped_at IS NOT NULL THEN 1 END) as has_scraped_at
        FROM tweet_data
    """)
    
    stats = cursor.fetchone()
    has_publish_time, has_scraped_at = stats
    
    print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
    print(f"  æ€»æ¨æ–‡æ•°: {total_count}")
    print(f"  æœ‰å‘å¸ƒæ—¶é—´: {has_publish_time} ({has_publish_time/total_count*100:.1f}%)")
    print(f"  æœ‰æŠ“å–æ—¶é—´: {has_scraped_at} ({has_scraped_at/total_count*100:.1f}%)")
    
    # é‡ç½®æ‰€æœ‰æ¨æ–‡çš„åŒæ­¥çŠ¶æ€
    cursor.execute("UPDATE tweet_data SET synced_to_feishu = 0")
    conn.commit()
    conn.close()
    
    print("âœ… å·²é‡ç½®æ‰€æœ‰æ¨æ–‡çš„åŒæ­¥çŠ¶æ€")
    
    # åˆå§‹åŒ–æŠ“å–å™¨å¹¶æ‰§è¡ŒåŒæ­¥
    scraper = EnhancedTwitterScraper()
    
    print("\nğŸš€ å¼€å§‹åŒæ­¥åˆ°é£ä¹¦...")
    success = scraper.sync_to_feishu(feishu_config)
    
    if success:
        # æ£€æŸ¥åŒæ­¥ç»“æœ
        conn = sqlite3.connect('twitter_scraper.db')
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM tweet_data WHERE synced_to_feishu = 1")
        synced_count = cursor.fetchone()[0]
        conn.close()
        
        print(f"\nâœ… åŒæ­¥æˆåŠŸ! {synced_count}/{total_count} æ¡æ¨æ–‡å·²åŒæ­¥åˆ°é£ä¹¦")
        print(f"  - å…¶ä¸­ {has_publish_time} æ¡ä½¿ç”¨åŸå§‹å‘å¸ƒæ—¶é—´")
        print(f"  - {total_count - has_publish_time} æ¡ä½¿ç”¨æŠ“å–æ—¶é—´ä½œä¸ºå‘å¸ƒæ—¶é—´")
        
        print("\nğŸ¯ ä¿®å¤æ•ˆæœ:")
        print("  âœ… æ‰€æœ‰æ¨æ–‡ç°åœ¨åœ¨é£ä¹¦ä¸­éƒ½æœ‰æœ‰æ•ˆçš„æ—¶é—´å­—æ®µ")
        print("  âœ… é£ä¹¦è¡¨æ ¼ä¸­çš„æ—¶é—´æ˜¾ç¤ºåº”è¯¥å·²æ¢å¤æ­£å¸¸")
    else:
        print("\nâŒ åŒæ­¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é£ä¹¦é…ç½®å’Œç½‘ç»œè¿æ¥")

if __name__ == "__main__":
    resync_all_to_feishu()