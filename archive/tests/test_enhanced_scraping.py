
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¿®å¤åçš„æ¨æ–‡æŠ“å–åŠŸèƒ½
"""

import asyncio
import logging
from twitter_parser import TwitterParser

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_enhanced_scraping():
    """æµ‹è¯•å¢å¼ºçš„æ¨æ–‡æŠ“å–"""
    parser = None
    try:
        logger.info("å¼€å§‹æµ‹è¯•å¢å¼ºæ¨æ–‡æŠ“å–...")
        
        # åˆ›å»ºè§£æå™¨
        parser = TwitterParser()
        await parser.initialize(debug_port="ws://localhost:9222")
        
        # æµ‹è¯•ç”¨æˆ·æ¨æ–‡æŠ“å–
        test_username = "socialmedia2day"
        target_tweets = 50
        
        logger.info(f"æµ‹è¯•æŠ“å–ç”¨æˆ· @{test_username} çš„ {target_tweets} æ¡æ¨æ–‡")
        
        tweets = await parser.scrape_user_tweets(test_username, target_tweets)
        
        logger.info(f"æŠ“å–ç»“æœ: ç›®æ ‡ {target_tweets} æ¡ï¼Œå®é™…è·å¾— {len(tweets)} æ¡")
        
        if len(tweets) < target_tweets:
            shortage = target_tweets - len(tweets)
            logger.warning(f"ä»ç„¶å­˜åœ¨æ•°é‡ä¸è¶³é—®é¢˜ï¼Œç¼ºå°‘ {shortage} æ¡æ¨æ–‡")
        else:
            logger.info("æŠ“å–æ•°é‡è¾¾åˆ°ç›®æ ‡ï¼")
        
        # åˆ†ææ¨æ–‡è´¨é‡
        if hasattr(parser, 'detect_tweet_quality'):
            quality_stats = {'high': 0, 'medium': 0, 'low': 0, 'unknown': 0}
            for tweet in tweets:
                tweet = parser.detect_tweet_quality(tweet)
                quality_level = tweet.get('quality_level', 'unknown')
                quality_stats[quality_level] += 1
            
            logger.info(f"æ¨æ–‡è´¨é‡åˆ†å¸ƒ: {quality_stats}")
        
        # æ˜¾ç¤ºå‰5æ¡æ¨æ–‡çš„è¯¦ç»†ä¿¡æ¯
        logger.info("å‰5æ¡æ¨æ–‡è¯¦æƒ…:")
        for i, tweet in enumerate(tweets[:5], 1):
            logger.info(f"æ¨æ–‡ {i}:")
            logger.info(f"  ç”¨æˆ·: @{tweet.get('username', 'unknown')}")
            logger.info(f"  å†…å®¹: {tweet.get('content', 'No content')[:100]}...")
            logger.info(f"  é“¾æ¥: {tweet.get('link', 'No link')}")
            logger.info(f"  äº’åŠ¨: ğŸ‘{tweet.get('likes', 0)} ğŸ’¬{tweet.get('comments', 0)} ğŸ”„{tweet.get('retweets', 0)}")
            if 'quality_score' in tweet:
                logger.info(f"  è´¨é‡: {tweet['quality_level']} ({tweet['quality_score']}/100)")
            logger.info("")
        
        return len(tweets)
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        return 0
    finally:
        if parser:
            await parser.close()

if __name__ == "__main__":
    result = asyncio.run(test_enhanced_scraping())
    print(f"\næµ‹è¯•å®Œæˆï¼Œå…±æŠ“å– {result} æ¡æ¨æ–‡")
