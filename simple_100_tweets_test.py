#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆ100æ¡æ¨æ–‡æŠ“å–æµ‹è¯•
ä»ä¸Šåˆ°ä¸‹æŠ“å–100æ¡å†…å®¹ï¼Œæ”¾å®½éªŒè¯æ¡ä»¶
"""

import asyncio
import logging
import json
from datetime import datetime
from typing import Dict, Any, List

from twitter_parser import TwitterParser
from ads_browser_launcher import AdsPowerLauncher
from config import ADS_POWER_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Simple100TweetsTester:
    """ç®€åŒ–ç‰ˆ100æ¡æ¨æ–‡æµ‹è¯•å™¨"""
    
    def __init__(self, target_count: int = 100):
        self.launcher = None
        self.parser = None
        self.scraped_tweets = []
        self.target_count = target_count
        
    async def scrape_100_tweets(self) -> List[Dict[str, Any]]:
        """æŠ“å–100æ¡æ¨æ–‡"""
        logger.info("ğŸš€ å¼€å§‹æŠ“å–100æ¡æ¨æ–‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰...")
        
        try:
            # å¯åŠ¨æµè§ˆå™¨
            self.launcher = AdsPowerLauncher(config=ADS_POWER_CONFIG)
            browser_info = self.launcher.start_browser()
            
            if not browser_info:
                logger.error("æµè§ˆå™¨å¯åŠ¨å¤±è´¥")
                return []
            
            # åˆ›å»ºè§£æå™¨
            debug_url = browser_info.get('ws', {}).get('puppeteer', '')
            if not debug_url:
                logger.error("æ— æ³•è·å–è°ƒè¯•ç«¯å£")
                return []
            
            self.parser = TwitterParser(debug_port=debug_url)
            
            # åˆå§‹åŒ–è§£æå™¨
            await self.parser.initialize()
            logger.info("âœ… æµè§ˆå™¨è¿æ¥æˆåŠŸ")
            
            # å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
            try:
                await self.parser.navigate_to_profile('elonmusk')
                logger.info("âœ… å¯¼èˆªåˆ° elonmusk é¡µé¢æˆåŠŸ")
            except Exception as e:
                logger.error(f"å¯¼èˆªå¤±è´¥: {e}")
                return []
            
            # å¯ç”¨ä¼˜åŒ–åŠŸèƒ½
            self.parser.optimization_enabled = True
            logger.info(f"ä¼˜åŒ–åŠŸèƒ½çŠ¶æ€: {self.parser.optimization_enabled}")
            
            # ä½¿ç”¨ç®€åŒ–çš„æŠ“å–ç­–ç•¥
            tweets = await self.simple_scrape_strategy()
            
            logger.info(f"âœ… æŠ“å–å®Œæˆï¼Œå…±è·å¾— {len(tweets)} æ¡æ¨æ–‡ (ç›®æ ‡: {self.target_count})")
            return tweets
                
        except Exception as e:
            logger.error(f"æŠ“å–è¿‡ç¨‹å¼‚å¸¸: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    async def simple_scrape_strategy(self) -> List[Dict[str, Any]]:
        """ç®€åŒ–çš„æŠ“å–ç­–ç•¥"""
        tweets = []
        scroll_count = 0
        no_new_tweets_count = 0  # è¿ç»­æ²¡æœ‰æ–°æ¨æ–‡çš„æ¬¡æ•°
        
        logger.info("å¼€å§‹ç®€åŒ–æŠ“å–ç­–ç•¥...")
        
        # å…ˆç­‰å¾…é¡µé¢åˆå§‹åŠ è½½
        try:
            await asyncio.sleep(3)  # ç»™é¡µé¢æ›´å¤šæ—¶é—´åŠ è½½
            logger.info("é¡µé¢åˆå§‹åŠ è½½ç­‰å¾…å®Œæˆ")
        except:
            pass
        
        while len(tweets) < self.target_count:
            try:
                # ä¸ç­‰å¾…ç½‘ç»œç©ºé—²ï¼Œç›´æ¥æŸ¥æ‰¾å…ƒç´ 
                logger.info(f"ç¬¬ {scroll_count + 1} æ¬¡å°è¯•ï¼Œå½“å‰æ¨æ–‡æ•°: {len(tweets)}/{self.target_count}")
                
                # æŸ¥æ‰¾æ¨æ–‡å…ƒç´ 
                tweet_elements = await self.parser.page.query_selector_all('[data-testid="tweet"]')
                logger.info(f"ç¬¬ {scroll_count + 1} æ¬¡æ»šåŠ¨ï¼Œæ‰¾åˆ° {len(tweet_elements)} ä¸ªæ¨æ–‡å…ƒç´ ")
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¨æ–‡å…ƒç´ ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
                if not tweet_elements:
                    alternative_selectors = [
                        'article[data-testid="tweet"]',
                        '[data-testid="cellInnerDiv"]',
                        'div[data-testid="tweet"]'
                    ]
                    
                    for selector in alternative_selectors:
                        tweet_elements = await self.parser.page.query_selector_all(selector)
                        if tweet_elements:
                            logger.info(f"ä½¿ç”¨å¤‡ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(tweet_elements)} ä¸ªå…ƒç´ ")
                            break
                
                # è®°å½•è§£æå‰çš„æ¨æ–‡æ•°é‡
                tweets_before = len(tweets)
                
                # è§£ææ¨æ–‡å…ƒç´ 
                for element in tweet_elements:
                    if len(tweets) >= self.target_count:
                        break
                    
                    try:
                        # ä½¿ç”¨ç®€åŒ–çš„è§£ææ–¹æ³•
                        tweet_data = await self.simple_parse_tweet(element)
                        if tweet_data:
                            # æ£€æŸ¥æ˜¯å¦é‡å¤
                            link = tweet_data.get('link', '')
                            if link and not any(t.get('link') == link for t in tweets):
                                tweets.append(tweet_data)
                                logger.info(f"æˆåŠŸè§£æç¬¬ {len(tweets)} æ¡æ¨æ–‡: {tweet_data.get('username', 'unknown')}")
                    
                    except Exception as e:
                        logger.debug(f"è§£æå•æ¡æ¨æ–‡å¤±è´¥: {e}")
                        continue
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡
                new_tweets_count = len(tweets) - tweets_before
                if new_tweets_count == 0:
                    no_new_tweets_count += 1
                    logger.warning(f"æœ¬æ¬¡æ»šåŠ¨æ²¡æœ‰è·å¾—æ–°æ¨æ–‡ï¼Œè¿ç»­ {no_new_tweets_count} æ¬¡")
                else:
                    no_new_tweets_count = 0  # é‡ç½®è®¡æ•°å™¨
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æˆ–éœ€è¦åœæ­¢
                if len(tweets) >= self.target_count:
                    logger.info(f"âœ… å·²è¾¾åˆ°ç›®æ ‡æ¨æ–‡æ•°é‡: {len(tweets)}/{self.target_count}")
                    break
                
                # å¦‚æœè¿ç»­å¤šæ¬¡æ²¡æœ‰æ–°æ¨æ–‡ï¼Œåœæ­¢æ»šåŠ¨ï¼ˆæ”¾å®½æ¡ä»¶ï¼‰
                if no_new_tweets_count >= 8:  # å¢åŠ åˆ°8æ¬¡ï¼Œç»™æ›´å¤šæœºä¼š
                    logger.warning(f"è¿ç»­ {no_new_tweets_count} æ¬¡æ»šåŠ¨éƒ½æ²¡æœ‰æ–°æ¨æ–‡ï¼Œåœæ­¢æŠ“å–")
                    break
                
                # æ»šåŠ¨é¡µé¢ï¼ˆå¢åŠ æ»šåŠ¨è·ç¦»ï¼‰
                await self.parser.page.evaluate('window.scrollBy(0, 1500)')  # å¢åŠ æ»šåŠ¨è·ç¦»
                await asyncio.sleep(4)  # å¢åŠ ç­‰å¾…æ—¶é—´
                scroll_count += 1
                logger.info(f"å·²æ»šåŠ¨ {scroll_count} æ¬¡ï¼Œå½“å‰æ¨æ–‡æ•°: {len(tweets)}/{self.target_count}")
                
                # æ¯10æ¬¡æ»šåŠ¨åé¢å¤–ç­‰å¾…ï¼Œè®©é¡µé¢å……åˆ†åŠ è½½
                if scroll_count % 10 == 0:
                    logger.info(f"ç¬¬ {scroll_count} æ¬¡æ»šåŠ¨ï¼Œé¢å¤–ç­‰å¾…é¡µé¢åŠ è½½...")
                    await asyncio.sleep(5)
                
            except Exception as e:
                logger.error(f"æ»šåŠ¨è¿‡ç¨‹å¼‚å¸¸: {e}")
                break
        
        logger.info(f"æŠ“å–ç­–ç•¥å®Œæˆï¼Œå…±è·å¾— {len(tweets)} æ¡æ¨æ–‡")
        return tweets
    
    async def simple_parse_tweet(self, element) -> Dict[str, Any]:
        """ç®€åŒ–çš„æ¨æ–‡è§£ææ–¹æ³•"""
        try:
            # æå–ç”¨æˆ·å
            username = 'unknown'
            try:
                username_element = await element.query_selector('[data-testid="User-Name"] span')
                if username_element:
                    username_text = await username_element.text_content()
                    if username_text:
                        username = username_text.strip().split()[0]  # å–ç¬¬ä¸€ä¸ªè¯
            except:
                pass
            
            # æå–å†…å®¹
            content = 'No content available'
            try:
                content_element = await element.query_selector('[data-testid="tweetText"]')
                if content_element:
                    content_text = await content_element.text_content()
                    if content_text:
                        content = content_text.strip()
            except:
                pass
            
            # æå–é“¾æ¥
            link = ''
            try:
                link_element = await element.query_selector('a[href*="/status/"]')
                if link_element:
                    href = await link_element.get_attribute('href')
                    if href:
                        if href.startswith('/'):
                            link = f'https://x.com{href}'
                        else:
                            link = href
            except:
                pass
            
            # æå–æ—¶é—´
            publish_time = ''
            try:
                time_element = await element.query_selector('time')
                if time_element:
                    datetime_attr = await time_element.get_attribute('datetime')
                    if datetime_attr:
                        publish_time = datetime_attr
            except:
                pass
            
            # æå–äº’åŠ¨æ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            likes = 0
            comments = 0
            retweets = 0
            
            try:
                # æŸ¥æ‰¾ç‚¹èµæ•°
                like_element = await element.query_selector('[data-testid="like"]')
                if like_element:
                    like_text = await like_element.text_content()
                    if like_text and any(c.isdigit() for c in like_text):
                        import re
                        numbers = re.findall(r'[\d,]+', like_text)
                        if numbers:
                            likes = int(numbers[0].replace(',', ''))
            except:
                pass
            
            # æ„å»ºæ¨æ–‡æ•°æ®
            tweet_data = {
                'username': username,
                'content': content,
                'publish_time': publish_time,
                'link': link,
                'likes': likes,
                'comments': comments,
                'retweets': retweets,
                'media': {'images': [], 'videos': []},
                'post_type': 'çº¯æ–‡æœ¬'
            }
            
            # ç®€åŒ–çš„éªŒè¯ï¼šåªè¦æœ‰ç”¨æˆ·åæˆ–å†…å®¹æˆ–é“¾æ¥å°±è®¤ä¸ºæœ‰æ•ˆ
            if (username and username != 'unknown') or \
               (content and content != 'No content available') or \
               (link and link.strip()):
                return tweet_data
            
            return None
            
        except Exception as e:
            logger.debug(f"ç®€åŒ–è§£ææ¨æ–‡å¤±è´¥: {e}")
            return None
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.parser:
                await self.parser.close()
            if self.launcher:
                self.launcher.stop_browser()
            logger.info("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")
    
    def save_results(self, tweets: List[Dict[str, Any]]):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'simple_100_tweets_{timestamp}.json'
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'target_tweets': 100,
            'actual_tweets': len(tweets),
            'success': len(tweets) > 0,
            'tweets': tweets
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")

async def main(target_count: int = 100):
    """ä¸»å‡½æ•°"""
    tester = Simple100TweetsTester(target_count)
    
    try:
        # æŠ“å–100æ¡æ¨æ–‡
        tweets = await tester.scrape_100_tweets()
        
        # ä¿å­˜ç»“æœ
        tester.save_results(tweets)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*50)
        print("ğŸ“Š ç®€åŒ–ç‰ˆæ¨æ–‡æŠ“å–æ‘˜è¦")
        print("="*50)
        print(f"ç›®æ ‡æ¨æ–‡æ•°: {tester.target_count}")
        print(f"å®é™…æŠ“å–æ•°: {len(tweets)}")
        print(f"å®Œæˆç‡: {len(tweets)/tester.target_count*100:.1f}%")
        print(f"æŠ“å–çŠ¶æ€: {'âœ… æˆåŠŸ' if len(tweets) > 0 else 'âŒ å¤±è´¥'}")
        
        if tweets:
            print(f"\nğŸ“ å‰3æ¡æ¨æ–‡é¢„è§ˆ:")
            for i, tweet in enumerate(tweets[:3], 1):
                print(f"  {i}. {tweet.get('username', 'unknown')}: {tweet.get('content', 'No content')[:50]}...")
        
        print("="*50)
        
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    finally:
        await tester.cleanup()

if __name__ == '__main__':
    import sys
    
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šç›®æ ‡æ•°é‡
    target_count = 100
    if len(sys.argv) > 1:
        try:
            target_count = int(sys.argv[1])
            print(f"ğŸ“‹ è®¾ç½®ç›®æ ‡æ¨æ–‡æ•°é‡: {target_count}")
        except ValueError:
            print("âš ï¸ æ— æ•ˆçš„ç›®æ ‡æ•°é‡å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ 100")
    
    asyncio.run(main(target_count))