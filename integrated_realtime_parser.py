#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆçš„å®æ—¶è§£æå™¨
å°†å®æ—¶è§£æé€»è¾‘é›†æˆåˆ°çœŸå®çš„Twitterè§£æå™¨ä¸­
è§£å†³æ»šåŠ¨è®¡æ•°ä¸å®é™…å…ƒç´ ä¸åŒ¹é…çš„é—®é¢˜
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional, Set
from datetime import datetime
import json

from twitter_parser import TwitterParser
from ads_browser_launcher import AdsPowerLauncher
from config import ADS_POWER_CONFIG

class IntegratedRealtimeParser:
    """é›†æˆçš„å®æ—¶è§£æå™¨ï¼Œæ”¯æŒçœŸå®ç¯å¢ƒä¸‹çš„å®æ—¶è§£æ"""
    
    def __init__(self, parser: TwitterParser):
        self.parser = parser
        self.logger = logging.getLogger(__name__)
        
        # å®æ—¶è§£æçŠ¶æ€
        self.parsed_tweets: List[Dict[str, Any]] = []
        self.seen_tweet_ids: Set[str] = set()
        self.parsing_stats = {
            'total_scrolls': 0,
            'tweets_parsed': 0,
            'duplicates_skipped': 0,
            'parsing_errors': 0,
            'incremental_saves': 0,
            'dom_elements_found': 0,
            'dom_elements_parsed': 0
        }
        
        # å®æ—¶è§£æé…ç½®
        self.incremental_save_interval = 5  # æ¯5æ¡æ¨æ–‡ä¿å­˜ä¸€æ¬¡
        self.max_parse_attempts_per_element = 3
    
    async def scroll_and_parse_realtime_integrated(self, target_tweets: int = 30, max_attempts: int = 50) -> Dict[str, Any]:
        """é›†æˆçš„å®æ—¶æ»šåŠ¨å’Œè§£ææ¨æ–‡"""
        self.logger.info(f"ğŸš€ å¼€å§‹é›†æˆå®æ—¶æ»šåŠ¨è§£æï¼Œç›®æ ‡: {target_tweets} æ¡æ¨æ–‡")
        
        scroll_attempt = 0
        stagnant_rounds = 0
        last_parsed_count = 0
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°
        base_scroll_distance = 800
        base_wait_time = 1.5
        
        # åˆå§‹çŠ¶æ€æ£€æŸ¥
        await self._check_initial_state()
        
        while scroll_attempt < max_attempts and len(self.parsed_tweets) < target_tweets:
            self.parsing_stats['total_scrolls'] = scroll_attempt + 1
            
            self.logger.info(f"ğŸ“Š æ»šåŠ¨å°è¯• {scroll_attempt + 1}/{max_attempts}ï¼Œå·²è§£æ: {len(self.parsed_tweets)}/{target_tweets}")
            
            # è·å–å½“å‰é¡µé¢çš„æ¨æ–‡å…ƒç´ 
            current_elements = await self._get_current_tweet_elements()
            
            if current_elements:
                self.parsing_stats['dom_elements_found'] += len(current_elements)
                self.logger.debug(f"âœ… æ‰¾åˆ° {len(current_elements)} ä¸ªæ¨æ–‡å…ƒç´ ")
                
                # å®æ—¶è§£ææ–°å‡ºç°çš„æ¨æ–‡
                new_tweets_parsed = await self._parse_elements_realtime(current_elements)
                
                if new_tweets_parsed > 0:
                    self.logger.info(f"âœ… æœ¬è½®è§£æäº† {new_tweets_parsed} æ¡æ–°æ¨æ–‡ï¼Œæ€»è®¡: {len(self.parsed_tweets)}")
                    stagnant_rounds = 0
                    
                    # å¢é‡ä¿å­˜æ£€æŸ¥
                    if len(self.parsed_tweets) % self.incremental_save_interval == 0:
                        await self._incremental_save()
                else:
                    stagnant_rounds += 1
                    self.logger.debug(f"âš ï¸ æœ¬è½®æœªå‘ç°æ–°æ¨æ–‡ï¼Œåœæ»è½®æ•°: {stagnant_rounds}")
            else:
                stagnant_rounds += 1
                self.logger.warning(f"âŒ æœªæ‰¾åˆ°æ¨æ–‡å…ƒç´ ï¼Œåœæ»è½®æ•°: {stagnant_rounds}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if len(self.parsed_tweets) >= target_tweets:
                self.logger.info(f"ğŸ¯ è¾¾åˆ°ç›®æ ‡æ¨æ–‡æ•°é‡: {len(self.parsed_tweets)}")
                break
            
            # æ£€æŸ¥åœæ»æƒ…å†µ
            if len(self.parsed_tweets) == last_parsed_count:
                stagnant_rounds += 1
            else:
                stagnant_rounds = 0
            
            last_parsed_count = len(self.parsed_tweets)
            
            # æ ¹æ®åœæ»æƒ…å†µè°ƒæ•´æ»šåŠ¨ç­–ç•¥
            scroll_distance, wait_time = self._adjust_scroll_strategy(stagnant_rounds, base_scroll_distance, base_wait_time)
            
            # æ‰§è¡Œæ»šåŠ¨
            await self._perform_scroll(scroll_distance, wait_time)
            
            # å¤„ç†é•¿æ—¶é—´åœæ»
            if stagnant_rounds >= 8:
                await self._handle_long_stagnation()
                stagnant_rounds = 0
            
            scroll_attempt += 1
        
        # æœ€ç»ˆä¿å­˜
        final_file = await self._final_save()
        
        # ç”Ÿæˆç»“æœæ‘˜è¦
        result = {
            'parsed_tweets_count': len(self.parsed_tweets),
            'target_tweets': target_tweets,
            'scroll_attempts': scroll_attempt,
            'target_reached': len(self.parsed_tweets) >= target_tweets,
            'efficiency': len(self.parsed_tweets) / max(scroll_attempt, 1),
            'parsing_stats': self.parsing_stats.copy(),
            'parsed_tweets': self.parsed_tweets.copy(),
            'final_save_file': final_file,
            'dom_parsing_ratio': self.parsing_stats['dom_elements_parsed'] / max(self.parsing_stats['dom_elements_found'], 1)
        }
        
        self.logger.info(f"ğŸ“Š é›†æˆå®æ—¶è§£æå®Œæˆ: {len(self.parsed_tweets)} æ¡æ¨æ–‡ï¼Œ{scroll_attempt} æ¬¡æ»šåŠ¨")
        self.logger.info(f"ğŸ“Š DOMè§£ææ¯”ç‡: {result['dom_parsing_ratio']:.2%} ({self.parsing_stats['dom_elements_parsed']}/{self.parsing_stats['dom_elements_found']})")
        
        return result
    
    async def _check_initial_state(self):
        """æ£€æŸ¥åˆå§‹çŠ¶æ€"""
        self.logger.info(f"ğŸ“Š åˆå§‹çŠ¶æ€æ£€æŸ¥...")
        try:
            initial_elements = await self.parser.page.query_selector_all('[data-testid="tweet"]')
            self.logger.info(f"ğŸ“Š é¡µé¢åˆå§‹æ¨æ–‡å…ƒç´ æ•°é‡: {len(initial_elements)}")
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
            page_title = await self.parser.page.title()
            current_url = self.parser.page.url
            self.logger.info(f"ğŸ“Š å½“å‰é¡µé¢: {page_title} - {current_url}")
            
        except Exception as e:
            self.logger.warning(f"åˆå§‹çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
    
    async def _get_current_tweet_elements(self) -> List:
        """è·å–å½“å‰é¡µé¢çš„æ¨æ–‡å…ƒç´ """
        try:
            # ç­‰å¾…æ¨æ–‡å…ƒç´ å‡ºç°
            await self.parser.page.wait_for_selector('[data-testid="tweet"]', timeout=5000)
            
            # å°è¯•å¤šä¸ªé€‰æ‹©å™¨
            selectors = [
                '[data-testid="tweet"]',
                'article[data-testid="tweet"]',
                '[data-testid="tweetText"]'
            ]
            
            all_elements = []
            for selector in selectors:
                try:
                    elements = await self.parser.page.query_selector_all(selector)
                    if elements:
                        all_elements.extend(elements)
                        self.logger.debug(f"âœ… é€‰æ‹©å™¨ {selector} æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                except Exception as e:
                    self.logger.debug(f"é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
            
            # å»é‡ï¼ˆåŸºäºå…ƒç´ ä½ç½®ï¼‰
            unique_elements = []
            seen_positions = set()
            
            for element in all_elements:
                try:
                    box = await element.bounding_box()
                    if box:
                        position = (int(box['x']), int(box['y']))
                        if position not in seen_positions:
                            unique_elements.append(element)
                            seen_positions.add(position)
                except Exception:
                    # å¦‚æœæ— æ³•è·å–ä½ç½®ï¼Œä»ç„¶åŒ…å«è¯¥å…ƒç´ 
                    unique_elements.append(element)
            
            self.logger.debug(f"ğŸ“Š å»é‡åæ¨æ–‡å…ƒç´ : {len(unique_elements)} ä¸ª")
            return unique_elements
            
        except Exception as e:
            self.logger.warning(f"è·å–æ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return []
    
    async def _parse_elements_realtime(self, elements: List) -> int:
        """å®æ—¶è§£ææ¨æ–‡å…ƒç´ """
        new_tweets_count = 0
        
        for element in elements:
            try:
                # å°è¯•è§£ææ¨æ–‡
                parsed_tweet = await self._parse_single_element_safe(element)
                
                if parsed_tweet:
                    tweet_id = parsed_tweet.get('id') or parsed_tweet.get('tweet_id')
                    
                    if tweet_id and tweet_id not in self.seen_tweet_ids:
                        # æ·»åŠ è§£ææ—¶é—´æˆ³
                        parsed_tweet['parsed_at'] = datetime.now().isoformat()
                        parsed_tweet['parsing_method'] = 'realtime_integrated'
                        
                        self.parsed_tweets.append(parsed_tweet)
                        self.seen_tweet_ids.add(tweet_id)
                        self.parsing_stats['tweets_parsed'] += 1
                        self.parsing_stats['dom_elements_parsed'] += 1
                        new_tweets_count += 1
                        
                        self.logger.info(f"âœ… å®æ—¶è§£ææ–°æ¨æ–‡: @{parsed_tweet.get('username', 'unknown')} - {tweet_id[:8] if tweet_id else 'no_id'}... - {parsed_tweet.get('content', '')[:30]}...")
                    
                    elif tweet_id in self.seen_tweet_ids:
                        self.parsing_stats['duplicates_skipped'] += 1
                        self.logger.debug(f"â­ï¸ è·³è¿‡é‡å¤æ¨æ–‡: {tweet_id[:8] if tweet_id else 'no_id'}...")
                
            except Exception as e:
                self.parsing_stats['parsing_errors'] += 1
                self.logger.debug(f"è§£ææ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
                continue
        
        return new_tweets_count
    
    async def _parse_single_element_safe(self, element) -> Optional[Dict[str, Any]]:
        """å®‰å…¨åœ°è§£æå•ä¸ªæ¨æ–‡å…ƒç´ """
        for attempt in range(self.max_parse_attempts_per_element):
            try:
                # ä½¿ç”¨ä¼˜åŒ–çš„è§£ææ–¹æ³•
                if hasattr(self.parser, 'parse_tweet_element_optimized'):
                    result = await self.parser.parse_tweet_element_optimized(element)
                elif hasattr(self.parser, 'parse_tweet_element'):
                    result = await self.parser.parse_tweet_element(element)
                else:
                    # åŸºç¡€è§£ææ–¹æ³•
                    result = await self._basic_parse_element(element)
                
                if result and (result.get('content') or result.get('id')):
                    return result
                
            except Exception as e:
                self.logger.debug(f"è§£æå°è¯• {attempt + 1} å¤±è´¥: {e}")
                if attempt < self.max_parse_attempts_per_element - 1:
                    await asyncio.sleep(0.1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
        
        return None
    
    async def _basic_parse_element(self, element) -> Dict[str, Any]:
        """åŸºç¡€çš„æ¨æ–‡å…ƒç´ è§£æ"""
        result = {}
        
        try:
            # æå–æ¨æ–‡é“¾æ¥å’ŒID
            link_element = await element.query_selector('a[href*="/status/"]')
            if link_element:
                href = await link_element.get_attribute('href')
                if href and '/status/' in href:
                    tweet_id = href.split('/status/')[-1].split('?')[0]
                    result['id'] = tweet_id
            
            # æå–ç”¨æˆ·å
            username_element = await element.query_selector('[data-testid="User-Name"] a')
            if username_element:
                username_href = await username_element.get_attribute('href')
                if username_href:
                    username = username_href.strip('/').split('/')[-1]
                    result['username'] = username
            
            # æå–æ¨æ–‡å†…å®¹
            content_element = await element.query_selector('[data-testid="tweetText"]')
            if content_element:
                content = await content_element.inner_text()
                result['content'] = content.strip() if content else ''
            
            # æå–æ—¶é—´æˆ³
            time_element = await element.query_selector('time')
            if time_element:
                datetime_attr = await time_element.get_attribute('datetime')
                if datetime_attr:
                    result['timestamp'] = datetime_attr
            
            return result
            
        except Exception as e:
            self.logger.debug(f"åŸºç¡€è§£æå¤±è´¥: {e}")
            return {}
    
    def _adjust_scroll_strategy(self, stagnant_rounds: int, base_distance: int, base_wait: float) -> tuple:
        """æ ¹æ®åœæ»æƒ…å†µè°ƒæ•´æ»šåŠ¨ç­–ç•¥"""
        if stagnant_rounds >= 6:
            # è¶…æ¿€è¿›æ¨¡å¼
            distance = base_distance * 3
            wait_time = base_wait * 0.5
            self.logger.debug(f"âš¡ è¶…æ¿€è¿›æ¨¡å¼ï¼šæ»šåŠ¨è·ç¦» {distance}ï¼Œç­‰å¾…æ—¶é—´ {wait_time:.1f}s")
        elif stagnant_rounds >= 3:
            # æ¿€è¿›æ¨¡å¼
            distance = base_distance * 2
            wait_time = base_wait * 0.7
            self.logger.debug(f"ğŸ”¥ æ¿€è¿›æ¨¡å¼ï¼šæ»šåŠ¨è·ç¦» {distance}ï¼Œç­‰å¾…æ—¶é—´ {wait_time:.1f}s")
        else:
            # æ­£å¸¸æ¨¡å¼
            distance = base_distance
            wait_time = base_wait
        
        return distance, wait_time
    
    async def _perform_scroll(self, scroll_distance: int, wait_time: float):
        """æ‰§è¡Œæ»šåŠ¨æ“ä½œ"""
        try:
            await self.parser.page.evaluate('window.focus()')
            current_scroll = await self.parser.page.evaluate('window.pageYOffset')
            
            await self.parser.page.evaluate(f'''
                window.scrollTo({{
                    top: {current_scroll + scroll_distance},
                    behavior: 'smooth'
                }});
            ''')
            
            await asyncio.sleep(wait_time)
            
        except Exception as e:
            self.logger.warning(f"æ»šåŠ¨å¤±è´¥: {e}")
            await asyncio.sleep(1)
    
    async def _handle_long_stagnation(self):
        """å¤„ç†é•¿æ—¶é—´åœæ»"""
        self.logger.info("ğŸ”„ é•¿æ—¶é—´æ— æ–°å†…å®¹ï¼Œå°è¯•é¡µé¢åˆ·æ–°...")
        try:
            await self.parser.page.reload(wait_until='domcontentloaded')
            await asyncio.sleep(3)
        except Exception as e:
            self.logger.warning(f"é¡µé¢åˆ·æ–°å¤±è´¥: {e}")
    
    async def _incremental_save(self) -> str:
        """å¢é‡ä¿å­˜è§£æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'integrated_realtime_incremental_{timestamp}.json'
        
        incremental_data = {
            'save_type': 'incremental',
            'timestamp': datetime.now().isoformat(),
            'total_tweets': len(self.parsed_tweets),
            'parsing_stats': self.parsing_stats.copy(),
            'latest_tweets': self.parsed_tweets[-self.incremental_save_interval:] if len(self.parsed_tweets) >= self.incremental_save_interval else self.parsed_tweets
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(incremental_data, f, ensure_ascii=False, indent=2)
            
            self.parsing_stats['incremental_saves'] += 1
            self.logger.info(f"ğŸ’¾ å¢é‡ä¿å­˜å®Œæˆ: {filename} ({len(self.parsed_tweets)} æ¡æ¨æ–‡)")
            return filename
        except Exception as e:
            self.logger.error(f"å¢é‡ä¿å­˜å¤±è´¥: {e}")
            return ""
    
    async def _final_save(self) -> str:
        """æœ€ç»ˆä¿å­˜æ‰€æœ‰è§£æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'integrated_realtime_final_{timestamp}.json'
        
        final_data = {
            'save_type': 'final',
            'timestamp': datetime.now().isoformat(),
            'total_tweets': len(self.parsed_tweets),
            'unique_tweets': len(self.seen_tweet_ids),
            'parsing_stats': self.parsing_stats.copy(),
            'quality_metrics': self._calculate_quality_metrics(),
            'all_tweets': self.parsed_tweets
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“„ æœ€ç»ˆç»“æœå·²ä¿å­˜: {filename}")
            return filename
        except Exception as e:
            self.logger.error(f"æœ€ç»ˆä¿å­˜å¤±è´¥: {e}")
            return ""
    
    def _calculate_quality_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡"""
        if not self.parsed_tweets:
            return {}
        
        total_tweets = len(self.parsed_tweets)
        
        # å†…å®¹è´¨é‡
        has_content = sum(1 for t in self.parsed_tweets if t.get('content'))
        has_username = sum(1 for t in self.parsed_tweets if t.get('username'))
        has_id = sum(1 for t in self.parsed_tweets if t.get('id'))
        has_timestamp = sum(1 for t in self.parsed_tweets if t.get('timestamp'))
        
        # è§£ææ•ˆç‡
        total_attempts = self.parsing_stats['tweets_parsed'] + self.parsing_stats['duplicates_skipped'] + self.parsing_stats['parsing_errors']
        
        return {
            'content_completeness': has_content / total_tweets,
            'username_completeness': has_username / total_tweets,
            'id_completeness': has_id / total_tweets,
            'timestamp_completeness': has_timestamp / total_tweets,
            'parsing_success_rate': self.parsing_stats['tweets_parsed'] / max(total_attempts, 1),
            'duplicate_rate': self.parsing_stats['duplicates_skipped'] / max(total_attempts, 1),
            'error_rate': self.parsing_stats['parsing_errors'] / max(total_attempts, 1),
            'efficiency_per_scroll': self.parsing_stats['tweets_parsed'] / max(self.parsing_stats['total_scrolls'], 1),
            'dom_parsing_efficiency': self.parsing_stats['dom_elements_parsed'] / max(self.parsing_stats['dom_elements_found'], 1)
        }
    
    def get_parsing_summary(self) -> Dict[str, Any]:
        """è·å–è§£ææ‘˜è¦"""
        return {
            'total_parsed': len(self.parsed_tweets),
            'unique_tweets': len(self.seen_tweet_ids),
            'parsing_stats': self.parsing_stats.copy(),
            'quality_metrics': self._calculate_quality_metrics()
        }


class IntegratedRealtimeTest:
    """é›†æˆå®æ—¶è§£ææµ‹è¯•ç±»"""
    
    def __init__(self, max_tweets: int = 20):
        self.max_tweets = max_tweets
        self.logger = logging.getLogger(__name__)
        self.launcher = None
        self.parser = None
        self.realtime_parser = None
    
    async def setup_browser(self) -> bool:
        """è®¾ç½®æµè§ˆå™¨ç¯å¢ƒ"""
        try:
            self.logger.info("ğŸš€ åˆå§‹åŒ–AdsPowerå¯åŠ¨å™¨...")
            self.launcher = AdsPowerLauncher(ADS_POWER_CONFIG)
            
            browser_info = self.launcher.start_browser()
            self.launcher.wait_for_browser_ready()
            debug_port = self.launcher.get_debug_port()
            
            self.logger.info(f"âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼Œè°ƒè¯•ç«¯å£: {debug_port}")
            
            # åˆå§‹åŒ–TwitterParser
            self.parser = TwitterParser(debug_port=debug_port)
            await self.parser.initialize()
            
            # åˆå§‹åŒ–é›†æˆå®æ—¶è§£æå™¨
            self.realtime_parser = IntegratedRealtimeParser(self.parser)
            
            return True
            
        except Exception as e:
            self.logger.error(f"æµè§ˆå™¨è®¾ç½®å¤±è´¥: {e}")
            return False
    
    async def test_integrated_realtime_parsing(self, username: str = "elonmusk") -> Dict[str, Any]:
        """æµ‹è¯•é›†æˆå®æ—¶è§£æåŠŸèƒ½"""
        test_result = {
            'test_name': 'integrated_realtime_parsing_test',
            'success': False,
            'details': {},
            'errors': []
        }
        
        try:
            # å¯¼èˆªåˆ°ç”¨æˆ·é¡µé¢
            self.logger.info(f"ğŸ” å¯¼èˆªåˆ° @{username} çš„ä¸ªäººèµ„æ–™é¡µé¢...")
            await self.parser.navigate_to_profile(username)
            await asyncio.sleep(3)
            
            # æ‰§è¡Œé›†æˆå®æ—¶æ»šåŠ¨è§£æ
            self.logger.info(f"ğŸ“œ å¼€å§‹é›†æˆå®æ—¶æ»šåŠ¨è§£æï¼ˆç›®æ ‡: {self.max_tweets}æ¡ï¼‰...")
            parse_result = await self.realtime_parser.scroll_and_parse_realtime_integrated(
                target_tweets=self.max_tweets,
                max_attempts=self.max_tweets * 2
            )
            
            # è·å–è§£ææ‘˜è¦
            parsing_summary = self.realtime_parser.get_parsing_summary()
            
            test_result['details'] = {
                'target_tweets': self.max_tweets,
                'parsed_tweets': parse_result['parsed_tweets_count'],
                'scroll_attempts': parse_result['scroll_attempts'],
                'target_reached': parse_result['target_reached'],
                'efficiency': parse_result['efficiency'],
                'dom_parsing_ratio': parse_result['dom_parsing_ratio'],
                'parsing_summary': parsing_summary,
                'final_save_file': parse_result['final_save_file'],
                'sample_tweet_keys': list(parse_result['parsed_tweets'][0].keys()) if parse_result['parsed_tweets'] else []
            }
            
            # éªŒè¯é›†æˆå®æ—¶è§£æçš„å…³é”®ç‰¹æ€§
            integration_validation = self._validate_integration_features(parse_result, parsing_summary)
            test_result['details']['integration_validation'] = integration_validation
            
            # åˆ¤æ–­æµ‹è¯•æˆåŠŸ
            if (parse_result['parsed_tweets_count'] > 0 and 
                integration_validation['realtime_parsing_working'] and
                integration_validation['dom_element_handling_working']):
                test_result['success'] = True
                self.logger.info(f"âœ… é›†æˆå®æ—¶è§£ææµ‹è¯•æˆåŠŸ: {parse_result['parsed_tweets_count']} æ¡æ¨æ–‡")
            else:
                test_result['errors'].append("é›†æˆå®æ—¶è§£æå…³é”®ç‰¹æ€§éªŒè¯å¤±è´¥")
                self.logger.error("âŒ é›†æˆå®æ—¶è§£ææµ‹è¯•å¤±è´¥")
            
        except Exception as e:
            test_result['errors'].append(str(e))
            self.logger.error(f"âŒ é›†æˆå®æ—¶è§£ææµ‹è¯•å¤±è´¥: {e}")
        
        return test_result
    
    def _validate_integration_features(self, parse_result: Dict[str, Any], parsing_summary: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯é›†æˆå®æ—¶è§£æçš„å…³é”®ç‰¹æ€§"""
        validation = {
            'realtime_parsing_working': False,
            'dom_element_handling_working': False,
            'incremental_saves_working': False,
            'quality_metrics_working': False,
            'scroll_efficiency_acceptable': False
        }
        
        # æ£€æŸ¥å®æ—¶è§£æ
        if parse_result['parsed_tweets_count'] > 0 and parse_result['efficiency'] > 0:
            validation['realtime_parsing_working'] = True
            self.logger.info(f"âœ… å®æ—¶è§£æåŠŸèƒ½æ­£å¸¸: æ•ˆç‡ {parse_result['efficiency']:.2f} æ¨æ–‡/æ»šåŠ¨")
        
        # æ£€æŸ¥DOMå…ƒç´ å¤„ç†
        dom_ratio = parse_result.get('dom_parsing_ratio', 0)
        if dom_ratio > 0:
            validation['dom_element_handling_working'] = True
            self.logger.info(f"âœ… DOMå…ƒç´ å¤„ç†æ­£å¸¸: è§£ææ¯”ç‡ {dom_ratio:.1%}")
        
        # æ£€æŸ¥å¢é‡ä¿å­˜
        if parsing_summary['parsing_stats']['incremental_saves'] >= 0:
            validation['incremental_saves_working'] = True
            self.logger.info(f"âœ… å¢é‡ä¿å­˜åŠŸèƒ½æ­£å¸¸: {parsing_summary['parsing_stats']['incremental_saves']} æ¬¡ä¿å­˜")
        
        # æ£€æŸ¥è´¨é‡æŒ‡æ ‡
        quality_metrics = parsing_summary.get('quality_metrics', {})
        if quality_metrics and quality_metrics.get('content_completeness', 0) > 0:
            validation['quality_metrics_working'] = True
            self.logger.info(f"âœ… è´¨é‡æŒ‡æ ‡åŠŸèƒ½æ­£å¸¸: å†…å®¹å®Œæ•´æ€§ {quality_metrics['content_completeness']:.1%}")
        
        # æ£€æŸ¥æ»šåŠ¨æ•ˆç‡
        if parse_result['efficiency'] > 0.1:  # è‡³å°‘æ¯10æ¬¡æ»šåŠ¨è§£æ1æ¡æ¨æ–‡
            validation['scroll_efficiency_acceptable'] = True
            self.logger.info(f"âœ… æ»šåŠ¨æ•ˆç‡å¯æ¥å—: {parse_result['efficiency']:.2f}")
        
        return validation
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.parser:
                await self.parser.close()
            if self.launcher:
                self.launcher.stop_browser()
            self.logger.info("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(__name__)
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test = IntegratedRealtimeTest(max_tweets=15)
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        if not await test.setup_browser():
            logger.error("âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥")
            return
        
        # è¿è¡Œé›†æˆå®æ—¶è§£ææµ‹è¯•
        result = await test.test_integrated_realtime_parsing()
        
        # è¾“å‡ºæµ‹è¯•ç»“æœ
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š é›†æˆå®æ—¶è§£ææµ‹è¯•ç»“æœ")
        logger.info("="*60)
        logger.info(f"æµ‹è¯•çŠ¶æ€: {'âœ… æˆåŠŸ' if result['success'] else 'âŒ å¤±è´¥'}")
        
        if result['success']:
            details = result['details']
            logger.info(f"ç›®æ ‡æ¨æ–‡æ•°: {details['target_tweets']}")
            logger.info(f"å®é™…è§£ææ•°: {details['parsed_tweets']}")
            logger.info(f"æ»šåŠ¨æ¬¡æ•°: {details['scroll_attempts']}")
            logger.info(f"è§£ææ•ˆç‡: {details['efficiency']:.2f} æ¨æ–‡/æ»šåŠ¨")
            logger.info(f"DOMè§£ææ¯”ç‡: {details['dom_parsing_ratio']:.1%}")
            
            # é›†æˆç‰¹æ€§éªŒè¯ç»“æœ
            validation = details['integration_validation']
            logger.info("\nğŸ” é›†æˆç‰¹æ€§éªŒè¯:")
            for feature, status in validation.items():
                status_icon = "âœ…" if status else "âŒ"
                logger.info(f"  {status_icon} {feature}: {status}")
            
            # è´¨é‡æŒ‡æ ‡
            quality = details['parsing_summary']['quality_metrics']
            logger.info("\nğŸ“ˆ æ•°æ®è´¨é‡æŒ‡æ ‡:")
            logger.info(f"  å†…å®¹å®Œæ•´æ€§: {quality['content_completeness']:.1%}")
            logger.info(f"  ç”¨æˆ·åå®Œæ•´æ€§: {quality['username_completeness']:.1%}")
            logger.info(f"  IDå®Œæ•´æ€§: {quality['id_completeness']:.1%}")
            logger.info(f"  DOMè§£ææ•ˆç‡: {quality['dom_parsing_efficiency']:.1%}")
            
            logger.info(f"\nğŸ“„ ç»“æœæ–‡ä»¶: {details['final_save_file']}")
        else:
            logger.error(f"é”™è¯¯ä¿¡æ¯: {result['errors']}")
        
        logger.info("="*60)
        
    finally:
        await test.cleanup()


if __name__ == "__main__":
    asyncio.run(main())