#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´å·¥ä½œæµæµ‹è¯•è„šæœ¬
æµ‹è¯•100æ¡æ¨æ–‡çš„æŠ“å–ã€æ•°æ®åº“ä¿å­˜å’Œé£ä¹¦åŒæ­¥
"""

import asyncio
import logging
import json
import sys
from datetime import datetime
from typing import Dict, Any, List

from twitter_parser import TwitterParser
from ads_browser_launcher import AdsPowerLauncher
from web_app import app, db, TweetData, ScrapingTask, FEISHU_CONFIG, load_config_from_database
from cloud_sync import CloudSyncManager
from config import ADS_POWER_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CompleteWorkflowTester:
    """å®Œæ•´å·¥ä½œæµæµ‹è¯•å™¨"""
    
    def __init__(self, target_tweets: int = 100):
        self.target_tweets = target_tweets
        self.launcher = None
        self.parser = None
        self.scraped_tweets = []
        self.test_results = {
            'timestamp': datetime.now().isoformat(),
            'target_tweets': target_tweets,
            'scraping': {'success': False, 'tweet_count': 0},
            'database_save': {'success': False, 'saved_count': 0},
            'feishu_sync': {'success': False, 'synced_count': 0},
            'overall_success': False
        }
    
    async def test_scraping(self) -> bool:
        """æµ‹è¯•æ¨æ–‡æŠ“å–"""
        logger.info(f"ğŸš€ å¼€å§‹æŠ“å– {self.target_tweets} æ¡æ¨æ–‡...")
        
        try:
            # å¯åŠ¨æµè§ˆå™¨
            self.launcher = AdsPowerLauncher(config=ADS_POWER_CONFIG)
            browser_info = self.launcher.start_browser()
            
            if not browser_info:
                logger.error("æµè§ˆå™¨å¯åŠ¨å¤±è´¥")
                return False
            
            # åˆ›å»ºè§£æå™¨
            debug_url = browser_info.get('ws', {}).get('puppeteer', '')
            if not debug_url:
                logger.error("æ— æ³•è·å–è°ƒè¯•ç«¯å£")
                return False
            
            self.parser = TwitterParser(debug_port=debug_url)
            
            # åˆå§‹åŒ–è§£æå™¨
            await self.parser.initialize()
            logger.info("âœ… æµè§ˆå™¨è¿æ¥æˆåŠŸ")
            
            # å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
            try:
                await self.parser.navigate_to_profile('elonmusk')
                logger.info("âœ… å¯¼èˆªæˆåŠŸ")
            except Exception as e:
                logger.error(f"å¯¼èˆªå¤±è´¥: {e}")
                return False
            
            # é‡æ–°å¯ç”¨ä¼˜åŒ–åŠŸèƒ½è¿›è¡Œè°ƒè¯•
            self.parser.optimization_enabled = True
            logger.info(f"ä¼˜åŒ–åŠŸèƒ½çŠ¶æ€: {self.parser.optimization_enabled}")
            
            # æŠ“å–æ¨æ–‡
            tweets = await self.parser.scrape_tweets(self.target_tweets)
            
            if tweets:
                self.scraped_tweets = tweets
                self.test_results['scraping'] = {
                    'success': True,
                    'tweet_count': len(tweets)
                }
                logger.info(f"âœ… æˆåŠŸæŠ“å– {len(tweets)} æ¡æ¨æ–‡")
                return True
            else:
                logger.error("æŠ“å–æ¨æ–‡å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"æŠ“å–è¿‡ç¨‹å¼‚å¸¸: {e}")
            return False
    
    def test_database_save(self) -> bool:
        """æµ‹è¯•æ•°æ®åº“ä¿å­˜"""
        logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜åˆ°æ•°æ®åº“...")
        
        if not self.scraped_tweets:
            logger.error("æ²¡æœ‰æ¨æ–‡æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            with app.app_context():
                # åˆ›å»ºæˆ–è·å–æµ‹è¯•ä»»åŠ¡
                test_task = ScrapingTask.query.filter_by(name='å®Œæ•´å·¥ä½œæµæµ‹è¯•').first()
                if not test_task:
                    test_task = ScrapingTask(
                        name='å®Œæ•´å·¥ä½œæµæµ‹è¯•',
                        target_accounts=json.dumps(['elonmusk']),
                        target_keywords=json.dumps([]),
                        max_tweets=self.target_tweets,
                        status='completed'
                    )
                    db.session.add(test_task)
                    db.session.commit()
                
                # ä¿å­˜æ¨æ–‡åˆ°æ•°æ®åº“
                saved_count = 0
                for tweet in self.scraped_tweets:
                    try:
                        # æ£€æŸ¥æ•°æ®æ ¼å¼
                        if isinstance(tweet, str):
                            logger.warning(f"è·³è¿‡å­—ç¬¦ä¸²æ ¼å¼çš„æ¨æ–‡æ•°æ®: {tweet[:50]}...")
                            continue
                        
                        if not isinstance(tweet, dict):
                            logger.warning(f"è·³è¿‡éå­—å…¸æ ¼å¼çš„æ¨æ–‡æ•°æ®: {type(tweet)}")
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        link = tweet.get('link') or tweet.get('url', '')
                        if link:
                            existing_tweet = TweetData.query.filter_by(link=link).first()
                            if existing_tweet:
                                continue
                        
                        # åˆ›å»ºæ–°è®°å½•
                        tweet_data = TweetData(
                            task_id=test_task.id,
                            username=tweet.get('username', ''),
                            content=tweet.get('content', ''),
                            publish_time=tweet.get('timestamp') or tweet.get('publish_time', ''),
                            likes=int(tweet.get('likes', 0)),
                            comments=int(tweet.get('comments', 0)),
                            retweets=int(tweet.get('retweets', 0)),
                            link=link,
                            hashtags=json.dumps(tweet.get('tags', []), ensure_ascii=False),
                            content_type='æµ‹è¯•æ•°æ®'
                        )
                        
                        db.session.add(tweet_data)
                        saved_count += 1
                        
                    except Exception as e:
                        logger.error(f"ä¿å­˜å•æ¡æ¨æ–‡å¤±è´¥: {e}")
                        continue
                
                # æäº¤åˆ°æ•°æ®åº“
                db.session.commit()
                
                self.test_results['database_save'] = {
                    'success': True,
                    'saved_count': saved_count
                }
                
                logger.info(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡æ¨æ–‡åˆ°æ•°æ®åº“")
                return True
                
        except Exception as e:
            logger.error(f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            if 'db' in locals():
                db.session.rollback()
            return False
    
    def test_feishu_sync(self) -> bool:
        """æµ‹è¯•é£ä¹¦åŒæ­¥"""
        logger.info("ğŸ“Š å¼€å§‹åŒæ­¥åˆ°é£ä¹¦...")
        
        if not self.scraped_tweets:
            logger.error("æ²¡æœ‰æ¨æ–‡æ•°æ®å¯åŒæ­¥")
            return False
        
        try:
            with app.app_context():
                # é‡æ–°åŠ è½½é…ç½®
                load_config_from_database()
                
                # æ£€æŸ¥é£ä¹¦é…ç½®
                if not FEISHU_CONFIG.get('enabled'):
                    logger.warning("é£ä¹¦åŒæ­¥æœªå¯ç”¨")
                    return False
                
                required_fields = ['app_id', 'app_secret', 'spreadsheet_token', 'table_id']
                missing_fields = [field for field in required_fields if not FEISHU_CONFIG.get(field)]
                if missing_fields:
                    logger.error(f"é£ä¹¦é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {', '.join(missing_fields)}")
                    return False
                
                # åˆ›å»ºåŒæ­¥ç®¡ç†å™¨
                sync_manager = CloudSyncManager()
                sync_manager.setup_feishu(
                    FEISHU_CONFIG['app_id'], 
                    FEISHU_CONFIG['app_secret']
                )
                
                # è·å–è®¿é—®ä»¤ç‰Œ
                access_token = sync_manager.get_feishu_access_token()
                if not access_token:
                    logger.error("è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œå¤±è´¥")
                    return False
                
                # å‡†å¤‡æ•°æ®æ ¼å¼
                feishu_data = []
                for i, tweet in enumerate(self.scraped_tweets, 1):
                    # æ£€æŸ¥æ•°æ®æ ¼å¼
                    if isinstance(tweet, str):
                        logger.warning(f"è·³è¿‡å­—ç¬¦ä¸²æ ¼å¼çš„æ¨æ–‡æ•°æ®: {tweet[:50]}...")
                        continue
                    
                    if not isinstance(tweet, dict):
                        logger.warning(f"è·³è¿‡éå­—å…¸æ ¼å¼çš„æ¨æ–‡æ•°æ®: {type(tweet)}")
                        continue
                    
                    feishu_data.append({
                        'åºå·': i,
                        'ç”¨æˆ·å': tweet.get('username', ''),
                        'æ¨æ–‡å†…å®¹': tweet.get('content', ''),
                        'å‘å¸ƒæ—¶é—´': tweet.get('timestamp') or tweet.get('publish_time', ''),
                        'ç‚¹èµæ•°': int(tweet.get('likes', 0)),
                        'è¯„è®ºæ•°': int(tweet.get('comments', 0)),
                        'è½¬å‘æ•°': int(tweet.get('retweets', 0)),
                        'é“¾æ¥': tweet.get('link') or tweet.get('url', ''),
                        'æ ‡ç­¾': ', '.join(tweet.get('tags', [])),
                        'ç­›é€‰çŠ¶æ€': 'æµ‹è¯•æ•°æ®'
                    })
                
                # æ‰§è¡Œé£ä¹¦åŒæ­¥
                success = sync_manager._execute_feishu_sync(
                    feishu_data,
                    FEISHU_CONFIG['spreadsheet_token'],
                    FEISHU_CONFIG['table_id'],
                    access_token
                )
                
                if success:
                    self.test_results['feishu_sync'] = {
                        'success': True,
                        'synced_count': len(self.scraped_tweets)
                    }
                    logger.info(f"âœ… æˆåŠŸåŒæ­¥ {len(self.scraped_tweets)} æ¡æ¨æ–‡åˆ°é£ä¹¦")
                    return True
                else:
                    logger.error("é£ä¹¦åŒæ­¥å¤±è´¥")
                    return False
                    
        except Exception as e:
            logger.error(f"é£ä¹¦åŒæ­¥å¼‚å¸¸: {e}")
            return False
    
    async def run_complete_test(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹å®Œæ•´å·¥ä½œæµæµ‹è¯•")
        
        try:
            # 1. æµ‹è¯•æŠ“å–
            scraping_success = await self.test_scraping()
            
            # 2. æµ‹è¯•æ•°æ®åº“ä¿å­˜
            database_success = False
            if scraping_success:
                database_success = self.test_database_save()
            
            # 3. æµ‹è¯•é£ä¹¦åŒæ­¥
            feishu_success = False
            if scraping_success:
                feishu_success = self.test_feishu_sync()
            
            # æ›´æ–°æ€»ä½“ç»“æœ
            self.test_results['overall_success'] = scraping_success and database_success and feishu_success
            
            return self.test_results
            
        except Exception as e:
            logger.error(f"å®Œæ•´æµ‹è¯•å¼‚å¸¸: {e}")
            return self.test_results
        
        finally:
            await self.cleanup()
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.parser:
                await self.parser.close()
            if self.launcher:
                self.launcher.stop_browser()
            logger.info("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")
    
    def save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'complete_workflow_test_{timestamp}.json'
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    target_tweets = int(sys.argv[1]) if len(sys.argv) > 1 else 100
    
    tester = CompleteWorkflowTester(target_tweets=target_tweets)
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    results = await tester.run_complete_test()
    
    # ä¿å­˜ç»“æœ
    tester.save_results()
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š å®Œæ•´å·¥ä½œæµæµ‹è¯•æ‘˜è¦")
    print("="*60)
    print(f"ç›®æ ‡æ¨æ–‡æ•°: {target_tweets}")
    print(f"æŠ“å–ç»“æœ: {'âœ… æˆåŠŸ' if results['scraping']['success'] else 'âŒ å¤±è´¥'} ({results['scraping']['tweet_count']} æ¡)")
    print(f"æ•°æ®åº“ä¿å­˜: {'âœ… æˆåŠŸ' if results['database_save']['success'] else 'âŒ å¤±è´¥'} ({results['database_save']['saved_count']} æ¡)")
    print(f"é£ä¹¦åŒæ­¥: {'âœ… æˆåŠŸ' if results['feishu_sync']['success'] else 'âŒ å¤±è´¥'} ({results['feishu_sync']['synced_count']} æ¡)")
    print(f"æ•´ä½“çŠ¶æ€: {'âœ… æˆåŠŸ' if results['overall_success'] else 'âŒ å¤±è´¥'}")
    print("="*60)

if __name__ == '__main__':
    asyncio.run(main())