#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TwitteræŠ“å–ç³»ç»Ÿæ•°æ®å®Œæ•´æ€§å’Œå¤‡ä»½æµ‹è¯•è„šæœ¬
æµ‹è¯•æ•°æ®å­˜å‚¨ã€å¤‡ä»½æ¢å¤ã€æ•°æ®ä¸€è‡´æ€§ç­‰åŠŸèƒ½
"""

import os
import sys
import json
import time
import sqlite3
import shutil
import hashlib
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed

class DataIntegrityTester:
    def __init__(self, base_url="http://localhost:8084"):
        self.base_url = base_url
        self.test_results = []
        self.project_root = Path(__file__).parent
        self.data_dir = self.project_root / "data"
        self.backup_dir = self.project_root / "backup_test"
        
        # ç¡®ä¿æµ‹è¯•ç›®å½•å­˜åœ¨
        self.backup_dir.mkdir(exist_ok=True)
        
    def log_test_result(self, test_name, success, details="", severity="INFO"):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        result = {
            'test_name': test_name,
            'success': success,
            'details': details,
            'severity': severity,
            'timestamp': datetime.now().isoformat()
        }
        self.test_results.append(result)
        
        status = "âœ…" if success else "âŒ"
        severity_emoji = {"INFO": "â„¹ï¸", "WARNING": "âš ï¸", "ERROR": "ğŸš¨"}.get(severity, "â„¹ï¸")
        print(f"   {status} {severity_emoji} {test_name}: {details}")
    
    def test_database_integrity(self):
        """æµ‹è¯•æ•°æ®åº“å®Œæ•´æ€§"""
        print("ğŸ—„ï¸ æµ‹è¯•æ•°æ®åº“å®Œæ•´æ€§...")
        
        # æŸ¥æ‰¾æ•°æ®åº“æ–‡ä»¶
        db_files = list(self.data_dir.glob("*.db")) + list(self.project_root.glob("*.db"))
        
        if not db_files:
            self.log_test_result(
                "æ•°æ®åº“æ–‡ä»¶æ£€æŸ¥",
                False,
                "æœªæ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶",
                "WARNING"
            )
            return False
        
        integrity_passed = 0
        total_dbs = len(db_files)
        
        for db_file in db_files:
            try:
                # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶å®Œæ•´æ€§
                conn = sqlite3.connect(str(db_file))
                cursor = conn.cursor()
                
                # æ‰§è¡ŒPRAGMA integrity_check
                cursor.execute("PRAGMA integrity_check")
                result = cursor.fetchone()
                
                if result and result[0] == 'ok':
                    integrity_passed += 1
                    self.log_test_result(
                        f"æ•°æ®åº“å®Œæ•´æ€§ - {db_file.name}",
                        True,
                        "å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡"
                    )
                else:
                    self.log_test_result(
                        f"æ•°æ®åº“å®Œæ•´æ€§ - {db_file.name}",
                        False,
                        f"å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {result}",
                        "ERROR"
                    )
                
                # æ£€æŸ¥è¡¨ç»“æ„
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                
                if tables:
                    self.log_test_result(
                        f"æ•°æ®åº“è¡¨ç»“æ„ - {db_file.name}",
                        True,
                        f"åŒ…å« {len(tables)} ä¸ªè¡¨"
                    )
                else:
                    self.log_test_result(
                        f"æ•°æ®åº“è¡¨ç»“æ„ - {db_file.name}",
                        False,
                        "æ•°æ®åº“ä¸ºç©ºæˆ–æ— è¡¨",
                        "WARNING"
                    )
                
                conn.close()
                
            except Exception as e:
                self.log_test_result(
                    f"æ•°æ®åº“è®¿é—® - {db_file.name}",
                    False,
                    f"æ— æ³•è®¿é—®æ•°æ®åº“: {str(e)}",
                    "ERROR"
                )
        
        success_rate = integrity_passed / total_dbs if total_dbs > 0 else 0
        
        self.log_test_result(
            "æ•°æ®åº“å®Œæ•´æ€§æ€»ä½“",
            success_rate >= 0.8,
            f"å®Œæ•´æ€§é€šè¿‡ç‡: {success_rate:.1%} ({integrity_passed}/{total_dbs})",
            "ERROR" if success_rate < 0.5 else "WARNING" if success_rate < 0.8 else "INFO"
        )
        
        return success_rate >= 0.8
    
    def test_data_backup_restore(self):
        """æµ‹è¯•æ•°æ®å¤‡ä»½å’Œæ¢å¤"""
        print("\nğŸ’¾ æµ‹è¯•æ•°æ®å¤‡ä»½å’Œæ¢å¤...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_db = self.backup_dir / "test_backup.db"
        
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
            conn = sqlite3.connect(str(test_db))
            cursor = conn.cursor()
            
            # åˆ›å»ºæµ‹è¯•è¡¨
            cursor.execute("""
                CREATE TABLE test_tweets (
                    id INTEGER PRIMARY KEY,
                    content TEXT,
                    created_at TIMESTAMP,
                    user_id TEXT
                )
            """)
            
            # æ’å…¥æµ‹è¯•æ•°æ®
            test_data = [
                (1, "Test tweet 1", datetime.now(), "user1"),
                (2, "Test tweet 2", datetime.now(), "user2"),
                (3, "Test tweet 3", datetime.now(), "user3")
            ]
            
            cursor.executemany(
                "INSERT INTO test_tweets (id, content, created_at, user_id) VALUES (?, ?, ?, ?)",
                test_data
            )
            
            conn.commit()
            conn.close()
            
            # è®¡ç®—åŸå§‹æ–‡ä»¶å“ˆå¸Œ
            original_hash = self.calculate_file_hash(test_db)
            
            self.log_test_result(
                "æµ‹è¯•æ•°æ®åˆ›å»º",
                True,
                f"åˆ›å»ºäº†åŒ…å« {len(test_data)} æ¡è®°å½•çš„æµ‹è¯•æ•°æ®åº“"
            )
            
            # æµ‹è¯•å¤‡ä»½
            backup_file = self.backup_dir / "test_backup_copy.db"
            shutil.copy2(test_db, backup_file)
            
            backup_hash = self.calculate_file_hash(backup_file)
            
            if original_hash == backup_hash:
                self.log_test_result(
                    "æ•°æ®å¤‡ä»½",
                    True,
                    "å¤‡ä»½æ–‡ä»¶å“ˆå¸Œå€¼åŒ¹é…"
                )
            else:
                self.log_test_result(
                    "æ•°æ®å¤‡ä»½",
                    False,
                    "å¤‡ä»½æ–‡ä»¶å“ˆå¸Œå€¼ä¸åŒ¹é…",
                    "ERROR"
                )
                return False
            
            # æµ‹è¯•æ•°æ®æ¢å¤
            # åˆ é™¤åŸæ–‡ä»¶
            test_db.unlink()
            
            # ä»å¤‡ä»½æ¢å¤
            shutil.copy2(backup_file, test_db)
            
            # éªŒè¯æ¢å¤çš„æ•°æ®
            conn = sqlite3.connect(str(test_db))
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM test_tweets")
            count = cursor.fetchone()[0]
            
            if count == len(test_data):
                self.log_test_result(
                    "æ•°æ®æ¢å¤",
                    True,
                    f"æˆåŠŸæ¢å¤ {count} æ¡è®°å½•"
                )
            else:
                self.log_test_result(
                    "æ•°æ®æ¢å¤",
                    False,
                    f"æ¢å¤è®°å½•æ•°ä¸åŒ¹é…: æœŸæœ› {len(test_data)}, å®é™… {count}",
                    "ERROR"
                )
                conn.close()
                return False
            
            # éªŒè¯æ•°æ®å†…å®¹
            cursor.execute("SELECT * FROM test_tweets ORDER BY id")
            restored_data = cursor.fetchall()
            
            data_match = True
            for i, (original, restored) in enumerate(zip(test_data, restored_data)):
                if original[0] != restored[0] or original[1] != restored[1] or original[3] != restored[3]:
                    data_match = False
                    break
            
            if data_match:
                self.log_test_result(
                    "æ•°æ®å†…å®¹éªŒè¯",
                    True,
                    "æ¢å¤çš„æ•°æ®å†…å®¹å®Œå…¨åŒ¹é…"
                )
            else:
                self.log_test_result(
                    "æ•°æ®å†…å®¹éªŒè¯",
                    False,
                    "æ¢å¤çš„æ•°æ®å†…å®¹ä¸åŒ¹é…",
                    "ERROR"
                )
            
            conn.close()
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            try:
                test_db.unlink()
            except FileNotFoundError:
                pass
            try:
                backup_file.unlink()
            except FileNotFoundError:
                pass
            
            return data_match
            
        except Exception as e:
            self.log_test_result(
                "å¤‡ä»½æ¢å¤æµ‹è¯•",
                False,
                f"æµ‹è¯•å¼‚å¸¸: {str(e)}",
                "ERROR"
            )
            return False
    
    def test_data_consistency(self):
        """æµ‹è¯•æ•°æ®ä¸€è‡´æ€§"""
        print("\nğŸ”„ æµ‹è¯•æ•°æ®ä¸€è‡´æ€§...")
        
        # æŸ¥æ‰¾ç°æœ‰æ•°æ®åº“
        db_files = list(self.data_dir.glob("*.db")) + list(self.project_root.glob("*.db"))
        
        if not db_files:
            self.log_test_result(
                "æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥",
                False,
                "æœªæ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶è¿›è¡Œä¸€è‡´æ€§æ£€æŸ¥",
                "WARNING"
            )
            return True  # æ²¡æœ‰æ•°æ®åº“ä¹Ÿç®—é€šè¿‡
        
        consistency_issues = 0
        
        for db_file in db_files:
            try:
                conn = sqlite3.connect(str(db_file))
                cursor = conn.cursor()
                
                # æ£€æŸ¥å¤–é”®çº¦æŸ
                cursor.execute("PRAGMA foreign_key_check")
                fk_violations = cursor.fetchall()
                
                if fk_violations:
                    consistency_issues += 1
                    self.log_test_result(
                        f"å¤–é”®çº¦æŸ - {db_file.name}",
                        False,
                        f"å‘ç° {len(fk_violations)} ä¸ªå¤–é”®çº¦æŸè¿å",
                        "ERROR"
                    )
                else:
                    self.log_test_result(
                        f"å¤–é”®çº¦æŸ - {db_file.name}",
                        True,
                        "å¤–é”®çº¦æŸæ£€æŸ¥é€šè¿‡"
                    )
                
                # æ£€æŸ¥æ•°æ®ç±»å‹ä¸€è‡´æ€§
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                
                for table in tables:
                    table_name = table[0]
                    
                    # è·å–è¡¨ç»“æ„
                    cursor.execute(f"PRAGMA table_info({table_name})")
                    columns = cursor.fetchall()
                    
                    # æ£€æŸ¥æ¯åˆ—çš„æ•°æ®ç±»å‹ä¸€è‡´æ€§
                    for column in columns:
                        col_name = column[1]
                        col_type = column[2]
                        
                        if col_type.upper() in ['INTEGER', 'INT']:
                            # æ£€æŸ¥æ•´æ•°åˆ—æ˜¯å¦åŒ…å«éæ•´æ•°å€¼
                            cursor.execute(f"""
                                SELECT COUNT(*) FROM {table_name} 
                                WHERE {col_name} IS NOT NULL 
                                AND TYPEOF({col_name}) != 'integer'
                            """)
                            
                            invalid_count = cursor.fetchone()[0]
                            if invalid_count > 0:
                                consistency_issues += 1
                                self.log_test_result(
                                    f"æ•°æ®ç±»å‹ä¸€è‡´æ€§ - {table_name}.{col_name}",
                                    False,
                                    f"å‘ç° {invalid_count} ä¸ªéæ•´æ•°å€¼",
                                    "WARNING"
                                )
                
                conn.close()
                
            except Exception as e:
                consistency_issues += 1
                self.log_test_result(
                    f"ä¸€è‡´æ€§æ£€æŸ¥ - {db_file.name}",
                    False,
                    f"æ£€æŸ¥å¼‚å¸¸: {str(e)}",
                    "ERROR"
                )
        
        self.log_test_result(
            "æ•°æ®ä¸€è‡´æ€§æ€»ä½“",
            consistency_issues == 0,
            f"å‘ç° {consistency_issues} ä¸ªä¸€è‡´æ€§é—®é¢˜",
            "ERROR" if consistency_issues > 0 else "INFO"
        )
        
        return consistency_issues == 0
    
    def test_concurrent_data_access(self):
        """æµ‹è¯•å¹¶å‘æ•°æ®è®¿é—®"""
        print("\nğŸ”€ æµ‹è¯•å¹¶å‘æ•°æ®è®¿é—®...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
        test_db = self.backup_dir / "concurrent_test.db"
        
        try:
            # åˆå§‹åŒ–æ•°æ®åº“
            conn = sqlite3.connect(str(test_db))
            cursor = conn.cursor()
            
            cursor.execute("""
                CREATE TABLE concurrent_test (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    thread_id INTEGER,
                    operation TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.commit()
            conn.close()
            
            # å¹¶å‘å†™å…¥æµ‹è¯•
            def write_worker(thread_id, operations_count=10):
                """å¹¶å‘å†™å…¥å·¥ä½œçº¿ç¨‹"""
                try:
                    conn = sqlite3.connect(str(test_db), timeout=30)
                    cursor = conn.cursor()
                    
                    for i in range(operations_count):
                        cursor.execute(
                            "INSERT INTO concurrent_test (thread_id, operation) VALUES (?, ?)",
                            (thread_id, f"write_{i}")
                        )
                        conn.commit()
                        time.sleep(0.01)  # å°å»¶è¿Ÿæ¨¡æ‹ŸçœŸå®æ“ä½œ
                    
                    conn.close()
                    return True
                    
                except Exception as e:
                    return False
            
            # å¯åŠ¨å¤šä¸ªå¹¶å‘çº¿ç¨‹
            thread_count = 5
            operations_per_thread = 10
            
            with ThreadPoolExecutor(max_workers=thread_count) as executor:
                futures = [executor.submit(write_worker, i, operations_per_thread) 
                          for i in range(thread_count)]
                
                successful_threads = sum(1 for future in as_completed(futures) if future.result())
            
            # éªŒè¯å¹¶å‘å†™å…¥ç»“æœ
            conn = sqlite3.connect(str(test_db))
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM concurrent_test")
            total_records = cursor.fetchone()[0]
            
            expected_records = thread_count * operations_per_thread
            
            if total_records == expected_records:
                self.log_test_result(
                    "å¹¶å‘å†™å…¥æµ‹è¯•",
                    True,
                    f"æˆåŠŸå†™å…¥ {total_records} æ¡è®°å½• (æœŸæœ›: {expected_records})"
                )
            else:
                self.log_test_result(
                    "å¹¶å‘å†™å…¥æµ‹è¯•",
                    False,
                    f"è®°å½•æ•°ä¸åŒ¹é…: å®é™… {total_records}, æœŸæœ› {expected_records}",
                    "ERROR"
                )
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            cursor.execute("SELECT thread_id, COUNT(*) FROM concurrent_test GROUP BY thread_id")
            thread_counts = cursor.fetchall()
            
            integrity_ok = all(count == operations_per_thread for _, count in thread_counts)
            
            if integrity_ok:
                self.log_test_result(
                    "å¹¶å‘æ•°æ®å®Œæ•´æ€§",
                    True,
                    "æ‰€æœ‰çº¿ç¨‹çš„æ•°æ®éƒ½å®Œæ•´å†™å…¥"
                )
            else:
                self.log_test_result(
                    "å¹¶å‘æ•°æ®å®Œæ•´æ€§",
                    False,
                    "éƒ¨åˆ†çº¿ç¨‹çš„æ•°æ®ä¸¢å¤±",
                    "ERROR"
                )
            
            conn.close()
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            try:
                test_db.unlink()
            except FileNotFoundError:
                pass
            
            return total_records == expected_records and integrity_ok
            
        except Exception as e:
            self.log_test_result(
                "å¹¶å‘è®¿é—®æµ‹è¯•",
                False,
                f"æµ‹è¯•å¼‚å¸¸: {str(e)}",
                "ERROR"
            )
            return False
    
    def test_data_export_import(self):
        """æµ‹è¯•æ•°æ®å¯¼å‡ºå¯¼å…¥"""
        print("\nğŸ“¤ æµ‹è¯•æ•°æ®å¯¼å‡ºå¯¼å…¥...")
        
        # æŸ¥æ‰¾ç°æœ‰æ•°æ®åº“
        db_files = list(self.data_dir.glob("*.db")) + list(self.project_root.glob("*.db"))
        
        if not db_files:
            # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
            test_db = self.backup_dir / "export_test.db"
            
            conn = sqlite3.connect(str(test_db))
            cursor = conn.cursor()
            
            cursor.execute("""
                CREATE TABLE export_test (
                    id INTEGER PRIMARY KEY,
                    name TEXT,
                    value INTEGER,
                    created_at TIMESTAMP
                )
            """)
            
            # æ’å…¥æµ‹è¯•æ•°æ®
            test_data = [
                (1, "Item 1", 100, datetime.now()),
                (2, "Item 2", 200, datetime.now()),
                (3, "Item 3", 300, datetime.now())
            ]
            
            cursor.executemany(
                "INSERT INTO export_test VALUES (?, ?, ?, ?)",
                test_data
            )
            
            conn.commit()
            conn.close()
            
            db_files = [test_db]
        
        export_success = 0
        total_exports = 0
        
        for db_file in db_files[:2]:  # é™åˆ¶æµ‹è¯•æ•°é‡
            try:
                conn = sqlite3.connect(str(db_file))
                
                # è·å–æ‰€æœ‰è¡¨
                cursor = conn.cursor()
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                
                for table in tables[:2]:  # é™åˆ¶è¡¨æ•°é‡
                    table_name = table[0]
                    total_exports += 1
                    
                    # å¯¼å‡ºä¸ºCSV
                    export_file = self.backup_dir / f"{db_file.stem}_{table_name}.csv"
                    
                    try:
                        df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
                        df.to_csv(export_file, index=False)
                        
                        # éªŒè¯å¯¼å‡ºæ–‡ä»¶
                        if export_file.exists() and export_file.stat().st_size > 0:
                            export_success += 1
                            
                            # æµ‹è¯•é‡æ–°å¯¼å…¥
                            imported_df = pd.read_csv(export_file)
                            
                            if len(imported_df) == len(df):
                                self.log_test_result(
                                    f"æ•°æ®å¯¼å‡ºå¯¼å…¥ - {table_name}",
                                    True,
                                    f"æˆåŠŸå¯¼å‡ºå¯¼å…¥ {len(df)} è¡Œæ•°æ®"
                                )
                            else:
                                self.log_test_result(
                                    f"æ•°æ®å¯¼å‡ºå¯¼å…¥ - {table_name}",
                                    False,
                                    f"å¯¼å…¥æ•°æ®è¡Œæ•°ä¸åŒ¹é…: åŸå§‹ {len(df)}, å¯¼å…¥ {len(imported_df)}",
                                    "ERROR"
                                )
                                export_success -= 1
                        else:
                            self.log_test_result(
                                f"æ•°æ®å¯¼å‡º - {table_name}",
                                False,
                                "å¯¼å‡ºæ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨",
                                "ERROR"
                            )
                        
                        # æ¸…ç†å¯¼å‡ºæ–‡ä»¶
                        try:
                            export_file.unlink()
                        except FileNotFoundError:
                            pass
                        
                    except Exception as e:
                        self.log_test_result(
                            f"æ•°æ®å¯¼å‡º - {table_name}",
                            False,
                            f"å¯¼å‡ºå¼‚å¸¸: {str(e)}",
                            "ERROR"
                        )
                
                conn.close()
                
            except Exception as e:
                self.log_test_result(
                    f"æ•°æ®åº“è®¿é—® - {db_file.name}",
                    False,
                    f"æ— æ³•è®¿é—®æ•°æ®åº“: {str(e)}",
                    "ERROR"
                )
        
        success_rate = export_success / total_exports if total_exports > 0 else 1
        
        self.log_test_result(
            "æ•°æ®å¯¼å‡ºå¯¼å…¥æ€»ä½“",
            success_rate >= 0.8,
            f"æˆåŠŸç‡: {success_rate:.1%} ({export_success}/{total_exports})",
            "ERROR" if success_rate < 0.5 else "WARNING" if success_rate < 0.8 else "INFO"
        )
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        test_db = self.backup_dir / "export_test.db"
        try:
            test_db.unlink()
        except FileNotFoundError:
            pass
        
        return success_rate >= 0.8
    
    def test_storage_space_monitoring(self):
        """æµ‹è¯•å­˜å‚¨ç©ºé—´ç›‘æ§"""
        print("\nğŸ’½ æµ‹è¯•å­˜å‚¨ç©ºé—´ç›‘æ§...")
        
        try:
            # æ£€æŸ¥æ•°æ®ç›®å½•ç©ºé—´ä½¿ç”¨
            if self.data_dir.exists():
                total_size = sum(f.stat().st_size for f in self.data_dir.rglob('*') if f.is_file())
                
                self.log_test_result(
                    "æ•°æ®ç›®å½•å¤§å°",
                    True,
                    f"æ•°æ®ç›®å½•æ€»å¤§å°: {self.format_size(total_size)}"
                )
                
                # æ£€æŸ¥å¤§æ–‡ä»¶
                large_files = []
                for f in self.data_dir.rglob('*'):
                    if f.is_file() and f.stat().st_size > 100 * 1024 * 1024:  # 100MB
                        large_files.append((f, f.stat().st_size))
                
                if large_files:
                    self.log_test_result(
                        "å¤§æ–‡ä»¶æ£€æŸ¥",
                        True,
                        f"å‘ç° {len(large_files)} ä¸ªå¤§æ–‡ä»¶ (>100MB)",
                        "WARNING"
                    )
                else:
                    self.log_test_result(
                        "å¤§æ–‡ä»¶æ£€æŸ¥",
                        True,
                        "æœªå‘ç°å¼‚å¸¸å¤§æ–‡ä»¶"
                    )
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´
            disk_usage = shutil.disk_usage(self.project_root)
            free_space_gb = disk_usage.free / (1024**3)
            
            if free_space_gb > 1:  # è‡³å°‘1GBå¯ç”¨ç©ºé—´
                self.log_test_result(
                    "ç£ç›˜ç©ºé—´æ£€æŸ¥",
                    True,
                    f"å¯ç”¨ç©ºé—´: {free_space_gb:.1f} GB"
                )
            else:
                self.log_test_result(
                    "ç£ç›˜ç©ºé—´æ£€æŸ¥",
                    False,
                    f"ç£ç›˜ç©ºé—´ä¸è¶³: {free_space_gb:.1f} GB",
                    "ERROR"
                )
                return False
            
            return True
            
        except Exception as e:
            self.log_test_result(
                "å­˜å‚¨ç©ºé—´ç›‘æ§",
                False,
                f"ç›‘æ§å¼‚å¸¸: {str(e)}",
                "ERROR"
            )
            return False
    
    def calculate_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def format_size(self, size_bytes):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes == 0:
            return "0 B"
        
        size_names = ["B", "KB", "MB", "GB", "TB"]
        i = 0
        while size_bytes >= 1024 and i < len(size_names) - 1:
            size_bytes /= 1024
            i += 1
        
        return f"{size_bytes:.1f} {size_names[i]}"
    
    def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæ•°æ®å®Œæ•´æ€§æµ‹è¯•"""
        print("ğŸ’¾ TwitteræŠ“å–ç³»ç»Ÿæ•°æ®å®Œæ•´æ€§æµ‹è¯•")
        print("=" * 60)
        
        # æ•°æ®å®Œæ•´æ€§æµ‹è¯•å¥—ä»¶
        test_suites = [
            ('æ•°æ®åº“å®Œæ•´æ€§', self.test_database_integrity),
            ('æ•°æ®å¤‡ä»½æ¢å¤', self.test_data_backup_restore),
            ('æ•°æ®ä¸€è‡´æ€§', self.test_data_consistency),
            ('å¹¶å‘æ•°æ®è®¿é—®', self.test_concurrent_data_access),
            ('æ•°æ®å¯¼å‡ºå¯¼å…¥', self.test_data_export_import),
            ('å­˜å‚¨ç©ºé—´ç›‘æ§', self.test_storage_space_monitoring)
        ]
        
        passed_tests = 0
        total_tests = len(test_suites)
        
        for test_name, test_func in test_suites:
            try:
                if test_func():
                    passed_tests += 1
                    print(f"âœ… {test_name}: é€šè¿‡\n")
                else:
                    print(f"âŒ {test_name}: å¤±è´¥\n")
            except Exception as e:
                print(f"âŒ {test_name}: å¼‚å¸¸ - {str(e)}\n")
        
        # ç”Ÿæˆæ•°æ®å®Œæ•´æ€§æŠ¥å‘Š
        self.generate_integrity_report(passed_tests, total_tests)
        
        return passed_tests == total_tests
    
    def generate_integrity_report(self, passed_tests, total_tests):
        """ç”Ÿæˆæ•°æ®å®Œæ•´æ€§æµ‹è¯•æŠ¥å‘Š"""
        print("=" * 60)
        print("ğŸ’¾ æ•°æ®å®Œæ•´æ€§æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        print(f"æ€»æµ‹è¯•å¥—ä»¶: {total_tests}")
        print(f"é€šè¿‡: {passed_tests} âœ…")
        print(f"å¤±è´¥: {total_tests - passed_tests} âŒ")
        print(f"æˆåŠŸç‡: {(passed_tests/total_tests*100):.1f}%")
        
        # ä¸¥é‡æ€§è¯„ä¼°
        error_count = sum(1 for result in self.test_results if result.get('severity') == 'ERROR' and not result['success'])
        warning_count = sum(1 for result in self.test_results if result.get('severity') == 'WARNING' and not result['success'])
        info_count = sum(1 for result in self.test_results if result.get('severity') == 'INFO' and not result['success'])
        
        print(f"\nğŸš¨ é—®é¢˜ä¸¥é‡æ€§åˆ†æ:")
        print(f"ä¸¥é‡é”™è¯¯: {error_count} ğŸ”´")
        print(f"è­¦å‘Š: {warning_count} ğŸŸ¡")
        print(f"ä¿¡æ¯: {info_count} ğŸŸ¢")
        
        # è¯¦ç»†ç»“æœç»Ÿè®¡
        success_count = sum(1 for result in self.test_results if result['success'])
        total_count = len(self.test_results)
        
        print(f"\nè¯¦ç»†æµ‹è¯•é¡¹: {total_count}")
        print(f"æˆåŠŸ: {success_count}")
        print(f"å¤±è´¥: {total_count - success_count}")
        print(f"è¯¦ç»†æˆåŠŸç‡: {(success_count/total_count*100):.1f}%")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.project_root / "data_integrity_test_report.json"
        report_data = {
            'test_summary': {
                'total_suites': total_tests,
                'passed_suites': passed_tests,
                'suite_success_rate': passed_tests/total_tests,
                'total_tests': total_count,
                'passed_tests': success_count,
                'test_success_rate': success_count/total_count,
                'timestamp': datetime.now().isoformat()
            },
            'severity_analysis': {
                'error_issues': error_count,
                'warning_issues': warning_count,
                'info_issues': info_count
            },
            'detailed_results': self.test_results
        }
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        except Exception as e:
            print(f"\nâš ï¸ æ— æ³•ä¿å­˜æŠ¥å‘Š: {str(e)}")
        
        # æ•°æ®å®Œæ•´æ€§è¯„ä¼°
        print("\n" + "=" * 60)
        print("ğŸ’¾ æ•°æ®å®Œæ•´æ€§è¯„ä¼°")
        print("=" * 60)
        
        if error_count == 0 and warning_count <= 1:
            print("ğŸ‰ æ•°æ®çŠ¶æ€: ä¼˜ç§€ - æ•°æ®å®Œæ•´æ€§è‰¯å¥½")
        elif error_count == 0 and warning_count <= 3:
            print("âœ… æ•°æ®çŠ¶æ€: è‰¯å¥½ - å­˜åœ¨å°‘é‡è­¦å‘Š")
        elif error_count <= 1:
            print("âš ï¸ æ•°æ®çŠ¶æ€: ä¸€èˆ¬ - éœ€è¦å…³æ³¨æ•°æ®é—®é¢˜")
        else:
            print("âŒ æ•°æ®çŠ¶æ€: éœ€è¦ä¿®å¤ - å­˜åœ¨ä¸¥é‡æ•°æ®é—®é¢˜")
        
        print("\nğŸ’¾ æ•°æ®ç®¡ç†å»ºè®®:")
        print("1. å®šæœŸè¿›è¡Œæ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥")
        print("2. å»ºç«‹è‡ªåŠ¨åŒ–å¤‡ä»½æœºåˆ¶")
        print("3. å®æ–½æ•°æ®ä¸€è‡´æ€§éªŒè¯")
        print("4. ç›‘æ§å­˜å‚¨ç©ºé—´ä½¿ç”¨æƒ…å†µ")
        print("5. æµ‹è¯•æ•°æ®æ¢å¤æµç¨‹")
        print("6. å®æ–½å¹¶å‘è®¿é—®æ§åˆ¶")
        print("7. å®šæœŸæ¸…ç†å’Œå½’æ¡£æ—§æ•°æ®")
        
        # æ¸…ç†æµ‹è¯•ç›®å½•
        try:
            if self.backup_dir.exists():
                shutil.rmtree(self.backup_dir)
        except:
            pass

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='TwitteræŠ“å–ç³»ç»Ÿæ•°æ®å®Œæ•´æ€§æµ‹è¯•')
    parser.add_argument('--url', default='http://localhost:8084', 
                       help='WebæœåŠ¡å™¨åœ°å€ (é»˜è®¤: http://localhost:8084)')
    
    args = parser.parse_args()
    
    tester = DataIntegrityTester(args.url)
    tester.run_comprehensive_test()

if __name__ == '__main__':
    main()